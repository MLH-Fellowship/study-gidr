{"name":"Yves Hilpisch - Python for Finance  Analyze Big Financial Data-O'Reilly Media (2015).pdf","heading":["","Yves HilpischPython for FinancePython for Finance","Editors: ","Production Editor: ","Copyeditor: ","Proofreader: ","Indexer: ","Cover Designer: ","Interior Designer: ","Illustrator: ","Revision History for the First Edition:","Table of ContentsPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xiPart I. Python and Finance1.Why Python for Finance%3F. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3","2.Infrastructure and Tools. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25","iii","3.Introductory Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  49","Part II. Financial Analytics and Development4.Data Types and Structures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  79","iv %7C Table of Contents5.Data Visualization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  109","6.Financial Time Series. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  137","7.Input/Output Operations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  173","Table of Contents %7C v8.Performance Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  203","9.Mathematical Tools. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  233","10.Stochastics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  265","vi %7C Table of Contents","11.Statistics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  307","12.Excel Integration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357","Table of Contents %7C vii","13.Object Orientation and Graphical User Interfaces. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  381","14.Web Integration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  403","Part III. Derivatives Analytics Library15.Valuation Framework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  455","viii %7C Table of Contents","16.Simulation of Financial Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  467","17.Derivatives Valuation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  489","18.Portfolio Valuation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  511","Table of Contents %7C ix","19.Volatility Options. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  529","A.Selected Best Practices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  547B.Call Option Class. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  557C.Dates and Times. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  563Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  575x %7C Table of ContentsPreface","xi","Conventions Used in This Book","xii %7C Preface","Using Code Examples","Safari%C2%AE Books Online","Preface %7C xiii","How to Contact Us","Acknowledgments","xiv %7C Preface","Preface %7C xvPART IPython and Finance","CHAPTER 1Why Python for Finance%3F","What Is Python%3F","3","4 %7C Chapter 1: Why Python for Finance%3F","Brief History of Python","Python 0.9.0","Python 1.0","Python 2.0","Python 2.6","Python 2.7","Python 3.0","Python 3.3","Python 3.4","What Is Python%3F %7C 5","The Python Ecosystem","6 %7C Chapter 1: Why Python for Finance%3F","Python User Spectrum","What Is Python%3F %7C 7","The Scientific Stack","8 %7C Chapter 1: Why Python for Finance%3F","Technology in Finance","Technology in Finance %7C 9Technology Spending","Technology as Enabler","Technology and Talent as Barriers to Entry","10 %7C Chapter 1: Why Python for Finance%3F","Ever-Increasing Speeds, Frequencies, Data Volumes","Technology in Finance %7C 11","The Rise of Real-Time Analytics","12 %7C Chapter 1: Why Python for Finance%3F","Python for Finance","Python for Finance %7C 13Finance and Python Syntax","14 %7C Chapter 1: Why Python for Finance%3F","Python for Finance %7C 15","English","Mathematics","Python","16 %7C Chapter 1: Why Python for Finance%3FMathematics and Python Syntax","Efficiency and Productivity Through Python","Python for Finance %7C 17Shorter time-to-results","18 %7C Chapter 1: Why Python for Finance%3F","Ensuring high performance","Python for Finance %7C 19","Performance Computing with Python","20 %7C Chapter 1: Why Python for Finance%3F","From Prototyping to Production","Python for Finance %7C 21","Conclusions","22 %7C Chapter 1: Why Python for Finance%3F","Further Reading","Further Reading %7C 23","CHAPTER 2Infrastructure and Tools","25","Python Deployment","Anaconda","26 %7C Chapter 2: Infrastructure and Tools","NameDescription","Python Deployment %7C 27NameDescription","28 %7C Chapter 2: Infrastructure and Tools","Python Deployment %7C 29","y","30 %7C Chapter 2: Infrastructure and Tools","Multiple Python Environments","Python Deployment %7C 31","Python Quant Platform","32 %7C Chapter 2: Infrastructure and Tools","Python Deployment %7C 33","Tools","Python","34 %7C Chapter 2: Infrastructure and Tools","IPython","From shell to browser","Tools %7C 35","36 %7C Chapter 2: Infrastructure and Tools","Basic usage","Tools %7C 37","38 %7C Chapter 2: Infrastructure and Tools","Analytics and Publishing Platform","Tools %7C 39","40 %7C Chapter 2: Infrastructure and Tools","Markdown and LaTeX","Tools %7C 41","42 %7C Chapter 2: Infrastructure and Tools","Magic commands","Tools %7C 43","Description","44 %7C Chapter 2: Infrastructure and Tools","System shell commands","Spyder","Tools %7C 45","46 %7C Chapter 2: Infrastructure and Tools","Conclusions","Conclusions %7C 47","Further Reading","48 %7C Chapter 2: Infrastructure and ToolsCHAPTER 3Introductory Examples","49","Implied Volatilities","50 %7C Chapter 3: Introductory Examples","Implied Volatilities %7C 51","52 %7C Chapter 3: Introductory Examples","Implied Volatilities %7C 53","54 %7C Chapter 3: Introductory Examples","Implied Volatilities %7C 55","56 %7C Chapter 3: Introductory Examples","Implied Volatilities %7C 57","58 %7C Chapter 3: Introductory Examples","Monte Carlo Simulation","Monte Carlo Simulation %7C 59","60 %7C Chapter 3: Introductory Examples","Pure Python","Monte Carlo Simulation %7C 61","62 %7C Chapter 3: Introductory Examples","Vectorization with NumPy","Monte Carlo Simulation %7C 63","64 %7C Chapter 3: Introductory Examples","Vectorization","Full Vectorization with Log Euler Scheme","Monte Carlo Simulation %7C 65","66 %7C Chapter 3: Introductory ExamplesGraphical Analysis","Monte Carlo Simulation %7C 67","Technical Analysis","68 %7C Chapter 3: Introductory Examples","Scientific and Financial Python Stack","Technical Analysis %7C 69","70 %7C Chapter 3: Introductory Examples","Technical Analysis %7C 71","72 %7C Chapter 3: Introductory Examples","Technical Analysis %7C 73","Financial Time Series","Conclusions","74 %7C Chapter 3: Introductory Examples","Further Reading","Further Reading %7C 75PART IIFinancial Analytics and Development","CHAPTER 4Data Types and Structures","79","Basic Data Types","Integers","80 %7C Chapter 4: Data Types and Structures","Large Integers","Floats","Basic Data Types %7C 81","82 %7C Chapter 4: Data Types and Structures","Arbitrary-Precision Floats","Basic Data Types %7C 83Strings","84 %7C Chapter 4: Data Types and Structures","MethodArgumentsReturns/result","Basic Data Types %7C 85","Regular Expressions","Basic Data Structures","86 %7C Chapter 4: Data Types and Structures","Tuples","Zero-Based Numbering","Basic Data Structures %7C 87","Lists","88 %7C Chapter 4: Data Types and Structures","MethodArgumentsReturns/result","Excursion: Control Structures","Basic Data Structures %7C 89","Looping over Lists","90 %7C Chapter 4: Data Types and Structures","Excursion: Functional Programming","Basic Data Structures %7C 91","List Comprehensions, Functional Programming, Anonymous Functions","Dicts","92 %7C Chapter 4: Data Types and Structures","Basic Data Structures %7C 93","MethodArgumentsReturns/result","Sets","94 %7C Chapter 4: Data Types and Structures","NumPy Data Structures","NumPy Data Structures %7C 95Arrays with Python Lists","96 %7C Chapter 4: Data Types and Structures","Regular NumPy Arrays","NumPy Data Structures %7C 97","98 %7C Chapter 4: Data Types and Structures","NumPy Data Structures %7C 99","dtypeDescriptionExample","100 %7C Chapter 4: Data Types and Structures","Using NumPy Arrays","Structured Arrays","NumPy Data Structures %7C 101","Structured Arrays","Vectorization of Code","Basic Vectorization","102 %7C Chapter 4: Data Types and Structures","Vectorization of Code %7C 103","104 %7C Chapter 4: Data Types and Structures","Universal Functions","Memory Layout","Vectorization of Code %7C 105","Conclusions","106 %7C Chapter 4: Data Types and Structures","Further Reading","Further Reading %7C 107CHAPTER 5Data Visualization","Two-Dimensional Plotting","109","One-Dimensional Data Set","x values","y values","110 %7C Chapter 5: Data Visualization","NumPy Arrays and matplotlib","Two-Dimensional Plotting %7C 111","ParameterDescription","112 %7C Chapter 5: Data Visualization","Two-Dimensional Plotting %7C 113","CharacterColor","CharacterSymbol","114 %7C Chapter 5: Data VisualizationCharacterSymbol","Two-Dimensional Data Set","Two-Dimensional Plotting %7C 115","LocDescription","116 %7C Chapter 5: Data VisualizationLocDescription","Two-Dimensional Plotting %7C 117","118 %7C Chapter 5: Data Visualization","Two-Dimensional Plotting %7C 119","120 %7C Chapter 5: Data Visualization","Other Plot Styles","Two-Dimensional Plotting %7C 121","122 %7C Chapter 5: Data Visualization","Two-Dimensional Plotting %7C 123","ParameterDescription","124 %7C Chapter 5: Data Visualization","Two-Dimensional Plotting %7C 125","126 %7C Chapter 5: Data Visualization","Two-Dimensional Plotting %7C 127","Financial Plots","128 %7C Chapter 5: Data VisualizationData Quality of Web Sources","Financial Plots %7C 129","ParameterDescription","130 %7C Chapter 5: Data Visualization","Financial Plots %7C 131","3D Plotting","132 %7C Chapter 5: Data Visualization","3D Plotting %7C 133","ParameterDescription","134 %7C Chapter 5: Data Visualization","Conclusions","Further Reading","Conclusions %7C 135","136 %7C Chapter 5: Data VisualizationCHAPTER 6Financial Time Series","137","pandas Basics","First Steps with DataFrame Class","138 %7C Chapter 6: Financial Time Series","pandas Basics %7C 139","140 %7C Chapter 6: Financial Time Series","pandas Basics %7C 141","Second Steps with DataFrame Class","142 %7C Chapter 6: Financial Time Series","ParameterFormatDescription","pandas Basics %7C 143","ParameterFormatDescription","144 %7C Chapter 6: Financial Time Series","AliasDescription","pandas Basics %7C 145","Arrays and DataFrames","Basic Analytics","146 %7C Chapter 6: Financial Time Series","NumPy Universal Functions","pandas Basics %7C 147","ParameterFormatDescription","148 %7C Chapter 6: Financial Time SeriesParameterFormatDescription","Series Class","pandas Basics %7C 149","GroupBy Operations","150 %7C Chapter 6: Financial Time Series","Financial Data","Financial Data %7C 151","152 %7C Chapter 6: Financial Time Series","ParameterFormatDescription","Financial Data %7C 153","Vectorization with DataFrames","154 %7C Chapter 6: Financial Time Series","Financial Data %7C 155","156 %7C Chapter 6: Financial Time Series","Regression Analysis","Regression Analysis %7C 157","158 %7C Chapter 6: Financial Time Series","Regression Analysis %7C 159","160 %7C Chapter 6: Financial Time Series","ParameterFormatDescription","Regression Analysis %7C 161ParameterFormatDescription","162 %7C Chapter 6: Financial Time Series","Regression Analysis %7C 163","164 %7C Chapter 6: Financial Time Series","Regression Analysis %7C 165","High-Frequency Data","166 %7C Chapter 6: Financial Time Series","High-Frequency Data %7C 167","168 %7C Chapter 6: Financial Time Series","High-Frequency Data %7C 169","Conclusions","170 %7C Chapter 6: Financial Time SeriesFurther Reading","Further Reading %7C 171","CHAPTER 7Input/Output Operations","173","Basic I/O with Python","Writing Objects to Disk","174 %7C Chapter 7: Input/Output Operations","Basic I/O with Python %7C 175","176 %7C Chapter 7: Input/Output Operations","Reading and Writing Text Files","Basic I/O with Python %7C 177","178 %7C Chapter 7: Input/Output Operations","SQL Databases","Basic I/O with Python %7C 179","180 %7C Chapter 7: Input/Output Operations","Writing and Reading NumPy Arrays","Basic I/O with Python %7C 181","182 %7C Chapter 7: Input/Output Operations","I/O with pandas","FormatInputOutputRemark","I/O with pandas %7C 183FormatInputOutputRemark","SQL Database","184 %7C Chapter 7: Input/Output Operations","From SQL to pandas","I/O with pandas %7C 185","186 %7C Chapter 7: Input/Output Operations","I/O with pandas %7C 187","Data as CSV File","188 %7C Chapter 7: Input/Output Operations","Data as Excel File","I/O with pandas %7C 189","Fast I/O with PyTables","Working with Tables","190 %7C Chapter 7: Input/Output Operations","Fast I/O with PyTables %7C 191","192 %7C Chapter 7: Input/Output Operations","Fast I/O with PyTables %7C 193","194 %7C Chapter 7: Input/Output Operations","Fast Complex Queries","Fast I/O with PyTables %7C 195","Working with Compressed Tables","196 %7C Chapter 7: Input/Output Operations","Working with Arrays","Fast I/O with PyTables %7C 197","HDF5-Based Data Storage","Out-of-Memory Computations","198 %7C Chapter 7: Input/Output Operations","Fast I/O with PyTables %7C 199","Conclusions","200 %7C Chapter 7: Input/Output Operations","Further Reading","Further Reading %7C 201","202 %7C Chapter 7: Input/Output OperationsCHAPTER 8Performance Python","203","Python Paradigms and Performance","204 %7C Chapter 8: Performance Python","Python Paradigms and Performance %7C 205","206 %7C Chapter 8: Performance Python","Memory Layout and Performance","Memory Layout and Performance %7C 207","208 %7C Chapter 8: Performance Python","Parallel Computing","The Monte Carlo Algorithm","Parallel Computing %7C 209","The Sequential Calculation","210 %7C Chapter 8: Performance Python","The Parallel Calculation","Parallel Computing %7C 211","212 %7C Chapter 8: Performance Python","Parallel Computing %7C 213","Performance Comparison","214 %7C Chapter 8: Performance Pythonmultiprocessing","multiprocessing %7C 215","Easy Parallelization","216 %7C Chapter 8: Performance Python","Dynamic Compiling","Introductory Example","Dynamic Compiling %7C 217","Quick Wins","Binomial Option Pricing","218 %7C Chapter 8: Performance Python","Dynamic Compiling %7C 219","220 %7C Chapter 8: Performance Python","Dynamic Compiling %7C 221","222 %7C Chapter 8: Performance Python","Efficiency","Speed-up","Memory","Static Compiling with Cython","Static Compiling with Cython %7C 223","224 %7C Chapter 8: Performance Python","Static Compiling with Cython %7C 225","Generation of Random Numbers on GPUs","226 %7C Chapter 8: Performance Python","Generation of Random Numbers on GPUs %7C 227","228 %7C Chapter 8: Performance Python","Generation of Random Numbers on GPUs %7C 229","Conclusions","230 %7C Chapter 8: Performance PythonFurther Reading","Further Reading %7C 231CHAPTER 9Mathematical Tools","233Approximation","Regression","234 %7C Chapter 9: Mathematical Tools","Monomials as basis functions","ParameterDescription","Approximation %7C 235","236 %7C Chapter 9: Mathematical Tools","Approximation %7C 237Individual basis functions","238 %7C Chapter 9: Mathematical Tools","Approximation %7C 239","Noisy data","240 %7C Chapter 9: Mathematical ToolsUnsorted data","Approximation %7C 241Multiple dimensions","242 %7C Chapter 9: Mathematical Tools","Approximation %7C 243","244 %7C Chapter 9: Mathematical Tools","Regression","Interpolation","Approximation %7C 245","ParameterDescription","ParameterDescription","246 %7C Chapter 9: Mathematical Tools","Approximation %7C 247","248 %7C Chapter 9: Mathematical Tools","Interpolation","Convex Optimization","Convex Optimization %7C 249","Global Optimization","250 %7C Chapter 9: Mathematical Tools","Local Optimization","Convex Optimization %7C 251","252 %7C Chapter 9: Mathematical ToolsConstrained Optimization","Convex Optimization %7C 253","254 %7C Chapter 9: Mathematical ToolsIntegration","Integration %7C 255","Numerical Integration","256 %7C Chapter 9: Mathematical Tools","Integration by Simulation","Symbolic Computation","Symbolic Computation %7C 257","Basics","258 %7C Chapter 9: Mathematical Tools","Equations","Symbolic Computation %7C 259","Integration","260 %7C Chapter 9: Mathematical Tools","Differentiation","Symbolic Computation %7C 261","Symbolic Computations","Conclusions","262 %7C Chapter 9: Mathematical Tools","Further Reading","Further Reading %7C 263CHAPTER 10Stochastics","265","Random Numbers","266 %7C Chapter 10: Stochastics","FunctionParametersDescription","Random Numbers %7C 267","FunctionParametersDescription","268 %7C Chapter 10: StochasticsFunctionParametersDescription","Random Numbers %7C 269","Standard normal","Normal","Chi square","Poisson","270 %7C Chapter 10: Stochastics","Simulation","Random Variables","Simulation %7C 271","272 %7C Chapter 10: Stochastics","Simulation %7C 273","Stochastic Processes","Geometric Brownian motion","274 %7C Chapter 10: Stochastics","Simulation %7C 275","Square-root diffusion","276 %7C Chapter 10: Stochastics","Simulation %7C 277","278 %7C Chapter 10: Stochastics","Simulation %7C 279","280 %7C Chapter 10: Stochastics","Stochastic volatility","Simulation %7C 281","282 %7C Chapter 10: Stochastics","Simulation %7C 283","284 %7C Chapter 10: Stochastics","Jump diffusion","N","Simulation %7C 285","286 %7C Chapter 10: Stochastics","Variance Reduction","Simulation %7C 287","288 %7C Chapter 10: Stochastics","Simulation %7C 289","Valuation","290 %7C Chapter 10: Stochastics","European Options","Valuation %7C 291","292 %7C Chapter 10: Stochastics","Valuation %7C 293","294 %7C Chapter 10: Stochastics","American Options","Valuation %7C 295","296 %7C Chapter 10: Stochastics","Valuation %7C 297","Risk Measures","Value-at-Risk","298 %7C Chapter 10: Stochastics","Risk Measures %7C 299","300 %7C Chapter 10: Stochastics","Risk Measures %7C 301","Credit Value Adjustments","302 %7C Chapter 10: Stochastics","Risk Measures %7C 303","304 %7C Chapter 10: Stochastics","Conclusions","Further Reading","Conclusions %7C 305","306 %7C Chapter 10: StochasticsCHAPTER 11Statistics","307","Normality Tests","308 %7C Chapter 11: Statistics","Benchmark Case","Normality Tests %7C 309","310 %7C Chapter 11: Statistics","Normality Tests %7C 311","312 %7C Chapter 11: Statistics","Normality Tests %7C 313","314 %7C Chapter 11: Statistics","Normality Tests %7C 315","316 %7C Chapter 11: Statistics","Normality","Real-World Data","Normality Tests %7C 317","318 %7C Chapter 11: Statistics","Normality Tests %7C 319","320 %7C Chapter 11: Statistics","Normality Tests %7C 321","Portfolio Optimization","322 %7C Chapter 11: Statistics","The Data","Portfolio Optimization %7C 323","The Basic Theory","324 %7C Chapter 11: Statistics","E","Portfolio Optimization %7C 325","Language","326 %7C Chapter 11: Statistics","Portfolio Optimization %7C 327","Portfolio Optimizations","328 %7C Chapter 11: Statistics","Portfolio Optimization %7C 329","Efficient Frontier","330 %7C Chapter 11: Statistics","Portfolio Optimization %7C 331","Capital Market Line","332 %7C Chapter 11: Statistics","Portfolio Optimization %7C 333","334 %7C Chapter 11: Statistics","Principal Component Analysis","Principal Component Analysis %7C 335","The DAX Index and Its 30 Stocks","336 %7C Chapter 11: Statistics","Applying PCA","Principal Component Analysis %7C 337","Constructing a PCA Index","338 %7C Chapter 11: Statistics","Principal Component Analysis %7C 339","340 %7C Chapter 11: Statistics","Bayesian Regression","Bayes's Formula","Bayesian Regression %7C 341","Prior","Normalizing constant","Likelihood","PyMC3","342 %7C Chapter 11: Statistics","PyMC3","Introductory Example","Bayesian Regression %7C 343","344 %7C Chapter 11: Statistics","Bayesian Regression %7C 345","346 %7C Chapter 11: StatisticsReal Data","Bayesian Regression %7C 347","348 %7C Chapter 11: Statistics","Bayesian Regression %7C 349","350 %7C Chapter 11: Statistics","Bayesian Regression %7C 351","352 %7C Chapter 11: Statistics","Bayesian Regression %7C 353Absolute Price Data Versus Relative Return Data","354 %7C Chapter 11: Statistics","Conclusions","Further Reading","Conclusions %7C 355","356 %7C Chapter 11: StatisticsCHAPTER 12Excel Integration","357","Basic Spreadsheet Interaction","358 %7C Chapter 12: Excel Integration","Generating Workbooks (.xls)","Basic Spreadsheet Interaction %7C 359","Generating Workbooks (.xslx)","360 %7C Chapter 12: Excel Integration","Basic Spreadsheet Interaction %7C 361","Reading from Workbooks","362 %7C Chapter 12: Excel Integration","TypeNumber"," type","Basic Spreadsheet Interaction %7C 363","Using OpenPyxl","364 %7C Chapter 12: Excel Integration","Basic Spreadsheet Interaction %7C 365","Using pandas for Reading and Writing","366 %7C Chapter 12: Excel Integration","Basic Spreadsheet Interaction %7C 367","368 %7C Chapter 12: Excel Integration","Scripting Excel with Python","Installing DataNitro","Scripting Excel with Python %7C 369","Working with DataNitro","370 %7C Chapter 12: Excel IntegrationScripting with DataNitro","Scripting Excel with Python %7C 371","AttributeDescription","AttributeDescription","372 %7C Chapter 12: Excel Integration","AttributeDescription","Scripting Excel with Python %7C 373","Plotting with DataNitro","374 %7C Chapter 12: Excel Integration","Scripting Excel with Python %7C 375","User-defined functions","376 %7C Chapter 12: Excel Integration","Scripting Excel with Python %7C 377","378 %7C Chapter 12: Excel Integration","xlwings","Conclusions","xlwings %7C 379","Further Reading","380 %7C Chapter 12: Excel IntegrationCHAPTER 13Object Orientation and GraphicalUser Interfaces","Object Orientation","381","Basics of Python Classes","382 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Object Orientation %7C 383","384 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Object Orientation %7C 385","386 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Simple Short Rate Class","Object Orientation %7C 387","388 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Object Orientation %7C 389","390 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Cash Flow Series Class","Object Orientation %7C 391","392 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Graphical User Interfaces","Graphical User Interfaces %7C 393Short Rate Class with GUI","394 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Graphical User Interfaces %7C 395","Updating of Values","396 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Graphical User Interfaces %7C 397","Cash Flow Series Class with GUI","398 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Graphical User Interfaces %7C 399","400 %7C Chapter 13: Object Orientation and Graphical User Interfaces","Conclusions","Further Reading","Conclusions %7C 401","402 %7C Chapter 13: Object Orientation and Graphical User InterfacesCHAPTER 14Web Integration","403","Web Basics","404 %7C Chapter 14: Web Integration","ftplib","Web Basics %7C 405","406 %7C Chapter 14: Web Integration","httplib","Web Basics %7C 407","urllib","408 %7C Chapter 14: Web Integration","Web Basics %7C 409","410 %7C Chapter 14: Web Integration","Web Plotting","Static Plots","Web Plotting %7C 411","412 %7C Chapter 14: Web Integration","Web Plotting %7C 413","Interactive Plots","414 %7C Chapter 14: Web Integration","Web Plotting %7C 415","416 %7C Chapter 14: Web Integration","Real-Time Plots","Web Plotting %7C 417","Real-time FX data","418 %7C Chapter 14: Web Integration","Web Plotting %7C 419","420 %7C Chapter 14: Web Integration","Real-time stock price quotes","Web Plotting %7C 421","422 %7C Chapter 14: Web Integration","Web Plotting %7C 423","Rapid Web Applications","424 %7C Chapter 14: Web Integration","Rapid Web Applications %7C 425","Traders' Chat Room","Data Modeling","426 %7C Chapter 14: Web Integration","The Python Code","Imports and database preliminaries","Rapid Web Applications %7C 427","428 %7C Chapter 14: Web IntegrationCore functionality","Rapid Web Applications %7C 429","430 %7C Chapter 14: Web Integration","Rapid Web Applications %7C 431","432 %7C Chapter 14: Web Integration","Rapid Web Applications %7C 433Security","Templating","434 %7C Chapter 14: Web Integration","Rapid Web Applications %7C 435","436 %7C Chapter 14: Web Integration","Rapid Web Applications %7C 437","438 %7C Chapter 14: Web Integration","Rapid Web Applications %7C 439","Styling","440 %7C Chapter 14: Web Integration","Rapid Web Applications %7C 441","Web Services","442 %7C Chapter 14: Web Integration","The Financial Model","Web Services %7C 443","444 %7C Chapter 14: Web IntegrationThe Implementation","Web Services %7C 445","446 %7C Chapter 14: Web Integration","Web Services %7C 447","448 %7C Chapter 14: Web Integration","Web Services %7C 449","450 %7C Chapter 14: Web Integration","Web Services Architecture","Conclusions","Conclusions %7C 451","Further Reading","452 %7C Chapter 14: Web Integration","PART IIIDerivatives Analytics Library","CHAPTER 15Valuation Framework","Fundamental Theorem of Asset Pricing","455","A Simple Example","456 %7C Chapter 15: Valuation Framework","The General Results","Fundamental Theorem of Asset Pricing %7C 457","Risk-Neutral Discounting","Modeling and Handling Dates","458 %7C Chapter 15: Valuation Framework","Risk-Neutral Discounting %7C 459","Constant Short Rate","460 %7C Chapter 15: Valuation Framework","Risk-Neutral Discounting %7C 461","Market Environments","462 %7C Chapter 15: Valuation Framework","Market Environments %7C 463","464 %7C Chapter 15: Valuation Framework","Conclusions","Conclusions %7C 465","Further Reading","466 %7C Chapter 15: Valuation FrameworkCHAPTER 16Simulation of Financial Models","467","Random Number Generation","468 %7C Chapter 16: Simulation of Financial Models","Random Number Generation %7C 469","Generic Simulation Class","470 %7C Chapter 16: Simulation of Financial Models","Generic Simulation Class %7C 471","472 %7C Chapter 16: Simulation of Financial Models","ElementTypeMandatoryDescription","Geometric Brownian Motion","Geometric Brownian Motion %7C 473","The Simulation Class","474 %7C Chapter 16: Simulation of Financial Models","Geometric Brownian Motion %7C 475","A Use Case","476 %7C Chapter 16: Simulation of Financial Models","Geometric Brownian Motion %7C 477","Jump Diffusion","The Simulation Class","478 %7C Chapter 16: Simulation of Financial Models","Jump Diffusion %7C 479","480 %7C Chapter 16: Simulation of Financial Models","ElementTypeMandatoryDescription","A Use Case","Jump Diffusion %7C 481","Square-Root Diffusion","482 %7C Chapter 16: Simulation of Financial Models","The Simulation Class","Square-Root Diffusion %7C 483","484 %7C Chapter 16: Simulation of Financial Models","ElementTypeMandatoryDescription","A Use Case","Square-Root Diffusion %7C 485","Conclusions","486 %7C Chapter 16: Simulation of Financial Models","Further Reading","Further Reading %7C 487","488 %7C Chapter 16: Simulation of Financial ModelsCHAPTER 17Derivatives Valuation","Generic Valuation Class","489","490 %7C Chapter 17: Derivatives Valuation","Generic Valuation Class %7C 491","492 %7C Chapter 17: Derivatives Valuation","European Exercise","European Exercise %7C 493","The Valuation Class","494 %7C Chapter 17: Derivatives Valuation","European Exercise %7C 495","A Use Case","496 %7C Chapter 17: Derivatives Valuation","European Exercise %7C 497","498 %7C Chapter 17: Derivatives Valuation","European Exercise %7C 499","American Exercise","500 %7C Chapter 17: Derivatives Valuation","Least-Squares Monte Carlo","American Exercise %7C 501","The Valuation Class","502 %7C Chapter 17: Derivatives Valuation","American Exercise %7C 503","A Use Case","504 %7C Chapter 17: Derivatives Valuation","American Exercise %7C 505","506 %7C Chapter 17: Derivatives Valuation","Conclusions","Conclusions %7C 507","508 %7C Chapter 17: Derivatives ValuationFurther Reading","Further Reading %7C 509CHAPTER 18Portfolio Valuation","511","Derivatives Positions","The Class","512 %7C Chapter 18: Portfolio Valuation","Derivatives Positions %7C 513","A Use Case","514 %7C Chapter 18: Portfolio Valuation","Derivatives Portfolios","Derivatives Portfolios %7C 515The Class","516 %7C Chapter 18: Portfolio Valuation","Derivatives Portfolios %7C 517","518 %7C Chapter 18: Portfolio Valuation","Derivatives Portfolios %7C 519A Use Case","520 %7C Chapter 18: Portfolio Valuation","Derivatives Portfolios %7C 521","522 %7C Chapter 18: Portfolio Valuation","Derivatives Portfolios %7C 523","524 %7C Chapter 18: Portfolio Valuation","Conclusions","Conclusions %7C 525","526 %7C Chapter 18: Portfolio Valuation","Further Reading","Further Reading %7C 527","528 %7C Chapter 18: Portfolio Valuation","CHAPTER 19Volatility Options","The VSTOXX Data","VSTOXX Index Data","530 %7C Chapter 19: Volatility Options530","VSTOXX Futures Data","The VSTOXX Data %7C 531","532 %7C Chapter 19: Volatility Options","VSTOXX Options Data","The VSTOXX Data %7C 533","Model Calibration","534 %7C Chapter 19: Volatility OptionsRelevant Market Data","Model Calibration %7C 535","Option Modeling","536 %7C Chapter 19: Volatility Options","Model Calibration %7C 537","Calibration Procedure","538 %7C Chapter 19: Volatility Options","Model Calibration %7C 539","540 %7C Chapter 19: Volatility Options","Model Calibration %7C 541","American Options on the VSTOXX","542 %7C Chapter 19: Volatility OptionsModeling Option Positions","American Options on the VSTOXX %7C 543","The Options Portfolio","544 %7C Chapter 19: Volatility Options","Conclusions","Conclusions %7C 545Further Reading","546 %7C Chapter 19: Volatility OptionsAPPENDIX ASelected Best Practices","Python Syntax","547","548 %7C Appendix A: Selected Best Practices","Python Syntax %7C 549","SymbolDescription","Documentation","550 %7C Appendix A: Selected Best Practices","Documentation %7C 551","Unit Testing","552 %7C Appendix A: Selected Best Practices","Unit Testing %7C 553","FunctionDescription","554 %7C Appendix A: Selected Best Practices","Unit Testing %7C 555APPENDIX BCall Option Class","557","558 %7C Appendix B: Call Option Class","Call Option Class %7C 559","560 %7C Appendix B: Call Option Class","Call Option Class %7C 561","APPENDIX CDates and Times","Python","563","564 %7C Appendix C: Dates and Times","Python %7C 565","566 %7C Appendix C: Dates and Times","Python %7C 567","NumPy","568 %7C Appendix C: Dates and Times","NumPy %7C 569","570 %7C Appendix C: Dates and Times","pandas","pandas %7C 571","572 %7C Appendix C: Dates and Times","pandas %7C 573","IndexSymbols","A","575","B"," C ","576 %7C Index","D","Index %7C 577","E","578 %7C Index","F"," G ","H","I","Index %7C 579","J","K","L","M","580 %7C Index","N","O","Index %7C 581","P","582 %7C Index","Q","R","S","Index %7C 583","T","584 %7C Index","U","V","W","X","Index %7C 585","Y","Z","586 %7C IndexAbout the AuthorYves Hilpisch"],"paragraph":["PYTHON/FINANCEPython for FinanceISBN: 978-1-491-94528-5US %2444.99 CAN %2447.99%E2%80%9CPython'sreadablesyntax,easyintegrationwithC/C%2B%2B,andthewidevarietyofnumericalcomputingtoolsmakeitanaturalchoiceforfinancialanalytics.It'srapidlybecomingthede-factoreplacementforapatchworkoflanguagesandtoolsatleadingfinancialinstitutions.%E2%80%9D-Kirat Singhcofounder, President and CTOWashington Square TechnologiesTwitter: @oreillymediafacebook.com/oreillyThe financial industry has adopted Python at a tremendous rate, with some of the largest investment banks and hedge funds using it to build core trading and risk management systems. This hands-on guide helps both developers and quantitative analysts get started with Python, and guides you through the most important aspects of using Python for quantitative finance.Using practical examples throughout the book, author Yves Hilpisch also shows you how to develop a full-fledged framework for Monte Carlo simulation-based derivatives and risk analytics, based on a large, realistic case study. Much of the book uses interactive IPython Notebooks, with topics that include:%E2%96%A0Fundamentals: Python data structures, NumPy array handling, time series analysis with pandas, visualization with matplotlib, high performance I/O operations with PyTables, date/time information handling, and selected best practices%E2%96%A0Financial topics: Mathematical techniques with NumPy, SciPy, and SymPy, such as regression and optimization%3B stochastics for Monte Carlo simulation, Value-at-Risk, and Credit-Value-at-Risk calculations%3B statistics for normality tests, mean-variance portfolio optimization, principal component analysis (PCA), and Bayesian regression%E2%96%A0Special topics: Performance Python for financial algorithms, such as vectorization and parallelization, integrating Python with Excel, and building financial applications based on Web technologiesYves Hilpisch is the founder and managing partner of The Python Quants, an analytics software provider and financial engineering group. Yves also lectures on mathematical finance and organizes meetups and conferences about Python for Quant Finance in New York and London.Yves HilpischPython for FinanceANALYZE BIG FINANCIAL DATAPython for FinanceHilpisch","by Yves HilpischCopyright %C2%A9 2015 Yves Hilpisch. All rights reserved.Printed in the United States of America.Published by O'Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.O'Reilly books may be purchased for educational, business, or sales promotional use. Online editions arealso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.","Brian MacDonald and Meghan Blanchette","Matthew Hacker","Charles Roumeliotis","Rachel Head","Judith McConville","Ellie Volckhausen","David Futato","Rebecca DemarestDecember 2014:First Edition","2014-12-09: First releaseSee http://oreilly.com/catalog/errata.csp%3Fisbn%3D9781491945285 for release details.The O'Reilly logo is a registered trademark of O'Reilly Media, Inc. Python for Finance, the cover image of aHispaniolan solenodon, and related trade dress are trademarks of O'Reilly Media, Inc.Many of the designations used by manufacturers and sellers to distinguish their products are claimed astrademarks. Where those designations appear in this book, and O'Reilly Media, Inc. was aware of a trademarkclaim, the designations have been printed in caps or initial caps.While the publisher and the author have used good faith efforts to ensure that the information and instruc%E2%80%90tions contained in this work are accurate, the publisher and the author disclaim all responsibility for errorsor omissions, including without limitation responsibility for damages resulting from the use of or relianceon this work. Use of the information and instructions contained in this work is at your own risk. If any codesamples or other technology this work contains or describes is subject to open source licenses or the intel%E2%80%90lectual property rights of others, it is your responsibility to ensure that your use thereof complies with suchlicenses and/or rights.This book is not intended as financial advice. Please consult a qualified professional if you require financialadvice.ISBN: 978-1-491-94528-5%5BLSI%5D","What Is Python%3F                                                                                                                 3Brief History of Python                                                                                                 5The Python Ecosystem                                                                                                  6Python User Spectrum                                                                                                   7The Scientific Stack                                                                                                        8Technology in Finance                                                                                                      9Technology Spending                                                                                                  10Technology as Enabler                                                                                                 10Technology and Talent as Barriers to Entry                                                             10Ever-Increasing Speeds, Frequencies, Data Volumes                                              11The Rise of Real-Time Analytics                                                                                12Python for Finance                                                                                                          13Finance and Python Syntax                                                                                        14Efficiency and Productivity Through Python                                                          17From Prototyping to Production                                                                               21Conclusions                                                                                                                      22Further Reading                                                                                                               23","Python Deployment                                                                                                        26Anaconda                                                                                                                      26Python Quant Platform                                                                                               32Tools                                                                                                                                   34Python                                                                                                                            34","IPython                                                                                                                          35Spyder                                                                                                                            45Conclusions                                                                                                                      47Further Reading                                                                                                               48","Implied Volatilities                                                                                                           50Monte Carlo Simulation                                                                                                  59Pure Python                                                                                                                  61Vectorization with NumPy                                                                                         63Full Vectorization with Log Euler Scheme                                                                65Graphical Analysis                                                                                                       67Technical Analysis                                                                                                           68Conclusions                                                                                                                      74Further Reading                                                                                                               75","Basic Data Types                                                                                                               80Integers                                                                                                                          80Floats                                                                                                                              81Strings                                                                                                                            84Basic Data Structures                                                                                                       86Tuples                                                                                                                             87Lists                                                                                                                                 88Excursion: Control Structures                                                                                    89Excursion: Functional Programming                                                                        91Dicts                                                                                                                               92Sets                                                                                                                                  94NumPy Data Structures                                                                                                  95Arrays with Python Lists                                                                                             96Regular NumPy Arrays                                                                                                97Structured Arrays                                                                                                       101Vectorization of Code                                                                                                    102Basic Vectorization                                                                                                     102Memory Layout                                                                                                          105Conclusions                                                                                                                    106Further Reading                                                                                                             107","Two-Dimensional Plotting                                                                                           109One-Dimensional Data Set                                                                                       110Two-Dimensional Data Set                                                                                       115Other Plot Styles                                                                                                         121Financial Plots                                                                                                                1283D Plotting                                                                                                                      132Conclusions                                                                                                                    135Further Reading                                                                                                             135","pandas Basics                                                                                                                  138First Steps with DataFrame Class                                                                            138Second Steps with DataFrame Class                                                                        142Basic Analytics                                                                                                            146Series Class                                                                                                                  149GroupBy Operations                                                                                                 150Financial Data                                                                                                                151Regression Analysis                                                                                                       157High-Frequency Data                                                                                                    166Conclusions                                                                                                                    170Further Reading                                                                                                             171","Basic I/O with Python                                                                                                   174Writing Objects to Disk                                                                                             174Reading and Writing Text Files                                                                                177SQL Databases                                                                                                            179Writing and Reading NumPy Arrays                                                                      181I/O with pandas                                                                                                             183SQL Database                                                                                                              184From SQL to pandas                                                                                                  185Data as CSV File                                                                                                         188Data as Excel File                                                                                                        189Fast I/O with PyTables                                                                                                  190Working with Tables                                                                                                  190Working with Compressed Tables                                                                           196Working with Arrays                                                                                                 197Out-of-Memory Computations                                                                               198Conclusions                                                                                                                    200Further Reading                                                                                                             201","Python Paradigms and Performance                                                                          204Memory Layout and Performance                                                                              207Parallel Computing                                                                                                        209The Monte Carlo Algorithm                                                                                     209The Sequential Calculation                                                                                       210The Parallel Calculation                                                                                            211Performance Comparison                                                                                         214multiprocessing                                                                                                              215Dynamic Compiling                                                                                                      217Introductory Example                                                                                               217Binomial Option Pricing                                                                                           218Static Compiling with Cython                                                                                     223Generation of Random Numbers on GPUs                                                               226Conclusions                                                                                                                    230Further Reading                                                                                                             231","Approximation                                                                                                               234Regression                                                                                                                   234Interpolation                                                                                                               245Convex Optimization                                                                                                    249Global Optimization                                                                                                  250Local Optimization                                                                                                    251Constrained Optimization                                                                                        253Integration                                                                                                                      255Numerical Integration                                                                                               256Integration by Simulation                                                                                         257Symbolic Computation                                                                                                 257Basics                                                                                                                            258Equations                                                                                                                     259Integration                                                                                                                   260Differentiation                                                                                                            261Conclusions                                                                                                                    262Further Reading                                                                                                             263","Random Numbers                                                                                                         266Simulation                                                                                                                       271Random Variables                                                                                                      271Stochastic Processes                                                                                                   274Variance Reduction                                                                                                    287","Valuation                                                                                                                         290European Options                                                                                                      291American Options                                                                                                      295Risk Measures                                                                                                                 298Value-at-Risk                                                                                                               298Credit Value Adjustments                                                                                         302Conclusions                                                                                                                    305Further Reading                                                                                                             305","Normality Tests                                                                                                              308Benchmark Case                                                                                                         309Real-World Data                                                                                                         317Portfolio Optimization                                                                                                  322The Data                                                                                                                      323The Basic Theory                                                                                                       324Portfolio Optimizations                                                                                            328Efficient Frontier                                                                                                        330Capital Market Line                                                                                                   332Principal Component Analysis                                                                                    335The DAX Index and Its 30 Stocks                                                                            336Applying PCA                                                                                                             337Constructing a PCA Index                                                                                        338Bayesian Regression                                                                                                      341Bayes's Formula                                                                                                           341PyMC3                                                                                                                         342Introductory Example                                                                                               343Real Data                                                                                                                      347Conclusions                                                                                                                    355Further Reading                                                                                                             355","Basic Spreadsheet Interaction                                                                                      358Generating Workbooks (.xls)                                                                                   359Generating Workbooks (.xslx)                                                                                 360Reading from Workbooks                                                                                         362Using OpenPyxl                                                                                                          364Using pandas for Reading and Writing                                                                   366Scripting Excel with Python                                                                                         369Installing DataNitro                                                                                                   369Working with DataNitro                                                                                           370xlwings                                                                                                                             379","Conclusions                                                                                                                    379Further Reading                                                                                                             380","Object Orientation                                                                                                         381Basics of Python Classes                                                                                            382Simple Short Rate Class                                                                                             387Cash Flow Series Class                                                                                               391Graphical User Interfaces                                                                                             393Short Rate Class with GUI                                                                                        394Updating of Values                                                                                                     396Cash Flow Series Class with GUI                                                                             398Conclusions                                                                                                                    401Further Reading                                                                                                             401","Web Basics                                                                                                                      404ftplib                                                                                                                             405httplib                                                                                                                           407urllib                                                                                                                             408Web Plotting                                                                                                                   411Static Plots                                                                                                                   411Interactive Plots                                                                                                          414Real-Time Plots                                                                                                          417Rapid Web Applications                                                                                               424Traders' Chat Room                                                                                                   426Data Modeling                                                                                                            426The Python Code                                                                                                       427Templating                                                                                                                   434Styling                                                                                                                          440Web Services                                                                                                                   442The Financial Model                                                                                                  443The Implementation                                                                                                  445Conclusions                                                                                                                    451Further Reading                                                                                                             452","Fundamental Theorem of Asset Pricing                                                                     455A Simple Example                                                                                                      456","The General Results                                                                                                   457Risk-Neutral Discounting                                                                                             458Modeling and Handling Dates                                                                                 458Constant Short Rate                                                                                                   460Market Environments                                                                                                   462Conclusions                                                                                                                    465Further Reading                                                                                                             466","Random Number Generation                                                                                      468Generic Simulation Class                                                                                              470Geometric Brownian Motion                                                                                       473The Simulation Class                                                                                                 474A Use Case                                                                                                                   476Jump Diffusion                                                                                                               478The Simulation Class                                                                                                 478A Use Case                                                                                                                   481Square-Root Diffusion                                                                                                  482The Simulation Class                                                                                                 483A Use Case                                                                                                                   485Conclusions                                                                                                                    486Further Reading                                                                                                             487","Generic Valuation Class                                                                                                489European Exercise                                                                                                         493The Valuation Class                                                                                                   494A Use Case                                                                                                                   496American Exercise                                                                                                         500Least-Squares Monte Carlo                                                                                       501The Valuation Class                                                                                                   502A Use Case                                                                                                                   504Conclusions                                                                                                                    507Further Reading                                                                                                             509","Derivatives Positions                                                                                                     512The Class                                                                                                                     512A Use Case                                                                                                                   514Derivatives Portfolios                                                                                                    515The Class                                                                                                                     516A Use Case                                                                                                                   520","Conclusions                                                                                                                    525Further Reading                                                                                                             527","The VSTOXX Data                                                                                                        530VSTOXX Index Data                                                                                                 530VSTOXX Futures Data                                                                                              531VSTOXX Options Data                                                                                             533Model Calibration                                                                                                          534Relevant Market Data                                                                                                535Option Modeling                                                                                                        536Calibration Procedure                                                                                               538American Options on the VSTOXX                                                                           542Modeling Option Positions                                                                                       543The Options Portfolio                                                                                                544Conclusions                                                                                                                    545Further Reading                                                                                                             546","Not too long ago, Python as a programming language and platform technology wasconsidered exotic-if not completely irrelevant-in the financial industry. By contrast,in 2014 there are many examples of large financial institutions-like Bank of AmericaMerrill Lynch with its Quartz project, or JP Morgan Chase with the Athena project-that strategically use Python alongside other established technologies to build, enhance,and maintain some of their core IT systems. There is also a multitude of larger andsmaller hedge funds that make heavy use of Python's capabilities when it comes to ef%E2%80%90ficient financial application development and productive financial analytics efforts.Similarly, many of today's Master of Financial Engineering programs (or programsawarding similar degrees) use Python as one of the core languages for teaching thetranslation of quantitative finance theory into executable computer code. Educationalprograms and trainings targeted to finance professionals are also increasingly incor%E2%80%90porating Python into their curricula. Some now teach it as the main implementationlanguage.There are many reasons why Python has had such recent success and why it seems itwill continue to do so in the future. Among these reasons are its syntax, the ecosystemof scientific and data analytics libraries available to developers using Python, its ease ofintegration with almost any other technology, and its status as open source. (See Chap%E2%80%90ter 1 for a few more insights in this regard.)For that reason, there is an abundance of good books available that teach Python fromdifferent angles and with different focuses. This book is one of the first to introduce andteach Python for finance-in particular, for quantitative finance and for financial ana%E2%80%90lytics. The approach is a practical one, in that implementation and illustration comebefore theoretical details, and the big picture is generally more focused on than the mostarcane parameterization options of a certain class or function.Most of this book has been written in the powerful, interactive, browser-based IPythonNotebook environment (explained in more detail in Chapter 2). This makes it possible","to provide the reader with executable, interactive versions of almost all examples usedin this book.Those who want to immediately get started with a full-fledged, interactive financialanalytics environment for Python (and, for instance, R and Julia) should go to http://oreilly.quant-platform.com and try out the Python Quant Platform (in combinationwith the IPython Notebookfiles and code that come with this book). You should alsohave a look at DX analytics, a Python-based financial analytics library. My other book,Derivatives Analytics with Python(Wiley Finance), presents more details on the theoryand numerical methods for advanced derivatives analytics. It also provides a wealth ofreadily usable Pythoncode. Further material, and, in particular, slide decks and videosof talks about Python for Quant Finance can be found on my private website.If you want to get involved in Pythonfor Quant Finance community events, there areopportunities in the financial centers of the world. For example, I myself (co)organizemeetup groups with this focus in London (cf. http://www.meetup.com/Python-for-Quant-Finance-London/) and New York City (cf. http://www.meetup.com/Python-for-Quant-Finance-NYC/). There are also For Python Quants conferences and workshopsseveral times a year (cf. http://forpythonquants.com and http://pythonquants.com).I am really excited that Python has established itself as an important technology in thefinancial industry. I am also sure that it will play an even more important role there inthe future, in fields like derivatives and risk analytics or high performance computing.My hope is that this book will help professionals, researchers, and students alike makethe most of Python when facing the challenges of this fascinating field.","The following typographical conventions are used in this book:ItalicIndicates new terms, URLs, and email addresses.Constant widthUsed for program listings, as well as within paragraphs to refer to software packages,programming languages, file extensions, filenames, program elements such as vari%E2%80%90able or function names, databases, data types, environment variables, statements,and keywords.Constant width italicShows text that should be replaced with user-supplied values or by values deter%E2%80%90mined by context.","This element signifies a tip or suggestion.This element indicates a warning or caution.","Supplemental material (in particular, IPython Notebooks and Python scripts/modules)is available for download at http://oreilly.quant-platform.com.This book is here to help you get your job done. In general, if example code is offeredwith this book, you may use it in your programs and documentation. You do not needto contact us for permission unless you're reproducing a significant portion of the code.For example, writing a program that uses several chunks of code from this book doesnot require permission. Selling or distributing a CD-ROM of examples from O'Reillybooks does require permission. Answering a question by citing this book and quotingexample code does not require permission. Incorporating a significant amount of ex%E2%80%90ample code from this book into your product's documentation does require permission.We appreciate, but do not require, attribution. An attribution usually includes the title,author, publisher, and ISBN. For example: %E2%80%9CPython for Finance by Yves Hilpisch (O'Reil%E2%80%90ly). Copyright 2015 Yves Hilpisch, 978-1-491-94528-5.%E2%80%9DIf you feel your use of code examples falls outside fair use or the permission given above,feel free to contact us at permissions@oreilly.com.","Safari Books Online is an on-demand digital library thatdeliversexpertcontentinbothbookandvideoformfromthe world's leading authors in technology and business.Technology professionals, software developers, web designers, and business andcreative professionals use Safari Books Online as their primary resource for research,problem solving, learning, and certification training.Safari Books Online offers a range of plans and pricing for enterprise, government,education, and individuals.","Members have access to thousands of books, training videos, and prepublication manu%E2%80%90scripts in one fully searchable database from publishers like O'Reilly Media, PrenticeHall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que, PeachpitPress, Focal Press, Cisco Press, John Wiley %26 Sons, Syngress, Morgan Kaufmann, IBMRedbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill,Jones %26 Bartlett, Course Technology, and hundreds more. For more information aboutSafari Books Online, please visit us online.","Please address comments and questions concerning this book to the publisher:O'Reilly Media, Inc.1005 Gravenstein Highway NorthSebastopol, CA 95472800-998-9938 (in the United States or Canada)707-829-0515 (international or local)707-829-0104 (fax)We have a web page for this book, where we list errata, examples, and any additionalinformation. You can access this page at http://bit.ly/python-finance.To comment or ask technical questions about this book, send email to bookquestions@oreilly.com.For more information about our books, courses, conferences, and news, see our websiteat http://www.oreilly.com.Find us on Facebook: http://facebook.com/oreillyFollow us on Twitter: http://twitter.com/oreillymediaWatch us on YouTube: http://www.youtube.com/oreillymedia","I want to thank all those who helped to make this book a reality, in particular those whohave provided honest feedback or even completely worked out examples, like BenLerner, James Powell, Michael Schwed, Thomas Wiecki or Felix Zumstein. Similarly, Iwould like to thank reviewers Hugh Brown, Jennifer Pierce, Kevin Sheppard, and GalenWilkerson. The book benefited from their valuable feedback and the many suggestions.The book has also benefited significantly as a result of feedback I received from theparticipants of the many conferences and workshops I was able to present at in 2013and 2014: PyData, For Python Quants, Big Data in Quant Finance, EuroPython, Euro%E2%80%90Scipy, PyCon DE, PyCon Ireland, Parallel Data Analysis, Budapest BI Forum and","CodeJam. I also got valuable feedback during my many presentations at Pythonmeetupsin Berlin, London, and New York City.Last but not least, I want to thank my family, which fully accepts that I do what I lovedoing most and this, in general, rather intensively. Writing and finishing a book of thislength over the course of a year requires a large time commitment-on top of my usuallyheavy workload and packed travel schedule-and makes it necessary to sit sometimesmore hours in solitude in front the computer than expected. Therefore, thank you San%E2%80%90dra, Lilli, and Henry for your understanding and support. I dedicate this book to mylovely wife Sandra, who is the heart of our family.Yves Saarland, November 2014","This part introduces Python for finance. It consists of three chapters:%E2%80%A2Chapter 1 briefly discusses Python in general and argues why Python is indeed wellsuited to address the technological challenges in the finance industry and in finan%E2%80%90cial (data) analytics.%E2%80%A2Chapter 2, on Pythoninfrastructure and tools, is meant to provide a concise over%E2%80%90view of the most important things you have to know to get started with interactiveanalytics and application development in Python%3B the related Appendix A surveyssome selected best practices for Python development.%E2%80%A2Chapter 3 immediately dives into three specific financial examples%3B it illustrates howto calculate implied volatilities of options with Python, how to simulate a financialmodel with Python and the array library NumPy, and how to implement a backtestingfor a trend-based investment strategy. This chapter should give the reader a feelingfor what it means to use Pythonfor financial analytics-details are not that impor%E2%80%90tant at this stage%3B they are all explained in Part II.","Banks are essentially technology firms.- Hugo Banziger","Python is a high-level, multipurpose programming language that is used in a wide rangeof domains and technical fields. On the Python website you find the following executivesummary (cf. https://www.python.org/doc/essays/blurb):Python is an interpreted, object-oriented, high-level programming language with dy%E2%80%90namic semantics. Its high-level built in data structures, combined with dynamic typingand dynamic binding, make it very attractive for Rapid Application Development, as wellas for use as a scripting or glue language to connect existing components together.Python's simple, easy to learn syntax emphasizes readability and therefore reduces thecost of program maintenance. Python supports modules and packages, which encouragesprogram modularity and code reuse. The Python interpreter and the extensive standardlibrary are available in source or binary form without charge for all major platforms, andcan be freely distributed.This pretty well describes whyPythonhas evolved into one of the major programminglanguages as of today. Nowadays, Pythonis used by the beginner programmer as wellas by the highly skilled expert developer, at schools, in universities, at web companies,in large corporations and financial institutions, as well as in any scientific field.Among others, Python is characterized by the following features:Open sourcePython and the majority of supporting libraries and tools available are open sourceand generally come with quite flexible and open licenses.","InterpretedThe reference CPython implementation is an interpreter of the language that trans%E2%80%90lates Python code at runtime to executable byte code.MultiparadigmPython supports different programming and implementation paradigms, such asobject orientation and imperative, functional, or procedural programming.MultipurposePython can be used for rapid, interactive code development as well as for buildinglarge applications%3B it can be used for low-level systems operations as well as for high-level analytics tasks.Cross-platformPython is available for the most important operating systems, such as Windows,Linux, and Mac OS%3B it is used to build desktop as well as web applications%3B it can beused on the largest clusters and most powerful servers as well as on such smalldevices as the Raspberry Pi (cf. http://www.raspberrypi.org).Dynamically typedTypes in Pythonare in general inferred during runtime and not statically declaredas in most compiled languages.Indentation awareIn contrast to the majority of other programming languages, Python uses inden%E2%80%90tation for marking code blocks instead of parentheses, brackets, or semicolons.Garbage collectingPython has automated garbage collection, avoiding the need for the programmerto manage memory.When it comes to Python syntax and what Python is all about, Python EnhancementProposal 20-i.e., the so-called %E2%80%9CZen of Python%E2%80%9D-provides the major guidelines. It canbe accessed from every interactive shell with the command import this:%24 ipythonPython 2.7.6 %7CAnaconda 1.9.1 (x86_64)%7C (default, Jan 10 2014, 11:23:15)Type %22copyright%22, %22credits%22 or %22license%22 for more information.IPython 2.0.0--An enhanced Interactive Python.%3F         -%3E Introduction and overview of IPython's features.%25quickref -%3E Quick reference.help      -%3E Python's own help system.object%3F   -%3E Details about 'object', use 'object%3F%3F' for extra details.In%5B1%5D:importthisThe Zen of Python, by Tim Peters","Beautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren't special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one--and preferably only one--obvious way to do it.Although that way may not be obvious at first unless you're Dutch.Now is better than never.Although never is often better than *right* now.If the implementation is hard to explain, it's a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea--let's do more of those!","Although Python might still have the appeal of something new to some people, it hasbeen around for quite a long time. In fact, development efforts began in the 1980s byGuido van Rossum from the Netherlands. He is still active in Python development andhas been awarded the title of Benevolent Dictator for Lifeby the Python community (cf.http://en.wikipedia.org/wiki/History_of_Python). The following can be consideredmilestones in the development of Python:%E2%80%A2"," released in 1991 (first release)%E2%80%A2"," released in 1994%E2%80%A2"," released in 2000%E2%80%A2"," released in 2008%E2%80%A2"," released in 2010%E2%80%A2"," released in 2008%E2%80%A2"," released in 2010%E2%80%A2"," released in 2014It is remarkable, and sometimes confusing to Python newcomers, that there are twomajor versions available, still being developed and, more importantly, in parallel usesince 2008. As of this writing, this will keep on for quite a while since neither is there100%25 code compatibility between the versions, nor are all popular libraries available forPython 3.x. The majority of code available and in production is still Python 2.6/2.7,","and this book is based on the 2.7.x version, although the majority of code examplesshould work with versions 3.x as well.","A major feature of Python as an ecosystem, compared to just being a programminglanguage, is the availability of a large number of libraries and tools. These libraries andtools generally have to be imported when needed (e.g., a plotting library) or have to bestarted as a separate system process (e.g., a Python development environment). Im%E2%80%90porting means making a library available to the current namespace and the currentPython interpreter process.Python itself already comes with a large set of libraries that enhance the basic interpreterin different directions. For example, basic mathematical calculations can be donewithout any importing, while more complex mathematical functions need to be im%E2%80%90ported through the math library:In%5B2%5D:100*2.5%2B50Out%5B2%5D: 300.0In%5B3%5D:log(1)...NameError: name 'log' is not definedIn%5B4%5D:frommathimport*In%5B5%5D:log(1)Out%5B5%5D: 0.0Although the so-called %E2%80%9Cstar import%E2%80%9D (i.e., the practice of importing everything from alibrary via from library import *) is sometimes convenient, one should generally usean alternative approach that avoids ambiguity with regard to name spaces and rela%E2%80%90tionships of functions to libraries. This then takes on the form:In%5B6%5D:importmathIn%5B7%5D:math.log(1)Out%5B7%5D: 0.0While math is a standard Pythonlibrary available with any installation, there are manymore libraries that can be installed optionally and that can be used in the very samefashion as the standard libraries. Such libraries are available from different (web) sour%E2%80%90ces. However, it is generally advisable to use a Pythondistribution that makes sure thatall libraries are consistent with each other (see Chapter 2 for more on this topic).","The code examples presented so far all use IPython (cf. http://www.ipython.org), whichis probably the most popular interactive development environment (IDE) for Python.Although it started out as an enhanced shell only, it today has many features typicallyfound in IDEs (e.g., support for profiling and debugging). Those features missing aretypically provided by advanced text/code editors, like Sublime Text (cf. http://www.sublimetext.com). Therefore, it is not unusual to combine IPythonwith one's text/code editor of choice to form the basic tool set for a Python development process.IPythonis also sometimes called the killer applicationof the Pythonecosystem. It en%E2%80%90hances the standard interactive shell in many ways. For example, it provides improvedcommand-line history functions and allows for easy object inspection. For instance, thehelp text for a function is printed by just adding a %3F behind the function name(adding %3F%3F will provide even more information):In%5B8%5D:math.log%3FType:       builtin_function_or_methodString Form:%3Cbuilt-in function log%3EDocstring:log(x%5B, base%5D)Return the logarithm of x to the given base.If the base not specified, returns the natural logarithm (base e) of x.In%5B9%5D:IPythoncomes in three different versions: a shellversion, one based on a QT graphicaluser interface (the QT console), and a browser-based version (the Notebook). This isjust meant as a teaser%3B there is no need to worry about the details now since Chapter 2introduces IPython in more detail.","Pythondoes not only appeal to professional software developers%3B it is also of use for thecasual developer as well as for domain experts and scientific developers.Professional software developers find all that they need to efficiently build large appli%E2%80%90cations. Almost all programming paradigms are supported%3B there are powerful devel%E2%80%90opment tools available%3B and any task can, in principle, be addressed with Python. Thesetypes of users typically build their own frameworks and classes, also work on the fun%E2%80%90damental Python and scientific stack, and strive to make the most of the ecosystem.Scientific developers or domain expertsare generally heavy users of certain libraries andframeworks, have built their own applications that they enhance and optimize over time,and tailor the ecosystem to their specific needs. These groups of users also generallyengage in longer interactive sessions, rapidly prototyping new code as well as exploringand visualizing their research and/or domain data sets.","1.Python, for example, is a major language used in the Master of Financial Engineering program at BaruchCollege of the City University of New York (cf. http://mfe.baruch.cuny.edu).2.Cf. http://wiki.python.org/moin/BeginnersGuide, where you will find links to many valuable resources forboth developers and nondevelopers getting started with Python.Casual programmers like to use Python generally for specific problems they know thatPython has its strengths in. For example, visiting the gallery page of matplotlib, copyinga certain piece of visualization code provided there, and adjusting the code to theirspecific needs might be a beneficial use case for members of this group.There is also another important group of Python users: beginner programmers, i.e., thosethat are just starting to program. Nowadays, Python has become a very popular languageat universities, colleges, and even schools to introduce students to programming.1Amajor reason for this is that its basic syntax is easy to learn and easy to understand, evenfor the nondeveloper. In addition, it is helpful that Python supports almost all pro%E2%80%90gramming styles.2","There is a certain set of libraries that is collectively labeled the scientific stack. This stackcomprises, among others, the following libraries:NumPyNumPy provides a multidimensional array object to store homogenous or hetero%E2%80%90geneous data%3B it also provides optimized functions/methods to operate on this arrayobject.SciPySciPy is a collection of sublibraries and functions implementing important stan%E2%80%90dard functionality often needed in science or finance%3B for example, you will findfunctions for cubic splines interpolation as well as for numerical integration.matplotlibThis is the most popular plotting and visualization library for Python, providingboth 2D and 3D visualization capabilities.PyTablesPyTables is a popular wrapper for the HDF5data storage library (cf. http://www.hdfgroup.org/HDF5/)%3B it is a library to implement optimized, disk-based I/Ooperations based on a hierarchical database/file format.","pandaspandas builds on NumPyand provides richer classes for the management and anal%E2%80%90ysis of time series and tabular data%3B it is tightly integrated with matplotlibforplotting and PyTables for data storage and retrieval.Depending on the specific domain or problem, this stack is enlarged by additional li%E2%80%90braries, which more often than not have in common that they build on top of one ormore of these fundamental libraries. However, the least common denominator or basicbuilding block in general is the NumPyndarray class (cf. Chapter 4).Taking Python as a programming language alone, there are a number of other languagesavailable that can probably keep up with its syntax and elegance. For example, Ruby isquite a popular language often compared to Python. On the language's website you findthe following description:A dynamic, open source programming language with a focus on simplicity and produc%E2%80%90tivity. It has an elegant syntax that is natural to read and easy to write.The majority of people using Python would probably also agree with the exact samestatement being made about Python itself. However, what distinguishes Python for manyusers from equally appealing languages like Ruby is the availability of the scientific stack.This makes Python not only a good and elegant language to use, but also one that iscapable of replacing domain-specific languages and tool sets like Matlab or R. In addi%E2%80%90tion, it provides by default anything that you would expect, say, as a seasoned webdeveloper or systems administrator.","Now that we have some rough ideas of what Python is all about, it makes sense to stepback a bit and to briefly contemplate the role of technology in finance. This will put usin a position to better judge the role Python already plays and, even more importantly,will probably play in the financial industry of the future.In a sense, technology per se is nothing specialto financial institutions (as compared,for instance, to industrial companies) or to the finance function (as compared to othercorporate functions, like logistics). However, in recent years, spurred by innovation andalso regulation, banks and other financial institutions like hedge funds have evolvedmore and more into technology companies instead of being just financial intermedia%E2%80%90ries. Technology has become a major asset for almost any financial institution aroundthe globe, having the potential to lead to competitive advantages as well as disadvantages.Some background information can shed light on the reasons for this development.","Banks and financial institutions together form the industry that spends the most ontechnology on an annual basis. The following statement therefore shows not only thattechnology is important for the financial industry, but that the financial industry is alsoreally important to the technology sector:Banks will spend 4.2%25 more on technology in 2014 than they did in 2013, according toIDC analysts. Overall IT spend in financial services globally will exceed %24430 billion in2014 and surpass %24500 billion by 2020, the analysts say.- Crosman 2013Large, multinational banks today generally employ thousands of developers that main%E2%80%90tain existing systems and build new ones. Large investment banks with heavy techno%E2%80%90logical requirements show technology budgets often of several billion USD per year.","The technological development has also contributed to innovations and efficiency im%E2%80%90provements in the financial sector:Technological innovations have contributed significantly to greater efficiency in the de%E2%80%90rivatives market. Through innovations in trading technology, trades at Eurex are todayexecuted much faster than ten years ago despite the strong increase in trading volumeand the number of quotes %E2%80%A6 These strong improvements have only been possible due tothe constant, high IT investments by derivatives exchanges and clearing houses.- Deutsche B%C3%B6rse Group 2008As a side effect of the increasing efficiency, competitive advantages must often be lookedfor in ever more complex products or transactions. This in turn inherently increasesrisks and makes risk management as well as oversight and regulation more and moredifficult. The financial crisis of 2007 and 2008 tells the story of potential dangers re%E2%80%90sulting from such developments. In a similar vein, %E2%80%9Calgorithms and computers gonewild%E2%80%9D also represent a potential risk to the financial markets%3B this materialized dramat%E2%80%90ically in the so-called flash crash of May 2010, where automated selling led to largeintraday drops in certain stocks and stock indices (cf. http://en.wikipedia.org/wiki/2010_Flash_Crash).","On the one hand, technology advances reduce cost over time, ceteris paribus. On theother hand, financial institutions continue to invest heavily in technology to both gainmarket share and defend their current positions. To be active in certain areas in financetoday often brings with it the need for large-scale investments in both technology andskilled staff. As an example, consider the derivatives analytics space (see also the casestudy in Part III of the book):","Aggregated over the total software lifecycle, firms adopting in-house strategies for OTC%5Bderivatives%5D pricing will require investments between %2425 million and %2436 million aloneto build, maintain, and enhance a complete derivatives library.- Ding 2010Not only is it costly and time-consuming to build a full-fledged derivatives analyticslibrary, but you also need to have enough experts to do so. And these experts have tohave the right tools and technologies available to accomplish their tasks.Another quote about the early days of Long-Term Capital Management (LTCM), for%E2%80%90merly one of the most respected quantitative hedge funds-which, however, went bustin the late 1990s-further supports this insight about technology and talent:Meriwether spent %2420 million on a state-of-the-art computer system and hired a crackteam of financial engineers to run the show at LTCM, which set up shop in Greenwich,Connecticut. It was risk management on an industrial level.- Patterson 2010The same computing power that Meriwether had to buy for millions of dollars is todayprobably available for thousands. On the other hand, trading, pricing, and risk man%E2%80%90agement have become so complex for larger financial institutions that today they needto deploy IT infrastructures with tens of thousands of computing cores.","There is one dimension of the finance industry that has been influenced most by tech%E2%80%90nological advances: the speed and frequency with which financial transactions are de%E2%80%90cided and executed. The recent book by Lewis (2014) describes so-called flash trading-i.e., trading at the highest speeds possible-in vivid detail.On the one hand, increasing data availability on ever-smaller scales makes it necessaryto react in real time. On the other hand, the increasing speed and frequency of tradinglet the data volumes further increase. This leads to processes that reinforce each otherand push the average time scale for financial transactions systematically down:Renaissance's Medallion fund gained an astonishing 80 percent in 2008, capitalizing onthe market's extreme volatility with its lightning-fast computers. Jim Simons was thehedge fund world's top earner for the year, pocketing a cool %242.5 billion.- Patterson 2010Thirty years' worth of daily stock price data for a single stock represents roughly 7,500quotes. This kind of data is what most of today's finance theory is based on. For example,theories like the modern portfolio theory (MPT), the capital asset pricing model(CAPM), and value-at-risk (VaR) all have their foundations in daily stock price data.In comparison, on a typical trading day the stock price of Apple Inc. (AAPL) is quotedaround 15,000 times-two times as many quotes as seen for end-of-day quoting over atime span of 30 years. This brings with it a number of challenges:","Data processingIt does not suffice to consider and process end-of-day quotes for stocks or otherfinancial instruments%3B %E2%80%9Ctoo much%E2%80%9D happens during the day for some instrumentsduring 24 hours for 7 days a week.Analytics speedDecisions often have to be made in milliseconds or even faster, making it necessaryto build the respective analytics capabilities and to analyze large amounts of datain real time.Theoretical foundationsAlthough traditional finance theories and concepts are far from being perfect, theyhave been well tested (and sometimes well rejected) over time%3B for the millisecondscales important as of today, consistent concepts and theories that have proven tobe somewhat robust over time are still missing.All these challenges can in principle only be addressed by modern technology. Some%E2%80%90thing that might also be a little bit surprising is that the lack of consistent theories oftenis addressed by technological approaches, in that high-speed algorithms exploit marketmicrostructure elements (e.g., order flow, bid-ask spreads) rather than relying on somekind of financial reasoning.","There is one discipline that has seen a strong increase in importance in the financeindustry: financial and data analytics. This phenomenon has a close relationship to theinsight that speeds, frequencies, and data volumes increase at a rapid pace in the in%E2%80%90dustry. In fact, real-time analytics can be considered the industry's answer to this trend.Roughly speaking, %E2%80%9Cfinancial and data analytics%E2%80%9D refers to the discipline of applyingsoftware and technology in combination with (possibly advanced) algorithms andmethods to gather, process, and analyze data in order to gain insights, to make decisions,or to fulfill regulatory requirements, for instance. Examples might include the estima%E2%80%90tion of sales impacts induced by a change in the pricing structure for a financial productin the retail branch of a bank. Another example might be the large-scale overnightcalculation of credit value adjustments (CVA) for complex portfolios of derivativestrades of an investment bank.There are two major challenges that financial institutions face in this context:Big dataBanks and other financial institutions had to deal with massive amounts of dataeven before the term %E2%80%9Cbig data%E2%80%9D was coined%3B however, the amount of data that hasto be processed during single analytics tasks has increased tremendously over time,demanding both increased computing power and ever-larger memory and storagecapacities.","3.Chapter 8provides an example for the benefits of using modern GPGPUs in the context of the generation ofrandom numbers.Real-time economyIn the past, decision makers could rely on structured, regular planning, decision,and (risk) management processes, whereas they today face the need to take care ofthese functions in real time%3B several tasks that have been taken care of in the pastvia overnight batch runs in the back office have now been moved to the front officeand are executed in real time.Again, one can observe an interplay between advances in technology and financial/business practice. On the one hand, there is the need to constantly improve analyticsapproaches in terms of speed and capability by applying modern technologies. On theother hand, advances on the technology side allow new analytics approaches that wereconsidered impossible (or infeasible due to budget constraints) a couple of years or evenmonths ago.One major trend in the analytics space has been the utilization of parallel architectureson the CPU (central processing unit) side and massively parallel architectures on theGPGPU (general-purpose graphical processing units) side. Current GPGPUs often havemore than 1,000 computing cores, making necessary a sometimes radical rethinking ofwhat parallelism might mean to different algorithms. What is still an obstacle in thisregard is that users generally have to learn new paradigms and techniques to harnessthe power of such hardware.3","The previous section describes some selected aspects characterizing the role of tech%E2%80%90nology in finance:%E2%80%A2Costs for technology in the finance industry%E2%80%A2Technology as an enabler for new business and innovation%E2%80%A2Technology and talent as barriers to entry in the finance industry%E2%80%A2Increasing speeds, frequencies, and data volumes%E2%80%A2The rise of real-time analyticsIn this section, we want to analyze how Python can help in addressing several of thechallenges implied by these aspects. But first, on a more fundamental level, let us ex%E2%80%90amine Python for finance from a language and syntax standpoint.","Most people who make their first steps with Pythonin a finance context may attack analgorithmic problem. This is similar to a scientist who, for example, wants to solve adifferential equation, wants to evaluate an integral, or simply wants to visualize somedata. In general, at this stage, there is only little thought spent on topics like a formaldevelopment process, testing, documentation, or deployment. However, this especiallyseems to be the stage when people fall in love with Python. A major reason for this mightbe that the Python syntax is generally quite close to the mathematical syntax used todescribe scientific problems or financial algorithms.We can illustrate this phenomenon by a simple financial algorithm, namely the valuationof a European call option by Monte Carlo simulation. We will consider a Black-Scholes-Merton (BSM) setup (see also Chapter 3) in which the option's underlying risk factorfollows a geometric Brownian motion.Suppose we have the following numerical parameter values for the valuation:%E2%80%A2Initial stock index level S0 %3D 100%E2%80%A2Strike price of the European call option K %3D 105%E2%80%A2Time-to-maturity T %3D 1 year%E2%80%A2Constant, riskless short rate r %3D 5%25%E2%80%A2Constant volatility %ED%9C%8E %3D 20%25In the BSM model, the index level at maturity is a random variable, given by Equation 1-1with z being a standard normally distributed random variable.Equation 1-1. Black-Scholes-Merton (1973) index level at maturityST%3DS0expr%E2%88%9212%CF%832T%2B%CF%83TzThe following is an algorithmic description of the Monte Carlo valuation procedure:1.Draw I(pseudo)random numbers z(i), i%E2%88%88 %7B1, 2, %E2%80%A6, I%7D, from the standard normaldistribution.2.Calculate all resulting index levels at maturity ST(i) for given z(i) and Equation 1-1.3.Calculate all inner values of the option at maturity as hT(i) %3D max(ST(i) %E2%80%93 K,0).4.Estimate the option present value via the Monte Carlo estimator given inEquation 1-2.","4.The output of such a numerical simulation depends on the pseudorandom numbers used. Therefore, resultsmight vary.Equation 1-2. Monte Carlo estimator for European option C 0%E2%89%88e%E2%88%92rT1I%E2%88%91IhTiWe are now going to translate this problem and algorithm into Pythoncode. The readermight follow the single steps by using, for example, IPython-this is, however, not reallynecessary at this stage.First, let us start with the parameter values. This is really easy:S0%3D100.K%3D105.T%3D1.0r%3D0.05sigma%3D0.2Next, the valuation algorithm. Here, we will for the first time use NumPy, which makeslife quite easy for our second task:fromnumpyimport*I%3D100000z%3Drandom.standard_normal(I)ST%3DS0*exp((r-0.5*sigma**2)*T%2Bsigma*sqrt(T)*z)hT%3Dmaximum(ST-K,0)C0%3Dexp(-r*T)*sum(hT)/IThird, we print the result:print%22Value of the European Call Option %255.3f%22%25C0The output might be:4Value of the European Call Option 8.019Three aspects are worth highlighting:SyntaxThe Python syntax is indeed quite close to the mathematical syntax, e.g., when itcomes to the parameter value assignments.TranslationEvery mathematical and/or algorithmic statement can generally be translated intoa single line of Python code.","VectorizationOne of the strengths of NumPyis the compact, vectorized syntax, e.g., allowing for100,000 calculations within a single line of code.This code can be used in an interactive environment like IPython. However, code thatis meant to be reused regularly typically gets organized in so-called modules(orscripts), which are single Python (i.e., text) files with the suffix .py. Such a module couldin this case look like Example 1-1 and could be saved as a file named bsm_mcs_euro.py.Example 1-1. Monte Carlo valuation of European call option## Monte Carlo valuation of European call option# in Black-Scholes-Merton model# bsm_mcs_euro.py#importnumpyasnp# Parameter ValuesS0%3D100.# initial index levelK%3D105.# strike priceT%3D1.0# time-to-maturityr%3D0.05# riskless short ratesigma%3D0.2# volatilityI%3D100000# number of simulations# Valuation Algorithmz%3Dnp.random.standard_normal(I)# pseudorandom numbersST%3DS0*np.exp((r-0.5*sigma**2)*T%2Bsigma*np.sqrt(T)*z)# index values at maturityhT%3Dnp.maximum(ST-K,0)# inner values at maturityC0%3Dnp.exp(-r*T)*np.sum(hT)/I# Monte Carlo estimator# Result Outputprint%22Value of the European Call Option %255.3f%22%25C0The rather simple algorithmic example in this subsection illustrates that Python, withits very syntax, is well suited to complement the classic duo of scientific languages,English and Mathematics. It seems that adding Pythonto the set of scientific languagesmakes it more well rounded. We have%E2%80%A2"," for writing, talking about scientific and financial problems, etc.%E2%80%A2"," for concisely and exactly describing and modeling abstract aspects,algorithms, complex quantities, etc.%E2%80%A2"," for technically modeling and implementing abstract aspects, algorithms,complex quantities, etc.","ThereishardlyanyprogramminglanguagethatcomesasclosetomathematicalsyntaxasPython.NumericalalgorithmsarethereforesimpletotranslatefromthemathematicalrepresentationintothePythonicimplementation.Thismakesprototyping,development,and code maintenance in such areas quite efficient with Python.In some areas, it is common practice to use pseudocode and therewith to introduce afourth language family member. The role of pseudocode is to represent, for example,financial algorithms in a more technical fashion that is both still close to the mathe%E2%80%90matical representation and already quite close to the technical implementation. In ad%E2%80%90dition to the algorithm itself, pseudocode takes into account how computers work inprinciple.This practice generally has its cause in the fact that with most programming languagesthe technical implementation is quite %E2%80%9Cfar away%E2%80%9D from its formal, mathematical repre%E2%80%90sentation. The majority of programming languages make it necessary to include so manyelements that are only technically required that it is hard to see the equivalence betweenthe mathematics and the code.Nowadays, Python is often used in a pseudocode way since its syntax is almost analogousto the mathematics and since the technical %E2%80%9Coverhead%E2%80%9D is kept to a minimum. This isaccomplished by a number of high-level concepts embodied in the language that notonly have their advantages but also come in general with risks and/or other costs. How%E2%80%90ever, it is safe to say that with Python you can, whenever the need arises, follow the samestrict implementation and coding practices that other languages might require from theoutset. In that sense, Pythoncan provide the best of both worlds: high-level abstractionand rigorous implementation.","At a high level, benefits from using Python can be measured in three dimensions:EfficiencyHow can Python help in getting results faster, in saving costs, and in saving time%3FProductivityHow can Python help in getting more done with the same resources (people,assets, etc.)%3FQualityWhat does Python allow us to do that we could not do with alternative technologies%3FA discussion of these aspects can by nature not be exhaustive. However, it can highlightsome arguments as a starting point.","A field where the efficiency of Python becomes quite obvious is interactive data analytics.This is a field that benefits strongly from such powerful tools as IPython and librarieslike pandas.Consider a finance student, writing her master's thesis and interested in Google stockprices. She wants to analyze historical stock price information for, say, five years to seehow the volatility of the stock price has fluctuated over time. She wants to find evidencethat volatility, in contrast to some typical model assumptions, fluctuates over time andis far from being constant. The results should also be visualized. She mainly has to dothe following:%E2%80%A2Download Google stock price data from the Web.%E2%80%A2Calculate the rolling standard deviation of the log returns (volatility).%E2%80%A2Plot the stock price data and the results.These tasks are complex enough that not too long ago one would have considered themto be something for professional financial analysts. Today, even the finance student caneasily cope with such problems. Let us see how exactly this works-without worryingabout syntax details at this stage (everything is explained in detail in subsequentchapters).First, make sure to have available all necessary libraries:In%5B1%5D:importnumpyasnpimportpandasaspdimportpandas.io.dataaswebSecond, retrieve the data from, say, Google itself:In%5B2%5D:goog%3Dweb.DataReader('GOOG',data_source%3D'google',start%3D'3/14/2009',end%3D'4/14/2014')goog.tail()Out%5B2%5D:             Open    High    Low     Close   Volume        Date        2014-04-08  542.60  555.00  541.61  554.90  3152406        2014-04-09  559.62  565.37  552.95  564.14  3324742        2014-04-10  565.00  565.00  539.90  540.95  4027743        2014-04-11  532.55  540.00  526.53  530.60  3916171        2014-04-14  538.25  544.10  529.56  532.52  2568020        5 rows %C3%97 5 columnsThird, implement the necessary analytics for the volatilities:In%5B3%5D:goog%5B'Log_Ret'%5D%3Dnp.log(goog%5B'Close'%5D/goog%5B'Close'%5D.shift(1))goog%5B'Volatility'%5D%3Dpd.rolling_std(goog%5B'Log_Ret'%5D,window%3D252)*np.sqrt(252)","Fourth, plot the results. To generate an inline plot, we use the IPythonmagic command%25matplotlib with the option inline:In%5B4%5D:%25matplotlibinlinegoog%5B%5B'Close','Volatility'%5D%5D.plot(subplots%3DTrue,color%3D'blue',figsize%3D(8,6))Figure 1-1 shows the graphical result of this brief interactive session with IPython. Itcan be considered almost amazing that four lines of code suffice to implement threerather complex tasks typically encountered in financial analytics: data gathering, com%E2%80%90plex and repeated mathematical calculations, and visualization of results. This exampleillustrates that pandasmakes working with whole time series almost as simple as doingmathematical operations on floating-point numbers.Figure 1-1. Google closing prices and yearly volatilityTranslated to a professional finance context, the example implies that financial analystscan-when applying the right Pythontools and libraries, providing high-level abstrac%E2%80%90tion-focus on their very domain and not on the technical intrinsicalities. Analysts canreact faster, providing valuable insights almost in real time and making sure they areone step ahead of the competition. This example of increased efficiency can easily trans%E2%80%90late into measurable bottom-line effects.","In general, it is accepted that Pythonhas a rather concise syntax and that it is relativelyefficient to code with. However, due to the very nature of Python being an interpretedlanguage, the prejudicepersists that Pythongenerally is too slow for compute-intensivetasks in finance. Indeed, depending on the specific implementation approach, Python","can be really slow. But it does not have to be slow-it can be highly performing in almostany application area. In principle, one can distinguish at least three different strategiesfor better performance:ParadigmIn general, many different ways can lead to the same result in Python, but withrather different performance characteristics%3B %E2%80%9Csimply%E2%80%9D choosing the right way (e.g.,a specific library) can improve results significantly.CompilingNowadays, there are several performance libraries available that provide compiledversions of important functions or that compile Python code statically or dynami%E2%80%90cally (at runtime or call time) to machine code, which can be orders of magnitudefaster%3B popular ones are Cython and Numba.ParallelizationMany computational tasks, in particular in finance, can strongly benefit from par%E2%80%90allel execution%3B this is nothing special to Python but something that can easily beaccomplished with it.","Pythonperseisnotahigh-performancecomputingtechnology.However, Python has developed into an ideal platform to access cur%E2%80%90rentperformancetechnologies.Inthatsense,Pythonhasbecomesomething like a glue language for performance computing.Later chapters illustrate all three techniques in detail. For the moment, we want to stickto a simple, but still realistic, example that touches upon all three techniques.A quite common task in financial analytics is to evaluate complex mathematical ex%E2%80%90pressions on large arrays of numbers. To this end, Python itself provides everythingneeded:In%5B1%5D:loops%3D25000000frommathimport*a%3Drange(1,loops)deff(x):return3*log(x)%2Bcos(x)**2%25timeitr%3D%5Bf(x)forxina%5DOut%5B1%5D: 1 loops, best of 3: 15 s per loopThe Python interpreter needs 15 seconds in this case to evaluate the function f25,000,000 times.The same task can be implemented using NumPy, which provides optimized (i.e., pre-compiled), functions to handle such array-based operations:","In%5B2%5D:importnumpyasnpa%3Dnp.arange(1,loops)%25timeitr%3D3*np.log(a)%2Bnp.cos(a)**2Out%5B2%5D: 1 loops, best of 3: 1.69 s per loopUsing NumPy considerably reduces the execution time to 1.7 seconds.However, there is even a library specifically dedicated to this kind of task. It is callednumexpr, for %E2%80%9Cnumerical expressions.%E2%80%9D It compiles the expression to improve upon theperformance of NumPy's general functionality by, for example, avoiding in-memorycopies of arrays along the way:In%5B3%5D:importnumexprasnene.set_num_threads(1)f%3D'3 * log(a) %2B cos(a) ** 2'%25timeitr%3Dne.evaluate(f)Out%5B3%5D: 1 loops, best of 3: 1.18 s per loopUsing this more specialized approach further reduces execution time to 1.2 seconds.However, numexpr also has built-in capabilities to parallelize the execution of the re%E2%80%90spective operation. This allows us to use all available threads of a CPU:In%5B4%5D:ne.set_num_threads(4)%25timeitr%3Dne.evaluate(f)Out%5B4%5D: 1 loops, best of 3: 523 ms per loopThis brings execution time further down to 0.5 seconds in this case, with two cores andfour threads utilized. Overall, this is a performance improvement of 30 times. Note, inparticular, that this kind of improvement is possible without altering the basic problem/algorithm and without knowing anything about compiling and parallelization issues.The capabilities are accessible from a high level even by nonexperts. However, one hasto be aware, of course, of which capabilities exist.The example shows that Python provides a number of options to make more out ofexisting resources-i.e., to increase productivity. With the sequential approach, about21 mn evaluations per second are accomplished, while the parallel approach allows foralmost 48 mn evaluations per second-in this case simply by telling Python to use allavailable CPU threads instead of just one.","Efficiency in interactive analytics and performance when it comes to execution speedare certainly two benefits of Python to consider. Yet another major benefit of usingPython for finance might at first sight seem a bit subtler%3B at second sight it might presentitself as an important strategic factor. It is the possibility to use Python end to end, fromprototyping to production.","Today's practice in financial institutions around the globe, when it comes to financialdevelopment processes, is often characterized by a separated, two-step process. On theone hand, there are the quantitative analysts (%E2%80%9Cquants%E2%80%9D) responsible for model devel%E2%80%90opment and technical prototyping. They like to use tools and environments like Matlaband R that allow for rapid, interactive application development. At this stage of thedevelopment efforts, issues like performance, stability, exception management, sepa%E2%80%90ration of data access, and analytics, among others, are not that important. One is mainlylooking for a proof of concept and/or a prototype that exhibits the main desired featuresof an algorithm or a whole application.Once the prototype is finished, IT departments with their developers take over and areresponsible for translating the existing prototype code into reliable, maintainable, andperformant production code. Typically, at this stage there is a paradigm shift in thatlanguages like  C %2B%2B or Java are now used to fulfill the requirements for production. Also,a formal development process with professional tools, version control, etc. is applied.This two-step approach has a number of generally unintended consequences:InefficienciesPrototype code is not reusable%3B algorithms have to be implemented twice%3B redundantefforts take time and resources.Diverse skill setsDifferent departments show different skill sets and use different languages to im%E2%80%90plement %E2%80%9Cthe same things.%E2%80%9DLegacy codeCode is available and has to be maintained in different languages, often using dif%E2%80%90ferent styles of implementation (e.g., from an architectural point of view).Using Python, on the other hand, enables a streamlined end-to-end process from thefirst interactive prototyping steps to highly reliable and efficiently maintainable pro%E2%80%90duction code. The communication between different departments becomes easier. Thetraining of the workforce is also more streamlined in that there is only one major lan%E2%80%90guage covering all areas of financial application building. It also avoids the inherentinefficiencies and redundancies when using different technologies in different steps ofthe development process. All in all, Python can provide a consistent technological frame%E2%80%90work for almost all tasks in financial application development and algorithmimplementation.","Python as a language-but much more so as an ecosystem-is an ideal technologicalframework for the financial industry. It is characterized by a number of benefits, like anelegant syntax, efficient development approaches, and usability for prototyping and","production, among others. With its huge amount of available libraries and tools, Pythonseems to have answers to most questions raised by recent developments in the financialindustry in terms of analytics, data volumes and frequency, compliance, and regulation,as well as technology itself. It has the potential to provide a single, powerful, consistentframework with which to streamline end-to-end development and production effortseven across larger financial institutions.","There are two books available that cover the use of Python in finance:%E2%80%A2Fletcher, Shayne and Christopher Gardner (2009): Financial Modelling in Python.John Wiley %26 Sons, Chichester, England.%E2%80%A2Hilpisch, Yves (2015): Derivatives Analytics with Python. Wiley Finance, Chiches%E2%80%90ter, England. http://derivatives-analytics-with-python.com.The quotes in this chapter are taken from the following resources:%E2%80%A2Crosman, Penny (2013): %E2%80%9CTop 8 Ways Banks Will Spend Their 2014 IT Budgets.%E2%80%9DBank Technology News.%E2%80%A2Deutsche B%C3%B6rse Group (2008): %E2%80%9CThe Global Derivatives Market-An Introduction.%E2%80%9DWhite paper.%E2%80%A2Ding, Cubillas (2010): %E2%80%9COptimizing the OTC Pricing and Valuation Infrastructure.%E2%80%9DCelent study.%E2%80%A2Lewis, Michael (2014): Flash Boys. W. W. Norton %26 Company, New York.%E2%80%A2Patterson, Scott (2010): The Quants. Crown Business, New York.","1.They can, for example, in general be executed even on a Raspberry Pi for about 30 USD (cf. http://www.raspberrypi.org), although memory issues quickly arise for some applications. Nevertheless, this can beconsidered a rather low requirement when it comes to hardware.","Infrastructure is much more important than architecture.- Rem KoolhaasYou could say infrastructure is not everything, but without infrastructure everythingcan be nothing-be it in the real world or in technology. What do we mean then byinfrastructure%3F In principle, it is those hardware and software components that allowthe development and execution of a simple Python script or more complex Pythonapplications.However, this chapter does not go into detail with regard to hardware infrastructure,since all Python code and examples should be executable on almost any hardware.1Nordoes it discuss different operating systems, since the code should be executable on anyoperating system on which Python, in principle, is available. This chapter rather focuseson the following topics:DeploymentHow can I make sure to have everything needed available in a consistent fashionto deploy Python code and applications%3F This chapter introduces Anaconda, aPython distribution that makes deployment quite efficient, as well as the PythonQuant Platform, which allows for a web- and browser-based deployment.ToolsWhich tools shall I use for (interactive) Python development and data analytics%3FThe chapter introduces two of the most popular development environments forPython, namely IPython and Spyder.","2.For those who want to control which libraries and packages get installed, there is Miniconda, which comeswith a minimal Python installation only. Cf. http://conda.pydata.org/miniconda.html.3.There is also an Anacondaversion available that contains proprietary packages from Continuum Analyticscalled Accelerate. This commercial version, whose main goal is to improve the performance of typicaloperations with Python, has to be licensed.There is also Appendix A, on:Best practicesWhich best practices should I follow when developing Pythoncode%3F The appendixbriefly reviews fundamentals of, for example, Python code syntax anddocumentation.","This section shows how to deploy Pythonlocally (or on a server) as well as via the webbrowser.","A number of operating systems come with a version of Pythonand a number of addi%E2%80%90tional libraries already installed. This is true, for example, of Linux operating systems,which often rely on Python as their main language (for packaging, administration, etc.).However, in what follows we assume that Python is not installed or that we are installingan additional version of Python (in parallel to an existing one) using the Anacondadistribution.You can download Anacondafor your operating system from the website http://continuum.io/downloads. There are a couple of reasons to consider using Anacondafor Pythondeployment. Among them are:Libraries/packagesYou get more than 100 of the most important Python libraries and packages in asingle installation step%3B in particular, you get all these installed in a version-consistent manner (i.e., all libraries and packages work with each other).2Open sourceThe Anaconda distribution is free of charge in general,3 as are all libraries and pack%E2%80%90ages included in the distribution.Cross platformIt is available for Windows, Mac OS, and Linux platforms.","Separate installationIt installs into a separate directory without interfering with any existing installation%3Bno root/admin rights are needed.Automatic updatesLibraries and packages included in Anacondacan be (semi)automatically updatedvia free online repositories.Conda package managerThe package manager allows the use of multiple Pythonversions and multiple ver%E2%80%90sions of libraries in parallel (for experimentation or development/testing purposes)%3Bit also has great support for virtual environments.After having downloaded the installer for Anaconda, the installation in general is quiteeasy. On Windowsplatforms, just double-click the installer file and follow the instruc%E2%80%90tions. Under Linux, open a shell, change to the directory where the installer file is lo%E2%80%90cated, and type:%24 bash Anaconda-1.x.x-Linux-x86%5B_64%5D.shReplacing the file name with the respective name of your installer file. Then again followthe instructions. It is the same on an Apple computer%3B just type:%24 bash Anaconda-1.x.x-MacOSX-x86_64.shmaking sure you replace the name given here with the correct one. Alternatively, youcan use the graphical installer that is available.After the installation you have more than 100 libraries and packages available that youcan use immediately. Among the scientific and data analytics packages are those listedin Table 2-1.Table 2-1. Selected libraries and packages included in Anaconda","BitArrayObject types for arrays of BooleansCubesOLAPFramework for Online Analytical Processing (OLAP) applicationsDiscomapreduce implementation for distributed computingGdataImplementation of Google Data Protocolh5pyPython wrapper around HDF5 file formatHDF5File format for fast I/O operationsIPythonInteractive development environment (IDE)lxmlProcessing XML and HTML with PythonmatplotlibStandard 2D and 3D plotting libraryMPI4PyMessage Parsing Interface (MPI) implementation for parallel computationMPICH2Another MPI implementation","NetworkXBuilding and analyzing network models and algorithmsnumexprOptimized execution of numerical expressionsNumPyPowerful array class and optimized functions on itpandasEfficient handling of time series dataPyTablesHierarchical database using HDF5SciPyCollection of scientific functionsScikit-LearnMachine learning algorithmsSpyderPython IDE with syntax checking, debugging, and inspection capabilitiesstatsmodelsStatistical modelsSymPySymbolic computation and mathematicsTheanoMathematical expression compilerIf the installation procedure was successful, you should open a new terminal windowand should then be able, for example, to start the SpyderIDE by simply typing in theshell:%24 spyderAlternatively, you can start a Python session from the shell as follows:%24 pythonPython 2.7.6 %7CAnaconda 1.9.2 (x86_64)%7C (default, Feb 10 2014, 17:56:29)%5BGCC 4.0.1 (Apple Inc. build 5493)%5D on darwinType %22help%22, %22copyright%22, %22credits%22 or %22license%22 for more information.%3E%3E%3E exit()%24Anaconda by default installs, at the time of this writing, with Python 2.7.x. It alwayscomes with conda, the open source package manager. Useful information about thistool can be obtained by the command:%24 conda infoCurrent conda install:             platform : osx-64        conda version : 3.4.1       python version : 2.7.6.final.0     root environment : /Library/anaconda  (writable)  default environment : /Library/anaconda     envs directories : /Library/anaconda/envs        package cache : /Library/anaconda/pkgs         channel URLs : http://repo.continuum.io/pkgs/free/osx-64/                        http://repo.continuum.io/pkgs/pro/osx-64/          config file : None    is foreign system : False","%24conda allows one to search for libraries and packages, both locally and in available onlinerepositories:%24 conda search pytablesFetching package metadata: ..pytables                  .  2.4.0                np17py27_0  defaults                             2.4.0                np17py26_0  defaults                             2.4.0                np16py27_0  defaults                             2.4.0                np16py26_0  defaults                          .  3.0.0                np17py27_0  defaults                             3.0.0                np17py26_0  defaults                             3.0.0                np16py27_0  defaults                             3.0.0                np16py26_0  defaults                          .  3.0.0                np17py33_1  defaults                          .  3.0.0                np17py27_1  defaults                             3.0.0                np17py26_1  defaults                          .  3.0.0                np16py27_1  defaults                             3.0.0                np16py26_1  defaults                             3.1.0                np18py33_0  defaults                          *  3.1.0                np18py27_0  defaults                             3.1.0                np18py26_0  defaults                             3.1.1                np18py34_0  defaults                             3.1.1                np18py33_0  defaults                             3.1.1                np18py27_0  defaults                             3.1.1                np18py26_0  defaultsThe results contain those versions of PyTables that are available for download andinstallation in this case and that are installed (indicated by the asterisk). Similary, thelist command gives all locally installed packages that match a certain pattern. Thefollowing lists all packages that start with %E2%80%9Cpyt%E2%80%9D:%24 conda list %5Epyt# packages in environment at /Library/anaconda:#pytables                  3.1.0                np18py27_0pytest                    2.5.2                    py27_0python                    2.7.6                         1python-dateutil           1.5                       %3Cpip%3Epython.app                1.2                      py27_1pytz                      2014.2                   py27_0More complex patterns, based on regular expressions, are also possible. For example:%24 conda list %5Ep.*les%24# packages in environment at /Library/anaconda:#pytables                  3.1.0                np18py27_0%24","Suppose we want to have Python 3.x available in addition to the 2.7.xversion. Thepackage manager condaallows the creation of an environment in which to accomplishthis goal. The following output shows how this works in principle:%24 conda create -n py33test anaconda%3D1.9 python%3D3.3 numpy%3D1.8Fetching package metadata: ..Solving package specifications: .Package plan for installation in environment /Library/anaconda/envs/py33test:The following packages will be downloaded:    package                    %7C            build    ---------------------------%7C-----------------    anaconda-1.9.2             %7C       np18py33_0           2 KB    ...    xlsxwriter-0.5.2           %7C           py33_0         168 KBThe following packages will be linked:    package                    %7C            build    ---------------------------%7C-----------------    anaconda-1.9.2             %7C       np18py33_0   hard-link    ...    zlib-1.2.7                 %7C                1   hard-linkProceed (%5By%5D/n)%3FWhen you type "," to confirm the creation, conda will do as proposed (i.e., downloading,extracting, and linking the packages):*******UPDATE**********Fetching packages ...anaconda-1.9.2-np18py33_0.tar.bz2 100%25 %7C##########%7C Time: 0:00:00 173.62 kB/s...xlsxwriter-0.5.2-py33_0.tar.bz2 100%25 %7C############%7C Time: 0:00:01 131.32 kB/sExtracting packages ...%5B      COMPLETE      %5D %7C##########################%7C 100%25Linking packages ...%5B      COMPLETE      %5D %7C##########################%7C 100%25## To activate this environment, use:# %24 source activate py33test## To deactivate this environment, use:# %24 source deactivate#Now activate the new environment as advised by conda:%24 source activate py33testdiscarding /Library/anaconda/bin from PATHprepending /Library/anaconda/envs/py33test/bin to PATH","4.This is only one subtle, but harmless, change in the Python syntax from 2.7.xto 3.x that might be a bitconfusing to someone new to Python.(py33test)%24 pythonPython 3.3.4 %7CAnaconda 1.9.2 (x86_64)%7C (default, Feb 10 2014, 17:56:29)%5BGCC 4.0.1 (Apple Inc. build 5493)%5D on darwinType %22help%22, %22copyright%22, %22credits%22 or %22license%22 for more information.%3E%3E%3E print %22Hello Python 3.3%22  # this shouldn't work with Python 3.3  File %22%3Cstdin%3E%22, line 1    print %22Hello Python 3.3%22  # this shouldn't work with Python 3.3                           %5ESyntaxError: invalid syntax%3E%3E%3E print (%22Hello Python 3.3%22)  # this syntax should workHello Python 3.3%3E%3E%3E exit()%24Obviously, we indeed are now in the Python 3.3world, which you can judge from thePythonversion number displayed and the fact that you need parentheses for the printstatement to work correctly.4","WiththecondapackagemanageryoucaninstallandusemultipleseparatedPythonenvironmentsonasinglemachine.This,amongotherfeatures,simplifiestestingofPythoncodeforcompatibilitywithdifferent Python versions.Single libraries and packages can be installed using the conda install command, eitherin the general Anaconda installation:%24 conda install scipyor for a specific environment, as in:%24 conda install -n py33test scipyHere, py33test is the environment we created before. Similarly, you can update singlepackages easily:%24 conda update pandasThe packages to download and link depend on the respective version of the packagethat is installed. These can be very few to numerous, e.g., when a package has a numberof dependencies for which no current version is installed. For our newly created envi%E2%80%90ronment, the updating would take the form:%24 conda update -n py33test pandas","Finally, conda makes it easy to remove packages with the removecommand from themain installation or a specific environment. The basic usage is:%24 conda remove scipyFor an environment it is:%24 conda remove -n py33test scipySince the removal is a somewhat %E2%80%9Cfinal%E2%80%9D operation, you might want to dry runthecommand:%24 conda remove --dry-run -n py33test scipyIf you are sure, you can go ahead with the actual removal. To get back to the originalPython and Anaconda version, deactivate the environment:%24 source deactivateFinally, we can clean up the whole environment by use of remove with the option --all:%24 conda remove --all -n py33testThe package manager conda makes Pythondeployment quite convenient. Apart fromthe basic functionalities illustrated in this section, there are also a number of moreadvanced features available. Detailed documentation is found at http://conda.pydata.org/docs/.","There are a number of reasons why one might like to deploy Pythonvia a web browser.Among them are:No need for installationLocal installations of a complete Python environment might be both complex (e.g.,in a large organization with many computers), and costly to support and maintain%3Bmaking Python available via a web browser makes deployment much more efficientin certain scenarios.Use of (better) remote hardwareWhen it comes to complex, compute- and memory-intensive analytics tasks, a localcomputer might not be able to perform such tasks%3B the use of (multiple) sharedservers with multiple cores, larger memories, and maybe GPGPUs makes such taskspossible and more efficient.CollaborationWorking, for example, with a team on a single or multiple servers makes collabo%E2%80%90ration simpler and also increases efficiency: data is not moved to every local ma%E2%80%90chine, nor, after the analytics tasks are finished, are the results moved back to somecentral storage unit and/or distributed among the team members.","The Python Quant Platform is a web- and browser-based financial analytics and col%E2%80%90laboration platform developed and maintained by The Python Quants GmbH. You canregister for the platform at http://quant-platform.com. It features, among others, thefollowing basic components:File managerA tool to manage file up/downloads and more via a web GUI.Linux terminalA Linuxterminal to work with the server (for example, a virtual server instance inthe cloud or a dedicated server run on-premise by a company)%3B you can use Vim,Nano, etc. for code editing and work with Git repositories for version control.AnacondaAn Anaconda installation that provides all the functionality discussed previously%3Bby default you can choose between Python 2.7 and Python 3.4.Python shellThe standard Python shell.IPython ShellAn enhanced IPython shell.IPython NotebookThe browser version of IPython. You will generally use this as the central tool.Chat room/forumTo collaborate, exchange ideas, and to up/download, for example, researchdocuments.Advanced analyticsIn addition to the Linux server and Python environments, the platform providesanalytical capabilities for, e.g., portfolio, risk, and derivatives analytics as well as forbacktesting trading strategies (in particular, DX analytics%3B see Part III for a simplifiedbut fully functional version of the library)%3B there is also an R stack available to call,for example, R functions from within IPython Notebook.Standard APIsStandard Python-based APIs for data delivery services of leading financial dataproviders.When it comes to collaboration, the Python Quant Platformalso allows one to define-under a %E2%80%9Ccompany%E2%80%9D-certain %E2%80%9Cuser groups%E2%80%9D with certain rights for different Pythonprojects (i.e., directories and files). The platform is easily scalable and is deployed viaDockercontainers. Figure 2-1 shows a screenshot of the main screen of the PythonQuant Platform.","5.For Windowsusers and developers, the full integration of Python in Visual Studiois a compelling alter%E2%80%90native. There is even a whole suite of Python tools for Visual Studioavailable (cf. http://pytools.codeplex.com).Figure 2-1. Screenshot of Python Quant Platform","The success and popularity of a programming language result to some extent from thetools that are available to work with the language. It has long been the case that Pythonwas considered a nice, easy-to-learn and easy-to-use language, but without a compellingset of tools for interactive analytics or development. This has changed. There are nowa large number of tools available that help analysts and developers to be as productiveas possible with Python. It is not possible to give even a somewhat exhaustive overview.However, it is possible to highlight two of the most popular tools in use today: IPythonand Spyder.5","For completeness, let us first consider using the standard Pythoninterpreter itself. Fromthe system shell/command-line interface, Python is invoked by simply typing python:%24 pythonPython 2.7.6 %7CAnaconda 1.9.2 (x86_64)%7C (default, Feb 10 2014, 17:56:29)%5BGCC 4.0.1 (Apple Inc. build 5493)%5D on darwin","Type %22help%22, %22copyright%22, %22credits%22 or %22license%22 for more information.%3E%3E%3E print %22Hello Python for Finance World.%22Hello Python for Finance World.%3E%3E%3E exit()%24Although you can do quite a bit of Python with the standard prompt, most people preferto use IPythonby default since this environment provides everythingthat the standardinterpreter prompt offers, and much more on top of that.","IPython was used in Chapter 1 to present the first examples of Python code. This sectiongives an overview of the capabilities of IPythonthrough specific examples. A completeecosystem has evolved around IPythonthat is so successful and appealing that users ofother languages make use of the basic approach and architecture it provides. For ex%E2%80%90ample, there is a version of IPython for the Julia language.","IPython comes in three flavors:ShellThe shell version is based on the system and Pythonshell, as the name suggests%3Bthere are no graphical capabilities included (apart from displaying plots in a separatewindow).QT consoleThis version is based on the QT graphical user interface framework (cf. http://qt-project.org), is more feature-rich, and allows, for example, for inline graphics.NotebookThis is a JavaScript-based web browser version that has become the communityfavorite for interactive analytics and also for teaching, presenting, etc.The shell version is invoked by simply typing ipython in the shell:%24 ipythonPython 2.7.6 %7CAnaconda 1.9.2 (x86_64)%7C (default, Feb 10 2014, 17:56:29)Type %22copyright%22, %22credits%22 or %22license%22 for more information.IPython 2.0.0 -- An enhanced Interactive Python.%3F         -%3E Introduction and overview of IPython's features.%25quickref -%3E Quick reference.help      -%3E Python's own help system.object%3F   -%3E Details about 'object', use 'object%3F%3F' for extra details.In%5B1%5D:3%2B4*2Out%5B1%5D: 11","In%5B2%5D:Using the option --pylab imports a large set of scientific and data analysis libraries, likeNumPy, in the namespace:%24 ipython --pylabPython 2.7.6 %7CAnaconda 1.9.2 (x86_64)%7C (default, Feb 10 2014, 17:56:29)Type %22copyright%22, %22credits%22 or %22license%22 for more information.IPython 2.0.0 -- An enhanced Interactive Python.%3F         -%3E Introduction and overview of IPython's features.%25quickref -%3E Quick reference.help      -%3E Python's own help system.object%3F   -%3E Details about 'object', use 'object%3F%3F' for extra details.Using matplotlib backend: MacOSXIn%5B1%5D:a%3Dlinspace(0,20,5)# linspace from NumPyIn%5B2%5D:aOut%5B2%5D:array(%5B0.,5.,10.,15.,20.%5D)In%5B3%5D:Similarly, the QT console of IPython is invoked by the following command:%24 ipython qtconsole --pylab inlineUsing the inline parameter in addition to the --pylab option lets IPythonplot allgraphics inline. Figure 2-2 shows a screenshot of the QT console with an inline plot.Finally, the Notebook version is invoked as follows:%24 ipython notebook --pylab inlineFigure 2-3 shows a screenshot of an IPython Notebook session. The inlineoptionagain has the effect that plots will be displayed in IPython Notebookand not in a separatewindow.All in all, there are a large number of options for how to invoke an IPython kernel. Youcan get a listing of all the options by typing:%24 ipython --hRefer to the IPython documentation for detailed explanations.","6.From IPython 2.0 on, these cells are called Raw NBConvert.Figure 2-2. IPython's QT console","In what follows, we describe the basic usage of the IPython Notebook. A fundamentalconcept of the Notebook is that you work with different kinds of cells. These include thefollowing types:CodeContains executable Python codeMarkdownContains text written in Markdown language and/or HTMLRaw textContains text without formatting6Heading (1-6)Headings for text structuring, e.g., section heads","Figure 2-3. IPython's browser-based NotebookThe different cell types already indicate that the Notebook is more than an enhancedPython shell only. It is intended to fulfill the requirements of a multitude of documen%E2%80%90tation and presentation scenarios. For example, an IPython Notebook file, having asuffix of .ipynb, can be converted to the following formats:Python fileGenerates a Python code file (.py) from an IPython Notebook file with noncodecells commented out.HTML pageGenerates a single HTML page from a single IPython Notebook file.HTML5 slidesMaking use of different cell markings for slide shows, a Notebook file is convertedinto a presentation with multiple HTML5 slides (using the reveal.js framework).LaTeX/PDFSuch a file can also be converted to a LaTeXfile, which then can be converted intoa PDF document.","RestructuredTextRestructuredText(.rst) is used, for example, by the SPHINXdocumentationpackage for Python projects.","AmajoradvantageofIPythonNotebookisthatyoucaneasilypublishand share your complete Notebook with others. Once your analyticsproject with IPython is finished, you can publish it as an HTML pageor a PDF, or use the content for a slide presentation.The format of an IPython Notebook file is based on the JavaScript Object Notation(JSON) standard. The following is the text version of the Notebookdisplayed inFigure 2-3-you will notice some metadata, the different types of cells, and their content,and that even graphics are translated into ASCII characters:%7B%22metadata%22:%7B%22name%22:%22%22%7D,%22nbformat%22:3,%22nbformat_minor%22:0,%22worksheets%22:%5B%7B%22cells%22:%5B%7B%22cell_type%22:%22code%22,%22collapsed%22:false,%22input%22:%5B%22import numpy as np%5Cn%22,%22import matplotlib.pyplot as plt%22%5D,%22language%22:%22python%22,%22metadata%22:%7B%7D,%22outputs%22:%5B%5D,%22prompt_number%22:1%7D,%7B%22cell_type%22:%22code%22,%22collapsed%22:false,%22input%22:%5B%22a %3D np.linspace(0, 10, 25)%5Cn%22,%22b %3D np.sin(a)%22%5D,%22language%22:%22python%22,%22metadata%22:%7B%7D,%22outputs%22:%5B%5D,%22prompt_number%22:2%7D,","%7B%22cell_type%22:%22markdown%22,%22metadata%22:%7B%7D,%22source%22:%5B%22Inline comments can be easily placed between code cells.%22%5D%7D,%7B%22cell_type%22:%22code%22,%22collapsed%22:false,%22input%22:%5B%22plt.plot(a, b, 'b%5E')%5Cn%22,%22plt.grid(True)%22%5D,%22language%22:%22python%22,%22metadata%22:%7B%7D,%22outputs%22:%5B%7B%22metadata%22:%7B%7D,%22output_type%22:%22display_data%22,%22png%22:%22iVBORw0KGgoAAAAN...SuQmCC%5Cn%22,%22text%22:%5B%22%3Cmatplotlib.figure.Figure at 0x105812a10%3E%22%5D%7D%5D,%22prompt_number%22:3%7D%5D,%22metadata%22:%7B%7D%7D%5D%7DFor example, when converting such a file to LaTeX, raw text cells can contain LaTeXcodesince the content of such cells is simply passed on by the converter. All this is one of thereasons why the IPython Notebook is nowadays often used for the composition oflarger, more complex documents, like scientific research papers. You have executablecode and documenting text in a single file that can be translated into a number of dif%E2%80%90ferent output formats.In a finance context this also makes IPythona valuable tool, since, for example, themathematical description of an algorithm and the executable Pythonversion can livein the same document. Depending on the usage scenario, a web page (e.g., intranet), aPDF document (e.g., client mailings), or a presentation (e.g., board meeting) can begenerated. With regard to the presentation option, you can, for example, skip those cellsthat may contain text passages that might be too long for a presentation.The basic usage of the Notebook is quite intuitive. You mainly navigate it with the arrowkeys and %E2%80%9Cexecute%E2%80%9D cells by using either Shift-Return or Ctrl-Return. The difference is","that the first option moves you automatically to the next cell after execution while thesecond option lets you remain at the same cell. The effect of %E2%80%9Cexecuting%E2%80%9D cells dependson the type of the cell. If it is a code cell, then the code is executed and the output (ifany) is shown. If it is a Markdown cell, the content is rendered to show the result.","The following shows a few selected examples for Markdown commands:**bold** prints the text in bold*italic* prints the text in italic_italic_ also prints it in italic**_italic_** bold and italicbullet point lists:* first_bullet* second_bullet%26ndash%3B renders to a dash%3Cbr%3E inserts a line breakFigure 2-4 shows the same code both in a raw text cell (which looks the same as thepreceding text) and rendered in a Markdown cell. In this way, you can easily combinePython code and formatted, nicely rendered text in a single document.A detailed description of the Markdownlanguage used for IPython Notebook is foundat http://daringfireball.net/projects/markdown/.As mentioned before, the rendering capabilities of IPythonare not restricted to theMarkdown language. IPythonalso renders by default mathematical formulae describedon the basis of the LaTeX typesetting system, the de facto standard for scientific pub%E2%80%90lishing. Consider, for example, from Chapter 1 the formula for the index level in theBlack-Scholes-Merton (1973) model, as provided in Equation 1-1. For convenience, werepeat it here as Equation 2-1.Equation 2-1. Black-Scholes-Merton (1973) index level at maturityST%3DS0expr%E2%88%9212%CF%832T%2B%CF%83Tz","Figure 2-4. Screenshot of IPython Notebook with Markdown renderingThe LaTeX code that describes Equation 2-1 looks roughly like the following:S_T %3D S_0 %5Cexp((r - 0.5%5Csigma%5E2) T %2B %5Csigma %5Csqrt%7BT%7D z)Figure 2-5shows a raw text cell with Markdowntext and the LaTeXcode, as well as theresult as rendered in a Markdowncell. The figure also shows a more complex formula:the Black-Scholes-Merton option pricing formula for European call options, as foundin Equation 3-1 in Chapter 3.","Figure 2-5. Markdown and LaTeX for financial formulae","One of IPython's strengths lies in its magic commands. They are %E2%80%9Cmagic%E2%80%9D in the sensethat they add some really helpful and powerful functions to the standard Python shellfunctionality. Basic information and help about these functions can be accessed via:In %5B1%5D: %25magicIPython's 'magic' functions%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3DThe magic function system provides a series of functions which allow you tocontrol the behavior of IPython itself, plus a lot of system-typefeatures. There are two kinds of magics, line-oriented and cell-oriented....A list of all available magic commands can be generated in an IPython session as follows:In%5B2%5D:%25lsmagicIn interactive computing, magic commands can, for example, be used for simple profil%E2%80%90ing tasks. For such a use case, you might use %25time or %25prun:","In%5B3%5D:importnumpyasnpIn%5B4%5D:%25timenp.sin(np.arange(1000000))CPUtimes:user31.8ms,sys:7.87ms,total:39.7msWalltime:39msOut%5B5%5D:array(%5B 0.        ,  0.84147098,  0.90929743, ...,  0.21429647,       -0.70613761, -0.97735203%5D)In%5B6%5D:%25prunnp.sin(np.arange(1000000))3functioncallsin0.043secondsOrderedby:internaltimencallstottimepercallcumtimepercallfilename:lineno(function)10.0410.0410.0430.043%3Cstring%3E:1(%3Cmodule%3E)10.0020.0020.0020.002%7Bnumpy.core.multiarray.arange%7D10.0000.0000.0000.000%7Bmethod'disable'of'_lsprof.Profiler'objects%7DThere is yet another command, %25timeit or %25%25timeit, for timing codes in a single lineor a whole cell in the IPythonNotebook:In %5B6%5D: %25timeit np.sin(np.arange(1000000))10 loops, best of 3: 27.5 ms per loopThis function executes a number of loops to get more reliable estimates for the durationof a function call or a snippet of code.It is not possible to explain in detail all the magic functions that IPythonprovides.However, IPython itself strives to make it as easy as possible to interactively look upinformation about IPython and its commands. Among the most helpful are those listedin Table 2-2 (cf. http://bit.ly/ipython_tutorial).Table 2-2. Selected help functions included in IPythonName","%3FIntroduction and overview of IPython features%25quickrefQuick referencehelpPython's own help systemobject%3FDetails about the %E2%80%9Cobject%E2%80%9D%3B use object%3F%3F for extra detailsAnother feature of IPython is that it is highly configurable. Information about the con%E2%80%90figuration capabilities is also found in the documentation.A magic command that also helps with customizing IPython is %25bookmark. This allowsthe bookmarking of arbitrary directories by the use of your custom names such thatyou can later-no matter where the IPython kernel is invoked from and no matter whatthe current directory is-navigate to any of your bookmarked directories immediately","7.However, you can configure your favorite editor for IPythonand invoke it by the magic command %25editorFILENAME.(i.e., you do not need to use cd). The following shows how to set a bookmark and howto get a list of all bookmarks:In%5B6%5D:%25bookmarkpy4fiIn%5B7%5D:%25bookmark-lCurrentbookmarks:py4fi-%3E/Users/yhilpisch/Documents/Work/Python4Finance/","Yet another really helpful feature is that you can execute command-line/system shellfunctions directly from an IPythonprompt or a Notebookcell. To this end you need touse the !to indicate that the following command should be escaped to the system shell(or %25%25!when a complete cell should be handled that way). As a simple illustration, thefollowing creates a directory, moves to that directory, moves back, and deletes the di%E2%80%90rectory:In%5B7%5D:!mkdirpython4financeIn%5B8%5D:cdpython4finance//Users/yhilpisch/python4financeIn%5B9%5D:cd../Users/yhilpischIn%5B10%5D:!rm-rfpython4finance/IPythonprovides you with all the functions you would expect from a powerful inter%E2%80%90active development environment. It is often the case that people, beginners and expertsalike, even find their way to Python via IPython. Throughout the book, there are aplentitude of examples illustrating the use of IPythonfor interactive data and financialanalytics. You should also consult the book by McKinney (2012), and in particularChapter 3, for further information on how to use IPython effectively.","While IPython satisfies all of most users' requirements for interactive analytics andprototyping, larger projects generally demand %E2%80%9Csomething more.%E2%80%9D In particular, IPythonitself has no editor directly built into the application.7For all those looking for a moretraditional development environment, Spyder might therefore be a good choice.Similar to IPython, Spyder has been designed to support rapid, interactive developmentwith Python. However, it also has, for example, a full-fledged editor, more powerful","project management and debugging capabilities, and an object and variable inspectoras well as a full integration of the IPython shell version. Within Spyder you can alsostart a standard Python prompt session.The built-in editor of Spyderprovides all you need to do Python development. Amongother features (cf. http://code.google.com/p/spyderlib/wiki/Features), it offers the fol%E2%80%90lowing:HighlightingSyntax coloring for Python, C/C%2B%2B, and Fortran code%3B occurrence highlightingIntrospectionPowerful dynamic code introspection features (e.g., code completion, calltips, ob%E2%80%90ject definition with a mouse click)Code browserBrowsing of classes and functionsProject managementDefining and managing projects%3B generating to-do listsInstant code checkingGetting errors and warnings on the fly (by using pyflakes, cf. https://pypi.python.org/pypi/pyflakes)DebuggingSetting breakpoints and conditional breakpoints to use with the Python debuggerpdb (cf. http://docs.python.org/2/library/pdb.html)In addition, Spyder provides further helpful functionality:ConsolesOpen multiple Python and IPythonconsoles with separate processes each%3B run thecode from the active editor tab (or parts of it) in a consoleVariable explorerEdit and compare variables and arrays%3B generate 2D plots of arrays on the fly%3B inspectvariables while debuggingObject inspectorDisplay documentation strings interactively%3B automatically render, for example,rich text formattingOther featuresHistory log%3B array editor similar to a spreadsheet%3B direct access to online help%3B man%E2%80%90agement and exploration of whole projects%3B syntax and code checking via Pylint.","Figure 2-6 provides a screenshot of Spyder showing the text editor (on the left), thevariable inspector (upper right), and an active Pythonconsole (lower right). Spyder isa good choice to start with Pythonprogramming, especially for those who are used, forexample, to such environments as those provided by Matlab or R. However, advancedprogrammers will also find a lot of helpful development functionality under a singleroof.Figure 2-6. Screenshot of Spyder","If you are a beginner or casual Pythondeveloper or an expert coming from a differentprogramming background, getting started with Pythonis generally pretty easy in thatonly a couple of simple steps are required. To begin, you should install an appropriatePython distribution, like Anaconda, to have a consistent Python environment availableand also to simplify the regular updating procedures.With a distribution like Anacondayou have available the most important tools to inter%E2%80%90actively practice data and financial analytics, like with IPython, or to develop largerapplications in a more traditional implement-test-debug fashion, like with Spyder. Ofcourse, you can add to the mix your favorite editor, which probably already has Pythonsyntax highlighting included. If you additionally are looking for syntax and code check%E2%80%90","ing capabilities, you might consider the built-in Spyder editor or any other Python-focused editor available.Appendix Aintroduces a number of best practices in the areas of syntax, documenta%E2%80%90tion, and unit testing. In terms of syntax, spaces and blank lines play an important role,as well as the indentation of code blocks. When it comes to documentation, you shouldconsider including documentation strings in any function or class, providing back%E2%80%90ground and help for such things as input parameters, output, and possible errors, aswell as usage examples. Finally, you should include unit tests in your development pro%E2%80%90cess from the beginning (at least for larger projects or those shared with a broader userbase) and use dedicated tools to simplify the test procedures.","The following web resources are helpful with regard to the topics covered in this chapter:%E2%80%A2http://docs.continuum.io/anaconda/ for the Anaconda documentation%E2%80%A2http://conda.pydata.org/docs/ for the conda documentation%E2%80%A2http://ipython.org/ipython-doc/stable/ for the IPython documentation%E2%80%A2http://daringfireball.net/projects/markdown/ for the Markdownlanguage used byIPython Notebook%E2%80%A2http://code.google.com/p/spyderlib for information about SpyderA good introduction to Pythondeployment and the use of IPythonas a developmentenvironment is provided in:%E2%80%A2Wes McKinney (2012): Python for Data Analysis. O'Reilly, Sebastopol, CA.","Quantitative analysis, as we define it, is the application ofmathematical and/or statistical methods to market data.- John FormanThis chapter dives into some concrete examples from quantitative finance to illustratehow convenient and powerful it is to use Pythonand its libraries for financial analytics.The focus lies on the flow of the exposition, and a number of details that might beimportant in real-world applications are not touched upon. Also, details of Pythonusageare mainly skipped because later chapters explain them further.Specifically, this chapter presents the following examples:Implied volatilitiesOption quotes for certain maturity dates are taken to back out the implied volatil%E2%80%90ities of these options and to plot them-a task option traders and risk managers,among others, are faced with on a daily basis.Monte Carlo simulationThe evolution of a stock index over time is simulated via Monte Carlo techniques,selected results are visualized, and European option values are calculated. MonteCarlo simulation is a cornerstone for numerical option pricing as well as for riskmanagement efforts involving value-at-risk calculations or credit valueadjustments.Technical analysisAn analysis of historical time series data is implemented to backtest an investmentstrategy based on trend signals%3B both professional investors and ambitious amateursregularly engage in this kind of investment analysis.All examples have to deal in some ways with date-time information. Appendix Cin%E2%80%90troduces handling such information with Python, NumPy, and pandas.","1.Chapter 19 also deals with options based on the VSTOXX volatility index%3B it calibrates an option pricingmodel to market quotes and values American, nontraded options given the calibrated model.","Given an option pricing formula like the seminal one of Black-Scholes-Merton (1973),implied volatilities are those volatility values that, ceteris paribus, when put into theformula, give observed market quotes for different option strikes and maturities. In thiscase, the volatility is not an input parameter for the model/formula, but the result of a(numerical) optimization procedure given that formula.The example we consider in the following discussion is about a new generation of op%E2%80%90tions, namely volatility options on the VSTOXX volatility index. Eurex, the derivativesexchange that provides these options on the VSTOXX and respective futures contracts,established a comprehensive Python-based tutorial called %E2%80%9CVSTOXX Advanced Serv%E2%80%90ices%E2%80%9D in June 2013 about the index and its derivatives contracts.1However, before proceeding with the VSTOXX options themselves, let us first repro%E2%80%90duce in Equation 3-1 the famous Black-Scholes-Merton formula for the pricing of Eu%E2%80%90ropean call options on an underlying without dividends.Equation 3-1. Black-Scholes-Merton (1973) option pricing formula C St,K,t,T,r,%CF%83%3DSt%C2%B7%ED%90%80d1%E2%88%92e%E2%88%92rT%E2%88%92t%C2%B7K%C2%B7%ED%90%80d2%ED%90%80d%3D12%CF%80%E2%88%92%E2%88%9Ede%E2%88%9212x2dxd1%3DlogStK%2Br%2B%CF%8322T%E2%88%92t%CF%83T%E2%88%92td2%3DlogStK%2Br%E2%88%92%CF%8322T%E2%88%92t%CF%83T%E2%88%92tThe different parameters have the following meaning:","StPrice/level of the underlying at time t%ED%9C%8EConstant volatility (i.e., standard deviation of returns) of the underlyingKStrike price of the optionTMaturity date of the optionrConstant riskless short rateConsider now that an option quote for a European call option C*is given. The impliedvolatility %ED%9C%8Eimp is the quantity that solves the implicit Equation 3-2.Equation 3-2. Implied volatility given market quote for option C St,K,t,T,r,%CF%83imp%3D C *There is no closed-form solution to this equation, such that one has to use a numericalsolution procedure like the Newton scheme to estimate the correct solution. Thisscheme iterates, using the first derivative of the relevant function, until a certain numberof iterations or a certain degree of precision is reached. Formally, we have Equation 3-3for some starting value %CF%830imp and for 0 %3C n %3C %E2%88%9E.Equation 3-3. Newton scheme for numerically solving equations%CF%83n%2B1imp%3D%CF%83nimp%E2%88%92 C %CF%83nimp%E2%88%92 C *%E2%88%82 C %CF%83nimp/%E2%88%82%CF%83nimpThe partial derivative of the option pricing formula with respect to the volatility is calledVega and is given in closed form by Equation 3-4.Equation 3-4. Vega of a European option in BSM model%E2%88%82 C %E2%88%82%CF%83%3DSt%ED%90%80'd1T%E2%88%92t","The financial and numerical tools needed are now complete-even if only roughly de%E2%80%90scribed-and we can have a look into the respective Python code that assumes the specialcase t %3D 0 (Example 3-1).Example 3-1. Black-Scholes-Merton (1973) functions## Valuation of European call options in Black-Scholes-Merton model# incl. Vega function and implied volatility estimation# bsm_functions.py## Analytical Black-Scholes-Merton (BSM) Formuladefbsm_call_value(S0,K,T,r,sigma):''' Valuation of European call option in BSM model.    Analytical formula.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    S0 : float        initial stock/index level    K : float        strike price    T : float        maturity date (in year fractions)    r : float        constant risk-free short rate    sigma : float        volatility factor in diffusion term    Returns    %3D%3D%3D%3D%3D%3D%3D    value : float        present value of the European call option    '''frommathimportlog,sqrt,expfromscipyimportstatsS0%3Dfloat(S0)d1%3D(log(S0/K)%2B(r%2B0.5*sigma**2)*T)/(sigma*sqrt(T))d2%3D(log(S0/K)%2B(r-0.5*sigma**2)*T)/(sigma*sqrt(T))value%3D(S0*stats.norm.cdf(d1,0.0,1.0)-K*exp(-r*T)*stats.norm.cdf(d2,0.0,1.0))# stats.norm.cdf --%3E cumulative distribution function#                    for normal distributionreturnvalue# Vega function","defbsm_vega(S0,K,T,r,sigma):''' Vega of European option in BSM model.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    S0 : float        initial stock/index level    K : float        strike price    T : float        maturity date (in year fractions)    r : float        constant risk-free short rate    sigma : float        volatility factor in diffusion term    Returns    %3D%3D%3D%3D%3D%3D%3D    vega : float        partial derivative of BSM formula with respect        to sigma, i.e. Vega    '''frommathimportlog,sqrtfromscipyimportstatsS0%3Dfloat(S0)d1%3D(log(S0/K)%2B(r%2B0.5*sigma**2)*T/(sigma*sqrt(T))vega%3DS0*stats.norm.cdf(d1,0.0,1.0)*sqrt(T)returnvega# Implied volatility functiondefbsm_call_imp_vol(S0,K,T,r,C0,sigma_est,it%3D100):''' Implied volatility of European call option in BSM model.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    S0 : float        initial stock/index level    K : float        strike price    T : float        maturity date (in year fractions)    r : float        constant risk-free short rate    sigma_est : float        estimate of impl. volatility    it : integer        number of iterations","    Returns    %3D%3D%3D%3D%3D%3D%3D    simga_est : float        numerically estimated implied volatility    '''foriinrange(it):sigma_est-%3D((bsm_call_value(S0,K,T,r,sigma_est)-C0)/bsm_vega(S0,K,T,r,sigma_est))returnsigma_estThese are only the basic functions needed to calculate implied volatilities. What we needas well, of course, are the respective option quotes, in our case for European call optionson the VSTOXX index, and the code that generates the single implied volatilities. Wewill see how to do this based on an interactive IPython session.Let us start with the day from which the quotes are taken%3B i.e., our t%3D 0 reference day.This is March 31, 2014. At this day, the closing value of the index was V0%3D 17.6639 (wechange from S to V to indicate that we are now working with the volatility index):In%5B1%5D:V0%3D17.6639For the risk-free short rate, we assume a value of r %3D 0.01 p.a.:In%5B2%5D:r%3D0.01All other input parameters are given by the options data (i.e., T and K) or have to becalculated (i.e., %ED%9C%8Eimp). The data is stored in a pandasDataFrameobject (see Chapter 6)and saved in a PyTablesdatabase file (see Chapter 7). We have to read it from disk intomemory:In%5B3%5D:importpandasaspdh5%3Dpd.HDFStore('./source/vstoxx_data_31032014.h5','r')futures_data%3Dh5%5B'futures_data'%5D# VSTOXX futures dataoptions_data%3Dh5%5B'options_data'%5D# VSTOXX call option datah5.close()We need the futures data to select a subset of the VSTOXX options given their (forward)moneyness. Eight futures on the VSTOXX are traded at any time. Their maturities arethe next eight third Fridaysof the month. At the end of March, there are futures withmaturities ranging from the third Friday of April to the third Friday of November. TTMin the following pandas table represents time-to-maturity in year fractions:In%5B4%5D:futures_dataOut%5B4%5D:           DATE  EXP_YEAR  EXP_MONTH  PRICE   MATURITY    TTM        496 2014-03-31      2014          4  17.85 2014-04-18  0.049        497 2014-03-31      2014          5  19.55 2014-05-16  0.126        498 2014-03-31      2014          6  19.95 2014-06-20  0.222        499 2014-03-31      2014          7  20.40 2014-07-18  0.299        500 2014-03-31      2014          8  20.70 2014-08-15  0.375        501 2014-03-31      2014          9  20.95 2014-09-19  0.471","        502 2014-03-31      2014         10  21.05 2014-10-17  0.548        503 2014-03-31      2014         11  21.25 2014-11-21  0.644The options data set is larger since at any given trading day multiple call and put optionsare traded per maturity date. The maturity dates, however, are the same as for the futures.There are a total of 395 call options quoted on March 31, 2014:In%5B5%5D:options_data.info()Out%5B5%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E        Int64Index: 395 entries, 46170 to 46564        Data columns (total 8 columns):        DATE         395 non-null datetime64%5Bns%5D        EXP_YEAR     395 non-null int64        EXP_MONTH    395 non-null int64        TYPE         395 non-null object        STRIKE       395 non-null float64        PRICE        395 non-null float64        MATURITY     395 non-null datetime64%5Bns%5D        TTM          395 non-null float64        dtypes: datetime64%5Bns%5D(2), float64(3), int64(2), object(1)In%5B6%5D:options_data%5B%5B'DATE','MATURITY','TTM','STRIKE','PRICE'%5D%5D.head()Out%5B6%5D:             DATE   MATURITY    TTM  STRIKE  PRICE        46170 2014-03-31 2014-04-18  0.049       1  16.85        46171 2014-03-31 2014-04-18  0.049       2  15.85        46172 2014-03-31 2014-04-18  0.049       3  14.85        46173 2014-03-31 2014-04-18  0.049       4  13.85        46174 2014-03-31 2014-04-18  0.049       5  12.85As is obvious in the pandastable, there are call options traded and quoted that are farin-the-money (index level much higher than option strike). There are also options tra%E2%80%90ded that are far out-of-the-money (index level much lower than option strike). Wetherefore want to restrict the analysis to those call options with a certain (forward)moneyness, given the value of the future for the respective maturity. We allow a maxi%E2%80%90mum deviation of 50%25 from the futures level.Before we can start, we need to define a new column in the options_dataDataFrameobject to store the results. We also need to import the functions from the script inExample 3-1:In%5B7%5D:options_data%5B'IMP_VOL'%5D%3D0.0# new column for implied volatilitiesIn%5B8%5D:frombsm_functionsimport*The following code now calculates the implied volatilities for all those call options:In%5B9%5D:tol%3D0.5# tolerance level for moneynessforoptioninoptions_data.index:# iterating over all option quotesforward%3Dfutures_data%5Bfutures_data%5B'MATURITY'%5D%3D%3D %5Coptions_data.loc%5Boption%5D%5B'MATURITY'%5D%5D%5B'PRICE'%5D.values%5B0%5D","# picking the right futures valueif(forward*(1-tol)%3Coptions_data.loc%5Boption%5D%5B'STRIKE'%5D%3Cforward*(1%2Btol)):# only for options with moneyness within toleranceimp_vol%3Dbsm_call_imp_vol(V0,# VSTOXX valueoptions_data.loc%5Boption%5D%5B'STRIKE'%5D,options_data.loc%5Boption%5D%5B'TTM'%5D,r,# short rateoptions_data.loc%5Boption%5D%5B'PRICE'%5D,sigma_est%3D2.,# estimate for implied volatilityit%3D100)options_data%5B'IMP_VOL'%5D.loc%5Boption%5D%3Dimp_volIn this code, there is some pandassyntax that might not be obvious at first sight. Chap%E2%80%90ter 6 explains pandasand its use for such operations in detail. At this stage, it sufficesto understand the following features:In%5B10%5D:futures_data%5B'MATURITY'%5D# select the column with name MATURITYOut%5B10%5D: 496   2014-04-18         497   2014-05-16         498   2014-06-20         499   2014-07-18         500   2014-08-15         501   2014-09-19         502   2014-10-17         503   2014-11-21         Name: MATURITY, dtype: datetime64%5Bns%5DIn%5B11%5D:options_data.loc%5B46170%5D# select data row for index 46170Out%5B11%5D: DATE         2014-03-31 00:00:00         EXP_YEAR                    2014         EXP_MONTH                      4         TYPE                           C         STRIKE                         1         PRICE                      16.85         MATURITY     2014-04-18 00:00:00         TTM                        0.049         IMP_VOL                        0         Name: 46170, dtype: objectIn%5B12%5D:options_data.loc%5B46170%5D%5B'STRIKE'%5D# select only the value in column STRIKE# for index 46170Out%5B12%5D: 1.0The implied volatilities for the selected options shall now be visualized. To this end, weuse only the subset of the options_data object for which we have calculated the impliedvolatilities:","2.As we are only considering a single day's worth of futures and options quotes, the MATURITY column of thefutures_data object would have delivered the information a bit more easily since there are no duplicates.In%5B13%5D:plot_data%3Doptions_data%5Boptions_data%5B'IMP_VOL'%5D%3E0%5DTo visualize the data, we iterate over all maturities of the data set and plot the impliedvolatilities both as lines and as single points. Since all maturities appear multiple times,we need to use a little trick to get to a nonredundent, sorted list with the maturities. Thesetoperation gets rid of all duplicates, but might deliver an unsorted set of the matur%E2%80%90ities. Therefore, we sort the set object (cf. also Chapter 4):2In%5B14%5D:maturities%3Dsorted(set(options_data%5B'MATURITY'%5D))maturitiesOut%5B14%5D: %5BTimestamp('2014-04-18 00:00:00'),          Timestamp('2014-05-16 00:00:00'),          Timestamp('2014-06-20 00:00:00'),          Timestamp('2014-07-18 00:00:00'),          Timestamp('2014-08-15 00:00:00'),          Timestamp('2014-09-19 00:00:00'),          Timestamp('2014-10-17 00:00:00'),          Timestamp('2014-11-21 00:00:00')%5DThe following code iterates over all maturities and does the plotting. The result is shownas Figure 3-1. As in stock or foreign exchange markets, you will notice the so-calledvolatility smile, which is most pronounced for the shortest maturity and which becomesa bit less pronounced for the longer maturities:In%5B15%5D:importmatplotlib.pyplotasplt%25matplotlibinlineplt.figure(figsize%3D(8,6))formaturityinmaturities:data%3Dplot_data%5Boptions_data.MATURITY%3D%3Dmaturity%5D# select data for this maturityplt.plot(data%5B'STRIKE'%5D,data%5B'IMP_VOL'%5D,label%3Dmaturity.date(),lw%3D1.5)plt.plot(data%5B'STRIKE'%5D,data%5B'IMP_VOL'%5D,'r.')plt.grid(True)plt.xlabel('strike')plt.ylabel('implied volatility of volatility')plt.legend()plt.show()","3.Note that you can always look up attributes and methods of unknown objects by using the Python built-infunction dir, like with dir(group_data).Figure 3-1. Implied volatilities (of volatility) for European call options on the VSTOXXon March 31, 2014To conclude this example, we want to show another strength of pandas: namely, forworking with hierarchically indexed data sets. The DataFrameobject options_datahasan integer index, which we have used in several places. However, this index is not reallymeaningful-it is %E2%80%9Cjust%E2%80%9D a number. The option quotes for the day March 31, 2014 areuniquely described (%E2%80%9Cidentified%E2%80%9D) by a combination of the maturityand the strike-i.e.,there is only one call option per maturity and strike.The groupbymethod can be used to capitalize on this insight and to get a more mean%E2%80%90ingful index. To this end, we group by MATURITY first and then by the STRIKE. We onlywant to keep the PRICE and IMP_VOL columns:In%5B16%5D:keep%3D%5B'PRICE','IMP_VOL'%5Dgroup_data%3Dplot_data.groupby(%5B'MATURITY','STRIKE'%5D)%5Bkeep%5Dgroup_dataOut%5B16%5D: %3Cpandas.core.groupby.DataFrameGroupBy object at 0x7faf483d5710%3EThe operation returns a DataFrameGroupByobject.3 To get to the data, we need to applyan aggregation operation on the object, like taking the sum. Taking the sum yields thesingle data point since there is only one data element in every group:In%5B17%5D:group_data%3Dgroup_data.sum()group_data.head()","4.Although not needed here, all approaches store complete simulation paths in-memory. For the valuation ofstandard European options this is not necessary, as the corresponding example in Chapter 1shows. However,for the valuation of American options or for certain risk management purposes, whole paths are needed.5.These Monte Carlo examples and implementation approaches also appear in the article Hilpisch (2013).Out%5B17%5D:                    PRICE   IMP_VOL         MATURITY   STRIKE         2014-04-18 9        8.85  2.083386                    10       7.85  1.804194                    11       6.85  1.550283                    12       5.85  1.316103                    13       4.85  1.097184The resulting DataFrame object has two index levels and two columns. The followingshows all values that the two indices can take:In%5B18%5D:group_data.index.levelsOut%5B18%5D: FrozenList(%5B%5B2014-04-18 00:00:00, 2014-05-16 00:00:00, 2014-06-20 00:00         :00, 2014-07-18 00:00:00, 2014-08-15 00:00:00, 2014-09-19 00:00:00, 201         4-10-17 00:00:00, 2014-11-21 00:00:00%5D, %5B9.0, 10.0, 11.0, 12.0, 13.0, 1         4.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0,         26.0, 27.0, 28.0, 29.0, 30.0%5D%5D)","Monte Carlo simulation is one of the most important algorithms in finance and nu%E2%80%90merical science in general. Its importance stems from the fact that it is quite powerfulwhen it comes to option pricing or risk management problems. In comparison to othernumerical methods, the Monte Carlo method can easily cope with high-dimensionalproblems where the complexity and computational demand, respectively, generally in%E2%80%90crease in linear fashion.The downside of the Monte Carlo method is that it is per se computationally demandingand often needs huge amounts of memory even for quite simple problems. Therefore,it is necessary to implement Monte Carlo algorithms efficiently. The example that fol%E2%80%90lows illustrates different implementation strategies in Pythonand offers three differentimplementation approaches for a Monte Carlo-based valuation of a European option.4The three approaches are:5Pure PythonThis example sticks with the standard library-i.e., those libraries and packagesthat come with a standard Pythoninstallation-and uses only built-in Python ca%E2%80%90pabilities to implement the Monte Carlo valuation.","6.For details, refer to the book by Hilpisch (2015).Vectorized NumPyThis implementation uses the capabilities of NumPy to make the implementationmore compact and much faster.Fully vectorized NumPyThe final example combines a different mathematical formulation with the vecto%E2%80%90rization capabilities of NumPy to get an even more compact version of the samealgorithm.The examples are again based on the model economy of Black-Scholes-Merton (1973),where the risky underlying (e.g., a stock price or index level) follows, under risk neu%E2%80%90trality, a geometric Brownian motion with a stochastic differential equation (SDE), asin Equation 3-5.Equation 3-5. Black-Scholes-Merton (1973) stochastic differential equationdSt%3DrStdt%2B%CF%83StdZtThe parameters are defined as in Equation 3-1 and Zis a Brownian motion. A discre%E2%80%90tization scheme for the SDE in Equation 3-5 is given by the difference equation inEquation 3-6.Equation 3-6. Euler discretization of SDESt%3DSt%E2%88%92%CE%94texpr%E2%88%9212%CF%832%CE%94t%2B%CF%83%CE%94tztThe variable zis a standard normally distributed random variable, 0 %3C %ED%9B%A5t %3C T, a (smallenough) time interval. It also holds 0 %3C t %E2%89%A4 T with T the final time horizon.6We parameterize the model with the values S0 %3D 100, K %3D 105, T %3D 1.0, r %3D 0.05, %ED%9C%8E %3D 0.2.Using the Black-Scholes-Merton formula as in Equation 3-1 and Example 3-1 from theprevious example, we can calculate the exact option value as follows:In%5B19%5D:frombsm_functionsimportbsm_call_valueS0%3D100.K%3D105.T%3D1.0r%3D0.05sigma%3D0.2bsm_call_value(S0,K,T,r,sigma)","Out%5B19%5D: 8.0213522351431763This is our benchmark value for the Monte Carlo estimators to follow. To implement aMonte Carlo valuation of the European call option, the following recipe can be applied:1.Divide the time interval %5B0,T%5D in equidistant subintervals of length %ED%9B%A5t.2.Start iterating i %3D 1, 2,%E2%80%A6, I.a.For every time step t%E2%88%88 %7B%ED%9B%A5t, 2%ED%9B%A5t,%E2%80%A6, T%7D, draw pseudorandom numbers zt(i).b.Determine the time T value of the index level ST(i) by applying the pseudo-random numbers time step by time step to the discretization scheme inEquation 3-6.c.Determine the inner value hT of the European call option at Tas hT(ST(i)) %3Dmax(ST(i) %E2%80%93 K,0).d.Iterate until i %3D I.3.Sum up the inner values, average, and discount them back with the riskless shortrate according to Equation 3-7.Equation 3-7 provides the numerical Monte Carlo estimator for the value of the Euro%E2%80%90pean call option.Equation 3-7. Monte Carlo estimator for European call option C 0%E2%89%88e%E2%88%92rT1I%E2%88%91IhTSTi","Example 3-2 translates the parametrization and the Monte Carlo recipe into purePython. The code simulates 250,000 paths over 50 time steps.Example 3-2. Monte Carlo valuation of European call option with pure Python## Monte Carlo valuation of European call options with pure Python# mcs_pure_python.py#fromtimeimporttimefrommathimportexp,sqrt,logfromrandomimportgauss,seedseed(20000)t0%3Dtime()","# ParametersS0%3D100.# initial valueK%3D105.# strike priceT%3D1.0# maturityr%3D0.05# riskless short ratesigma%3D0.2# volatilityM%3D50# number of time stepsdt%3DT/M# length of time intervalI%3D250000# number of paths# Simulating I paths with M time stepsS%3D%5B%5Dforiinrange(I):path%3D%5B%5Dfortinrange(M%2B1):ift%3D%3D0:path.append(S0)else:z%3Dgauss(0.0,1.0)St%3Dpath%5Bt-1%5D*exp((r-0.5*sigma**2)*dt%2Bsigma*sqrt(dt)*z)path.append(St)S.append(path)# Calculating the Monte Carlo estimatorC0%3Dexp(-r*T)*sum(%5Bmax(path%5B-1%5D-K,0)forpathinS%5D)/I# Results outputtpy%3Dtime()-t0print%22European Option Value %257.3f%22%25C0print%22Duration in Seconds   %257.3f%22%25tpyRunning the script yields the following output:In%5B20%5D:%25runmcs_pure_python.pyOut%5B20%5D:EuropeanOptionValue7.999DurationinSeconds34.258Note that the estimated option value itself depends on the pseudorandom numbersgenerated while the time needed is influenced by the hardware the script isexecuted on.The major part of the code in Example 3-2 consists of a nested loop that generates step-by-step single values of an index level path in the inner loop and adds completed pathsto a list object with the outer loop. The Monte Carlo estimator is calculated usingPython's list comprehension syntax. The estimator could also be calculated by a forloop:In%5B21%5D:sum_val%3D0.0forpathinS:# C-like iteration for comparison","sum_val%2B%3Dmax(path%5B-1%5D-K,0)C0%3Dexp(-r*T)*sum_val/Iround(C0,3)Out%5B21%5D: 7.999Although this loop yields the same result, the list comprehension syntax is morecompact and closer to the mathematical notation of the Monte Carlo estimator.","NumPy provides a powerful multidimensional array class, called ndarray, as well as acomprehensive set of functions and methods to manipulate arrays and implement(complex) operations on such objects. From a more general point of view, there are twomajor benefits of using NumPy:SyntaxNumPygenerally allows implementations that are more compact than pure Pythonand that are often easier to read and maintain.SpeedThe majority of NumPycode is implemented in  C  or Fortran, which makes NumPy,when used in the right way, faster than pure Python.The generally more compact syntax stems from the fact that NumPy brings powerfulvectorization and broadcasting capabilities to Python. This is similar to having vectornotation in mathematics for large vectors or matrices. For example, assume that we havea vector with the first 100 natural numbers, 1, %E2%80%A6, 100:v%3D12%E2%8B%AE100Scalar multiplication of this vector is written compactly as:u%3D2%C2%B7v%3D24%E2%8B%AE200Let's see if we can do this with Pythonlist objects, for example:In%5B22%5D:v%3Drange(1,6)printv","Out%5B22%5D: %5B1, 2, 3, 4, 5%5DIn%5B23%5D:2*vOut%5B23%5D: %5B1, 2, 3, 4, 5, 1, 2, 3, 4, 5%5DNaivescalar multiplication does not return the scalar product. It rather returns, in thiscase, two times the object (vector). With NumPy the result is, however, as desired:In%5B24%5D:importnumpyasnpv%3Dnp.arange(1,6)vOut%5B24%5D: array(%5B1, 2, 3, 4, 5%5D)In%5B25%5D:2*vOut%5B25%5D: array(%5B 2,  4,  6,  8, 10%5D)This approach can be beneficially applied to the Monte Carlo algorithm. Example 3-3provides the respective code, this time making use of NumPy's vectorization capabilities.Example 3-3. Monte Carlo valuation of European call option with NumPy (firstversion)## Monte Carlo valuation of European call options with NumPy# mcs_vector_numpy.py#importmathimportnumpyasnpfromtimeimporttimenp.random.seed(20000)t0%3Dtime()# ParametersS0%3D100.%3BK%3D105.%3BT%3D1.0%3Br%3D0.05%3Bsigma%3D0.2M%3D50%3Bdt%3DT/M%3BI%3D250000# Simulating I paths with M time stepsS%3Dnp.zeros((M%2B1,I))S%5B0%5D%3DS0fortinrange(1,M%2B1):z%3Dnp.random.standard_normal(I)# pseudorandom numbersS%5Bt%5D%3DS%5Bt-1%5D*np.exp((r-0.5*sigma**2)*dt%2Bsigma*math.sqrt(dt)*z)# vectorized operation per time step over all paths# Calculating the Monte Carlo estimatorC0%3Dmath.exp(-r*T)*np.sum(np.maximum(S%5B-1%5D-K,0))/I# Results outputtnp1%3Dtime()-t0","print%22European Option Value %257.3f%22%25C0print%22Duration in Seconds   %257.3f%22%25tnp1Let us run this script:In%5B26%5D:%25runmcs_vector_numpy.pyOut%5B26%5D: European Option Value   8.037         Duration in Seconds     1.215In%5B27%5D:round(tpy/tnp1,2)Out%5B27%5D: 28.2Vectorization brings a speedup of more than 30 times in comparison to pure Python.The estimated Monte Carlo value is again quite close to the benchmark value.The vectorization becomes obvious when the pseudorandom numbers are generated.In the line in question, 250,000 numbers are generated in a single step, i.e., a single lineof code:z%3Dnp.random.standard_normal(I)Similarly, this vector of pseudorandom numbers is applied to the discretization schemeat once per time step in a vectorized fashion. In that sense, the tasks that are accom%E2%80%90plished by the outer loop in Example 3-2 are now delegated to NumPy, avoiding the outerloop completely on the Python level.","Using vectorization with NumPy generally results in code that is morecompact,easiertoread(andmaintain),andfastertoexecute.Alltheseaspects are in general important for financial applications.","Using a different discretization scheme for the SDE in Equation 3-5can yield an evenmore compact implementation of the Monte Carlo algorithm. To this end, consider thelog version of the discretization in Equation 3-6, which takes on the form inEquation 3-8.Equation 3-8. Euler discretization of SDE (log version)logSt%3DlogSt%E2%88%92%CE%94t%2Br%E2%88%9212%CF%832%CE%94t%2B%CF%83%CE%94tztThis version is completely additive, allowing for an implementation of the Monte Carloalgorithm without any loop on the Python level. Example 3-4shows the resulting code.","Example 3-4. Monte Carlo valuation of European call option with NumPy (secondversion)## Monte Carlo valuation of European call options with NumPy (log version)# mcs_full_vector_numpy.py#importmathfromnumpyimport*fromtimeimporttime# star import for shorter coderandom.seed(20000)t0%3Dtime()# ParametersS0%3D100.%3BK%3D105.%3BT%3D1.0%3Br%3D0.05%3Bsigma%3D0.2M%3D50%3Bdt%3DT/M%3BI%3D250000# Simulating I paths with M time stepsS%3DS0*exp(cumsum((r-0.5*sigma**2)*dt%2Bsigma*math.sqrt(dt)*random.standard_normal((M%2B1,I)),axis%3D0))# sum instead of cumsum would also do# if only the final values are of interestS%5B0%5D%3DS0# Calculating the Monte Carlo estimatorC0%3Dmath.exp(-r*T)*sum(maximum(S%5B-1%5D-K,0))/I# Results outputtnp2%3Dtime()-t0print%22European Option Value %257.3f%22%25C0print%22Duration in Seconds   %257.3f%22%25tnp2Let us run this third simulation script.In%5B28%5D:%25runmcs_full_vector_numpy.pyOut%5B28%5D: European Option Value   8.166         Duration in Seconds     1.439The execution speed is somewhat slower compared to the first NumPy implementation.There might also be a trade-off between compactness and readability in that this im%E2%80%90plementation approach makes it quite difficult to grasp what exactly is going on on theNumPy level. However, it shows how far one can go sometimes with NumPyvectorization.","Finally, let us have a graphical look at the underlying mechanics (refer to Chapter 5 foran explanation of the matplotlibplotting library). First, we plot the first 10 simulatedpaths over all time steps. Figure 3-2 shows the output:In%5B29%5D:importmatplotlib.pyplotaspltplt.plot(S%5B:,:10%5D)plt.grid(True)plt.xlabel('time step')plt.ylabel('index level')Figure 3-2. The first 10 simulated index level pathsSecond, we want to see the frequency of the simulated index levels at the end of thesimulation period. Figure 3-3 shows the output, this time illustrating the (approxi%E2%80%90mately) log-normal distribution of the end-of-period index level values:In%5B30%5D:plt.hist(S%5B-1%5D,bins%3D50)plt.grid(True)plt.xlabel('index level')plt.ylabel('frequency')The same type of figure looks completely different for the option's end-of-period (ma%E2%80%90turity) inner values, as Figure 3-4 illustrates:In%5B31%5D:plt.hist(np.maximum(S%5B-1%5D-K,0),bins%3D50)plt.grid(True)plt.xlabel('option inner value')plt.ylabel('frequency')plt.ylim(0,50000)","Figure 3-3. Histogram of all simulated end-of-period index level valuesFigure 3-4. Histogram of all simulated end-of-period option inner valuesIn this case, the majority of the simluated values are zero, indicating that the Europeancall option expires worthless in a significant amount of cases. The exact number isgenerated through the following calculation:In%5B32%5D:sum(S%5B-1%5D%3CK)Out%5B32%5D: 133533This number might vary somewhat, of course, from simulation to simulation.","Technical analysis based on historical price information is a typical task finance pro%E2%80%90fessionals and interested amateurs engage in. On Wikipediayou find the followingdefinition:","In finance, technical analysis is a security analysis methodology for forecasting the di%E2%80%90rection of prices through the study of past market data, primarily price and volume.In what follows, we focus on the study of past market data for backtesting purposes,and not too much on using our insights to predict future price movements. Our objectof study is the benchmark index Standard %26 Poor's 500 (S%26P 500), which is generallyconsidered to be a good proxy for the wholestock market in the United States. This isdue to the high number of names included in the index and the total market capitali%E2%80%90zation represented by it. It also has highly liquid futures and options markets.We will read historical index level information from a web source and will implementa simple backtesting for a trading system based on trend signals. But first we need thedata to get started. To this end, we mainly rely on the pandaslibrary, which simplifiesa number of related technical issues. Since it is almost always used, we should also importNumPy by default:In%5B33%5D:importnumpyasnpimportpandasaspdimportpandas.io.dataasweb","In addition to NumPy and SciPy, there are only a couple of importantlibrariesthatformthefundamentalscientificandfinancialPythonstack.Amongthemispandas.Makesuretoalwayshavecurrent(sta%E2%80%90ble)versionsoftheselibrariesinstalled(butbeawareofpotentialsyntax and/or API changes).The sublibrary pandas.io.data contains the function DataReader, which helps withgetting financial time series data from different sources and in particular from the pop%E2%80%90ular Yahoo! Finance site. Let's retrieve the data we are looking for, starting on January1, 2000:In%5B34%5D:sp500%3Dweb.DataReader('%5EGSPC',data_source%3D'yahoo',start%3D'1/1/2000',end%3D'4/14/2014')sp500.info()Out%5B34%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E         DatetimeIndex: 3592 entries, 2000-01-03 00:00:00 to 2014-04-14 00:00:00         Data columns (total 6 columns):         Open         3592 non-null float64         High         3592 non-null float64         Low          3592 non-null float64         Close        3592 non-null float64         Volume       3592 non-null int64         Adj Close    3592 non-null float64         dtypes: float64(5), int64(1)","DataReaderhas connected to the data source via an Internet connection and has givenback the time series data for the S%26P 500 index, from the first trading day in 2000 untilthe end date. It has also generated automatically a time index with Timestamp objects.To get a first impression, we can plot the closing quotes over time. This gives an outputlike that in Figure 3-5:In%5B35%5D:sp500%5B'Close'%5D.plot(grid%3DTrue,figsize%3D(8,5))Figure 3-5. Historical levels of the S%26P 500 indexThe trend strategy we want to implement is based on both a two-month(i.e., 42 tradingdays) and a one-year(i.e., 252 trading days) trend(i.e., the moving average of the indexlevel for the respective period). Again, pandasmakes it efficient to generate the respec%E2%80%90tive time series and to plot the three relevant time series in a single figure. First, thegeneration of the trend data:In%5B36%5D:sp500%5B'42d'%5D%3Dnp.round(pd.rolling_mean(sp500%5B'Close'%5D,window%3D42),2)sp500%5B'252d'%5D%3Dnp.round(pd.rolling_mean(sp500%5B'Close'%5D,window%3D252),2)In this example, the first line simultaneously adds a new columnto the pandasDataFrameobject and puts in the valuesfor the 42-day trend. The second line does the same withrespect to the 252-day trend. Consequently, we now have two new columns. These havefewer entries due to the very nature of the data we have generated for these columns-i.e., they start only at those dates when 42 and 252 observation points, respectively, areavailable for the first time to calculate the desired statistics:In%5B37%5D:sp500%5B%5B'Close','42d','252d'%5D%5D.tail()Out%5B37%5D:               Close      42d     252d         Date         2014-04-08  1851.96  1853.88  1728.66         2014-04-09  1872.18  1855.66  1729.79         2014-04-10  1833.08  1856.46  1730.74","         2014-04-11  1815.69  1856.36  1731.64         2014-04-14  1830.61  1856.63  1732.74Second, the plotting of the new data. The resulting plot in Figure 3-6 already providessome insights into what was going on in the past with respect to upward and downwardtrends:In%5B38%5D:sp500%5B%5B'Close','42d','252d'%5D%5D.plot(grid%3DTrue,figsize%3D(8,5))Figure 3-6. The S%26P 500 index with 42d and 252d trend linesOur basic data set is mainly complete, such that we now can devise a rule to generatetrading signals. The rule says the following:Buy signal (go long)the 42d trend is for the first time SD points above the 252d trend.Wait (park in cash)the 42d trend is within a range of %2B/%E2%80%93 SD points around the 252d trend.Sell signal (go short)the 42d trend is for the first time SD points below the 252d trend.To this end, we add a new column to the pandasDataFrameobject for the differencesbetween the two trends. As you can see, numerical operations with pandascan in generalbe implemented in a vectorized fashion, in that one can take the difference between twowhole columns:In%5B39%5D:sp500%5B'42-252'%5D%3Dsp500%5B'42d'%5D-sp500%5B'252d'%5Dsp500%5B'42-252'%5D.tail()Out%5B39%5D: Date         2014-04-08    125.22         2014-04-09    125.87         2014-04-10    125.72","         2014-04-11    124.72         2014-04-14    123.89         Name: 42-252, dtype: float64On the last available trading date the 42d trend lies well above the 252d trend. Althoughthe number of entries in the two trend columns is not equal, pandastakes care of thisby putting NaN values at the respective index positions:In%5B40%5D:sp500%5B'42-252'%5D.head()Out%5B40%5D: Date         2000-01-03   NaN         2000-01-04   NaN         2000-01-05   NaN         2000-01-06   NaN         2000-01-07   NaN         Name: 42-252, dtype: float64To make it more formal, we again generate a new column for what we call a regime. Weassume a value of 50 for the signal threshold:In%5B41%5D:SD%3D50sp500%5B'Regime'%5D%3Dnp.where(sp500%5B'42-252'%5D%3ESD,1,0)sp500%5B'Regime'%5D%3Dnp.where(sp500%5B'42-252'%5D%3C-SD,-1,sp500%5B'Regime'%5D)sp500%5B'Regime'%5D.value_counts()Out%5B41%5D:  1    1489          0    1232         -1     871         dtype: int64In words, on 1,489 trading dates, the 42d trend lies more than SDpoints above the 252dtrend. On 1,232 days, the 42d trend is more than SDpoints below the 252d trend. Ob%E2%80%90viously, if the short-term trend crosses the line of the long-term trend it tends to restthere for a (longer) while. This is what we call regime and what is illustrated inFigure 3-7, which is generated by the following two lines of code:In%5B42%5D:sp500%5B'Regime'%5D.plot(lw%3D1.5)plt.ylim(%5B-1.1,1.1%5D)","Figure 3-7. Signal regimes over timeEverything is now available to test the investment strategy based on the signals. Weassume for simplicity that an investor can directly invest in the index or can directlyshort the index, which in the real world must be accomplished by using index funds,exchange-traded funds, or futures on the index, for example. Such trades inevitably leadto transaction costs, which we neglect here. This seems justifiable since we do not planto trade %E2%80%9Ctoo often.%E2%80%9DBased on the respective regime, the investor either is long or short in the market (index)or parks his wealth in cash, which does not bear any interest. This simplified strategyallows us to work with market returns only. The investor makes the market return whenhe is long (1), makes the negative market returns when he is short (%E2%80%931), and makes noreturns (0) when he parks his wealth in cash. We therefore need the returns first. InPython, we have the following vectorized pandasoperation to calculate the log returns.Note that the shiftmethod shifts a time series by as many index entries as desired-in our case by one trading day, such that we get daily log returns:In%5B43%5D:sp500%5B'Market'%5D%3Dnp.log(sp500%5B'Close'%5D/sp500%5B'Close'%5D.shift(1))Recalling how we constructed our regimes, it is now simple to get the returns of thetrend-based trading strategy-we just have to multiply our Regime column, shifted byone day, by the Returns columns (the position is built %E2%80%9Cyesterday%E2%80%9D and yields %E2%80%9Ctoday's%E2%80%9Dreturns):In%5B44%5D:sp500%5B'Strategy'%5D%3Dsp500%5B'Regime'%5D.shift(1)*sp500%5B'Market'%5DThe strategy pays off well%3B the investor is able to lock in a much higher return over therelevant period than a plain long investment would provide. Figure 3-8 compares thecumulative, continuous returns of the index with the cumulative, continuous returns ofour strategy:In%5B45%5D:sp500%5B%5B'Market','Strategy'%5D%5D.cumsum().apply(np.exp).plot(grid%3DTrue,figsize%3D(8,5))","Figure 3-8. The S%26P 500 index vs. investor's wealthFigure 3-8 shows that especially during market downturns (2003 and 2008/2009) theshorting of the market yields quite high returns. Although the strategy does not capturethe whole upside during bullish periods, the strategy as a whole outperforms the marketquite significantly.However, we have to keep in mind that we completely neglect operational issues (liketrade execution) and relevant market microstructure elements (e.g., transaction costs).For example, we are working with daily closing values. A question would be when toexecute an exit from the market (from being long to being neutral/in cash): on the sameday at the closing value or the next day at the opening value. Such considerations forsure have an impact on the performance, but the overall result would probably persist.Also, transaction costs generally diminish returns, but the trading rule does not generatetoo many signals.","Wheneveritcomestotheanalysisoffinancialtimeseries,considerusing pandas. Almost any time series-related problem can be tackledwith this powerful library.","Without going into too much detail, this chapter illustrates the use of Python by themeans of concrete and typical financial examples:Calculation of implied volatilitiesUsing real-world data, in the form of a cross section of option data for a given day,we calculate numerically the implied volatilities of European call options on the","VSTOXX volatility index. This example introduces some custom Pythonfunctions(e.g., for analytical option valuation) and uses functionality from NumPy, SciPy, andpandas.Monte Carlo simulationUsing different implementation approaches, we simulate the evolution of an indexlevel over time and use our simulated end-of-period values to derive Monte Carloestimators for European call options. Using NumPy, the major benefits of vectori%E2%80%90zation of Python code are illustrated: namely, compactness of code and speed ofexecution.Backtesting of trend signal strategyUsing real historical time series data for the S%26P 500, we backtest the performanceof a trading strategy based on signals generated by 42-day and 252-day trends(moving averages). This example illustrates the capabilities and convenience ofpandas when it comes to time series analytics.In terms of working with Python, this chapter introduces interactive financial analytics(using the IPythoninteractive shell), working with more complex functions stored inmodules, as well as the performance-oriented implementation of algorithms using vec%E2%80%90torization. One important topic is not covered: namely, object orientation and classesin Python. For the curious reader, Appendix B contains a class definition for a Europeancall option with methods based on the functions found in the code of Example 3-1 inthis chapter.","The major references used in this chapter are:%E2%80%A2Black, Fischer and Myron Scholes (1973): %E2%80%9CThe Pricing of Options and CorporateLiabilities.%E2%80%9D Journal of Political Economy, Vol. 81, No. 3, pp. 638-659.%E2%80%A2Hilpisch, Yves (2015): Derivatives Analytics with Python. Wiley Finance, Chiches%E2%80%90ter, England. http://www.derivatives-analytics-with-python.com.%E2%80%A2Hilpisch, Yves (2013): %E2%80%9CEfficient Data and Financial Analytics with Python.%E2%80%9D Soft%E2%80%90ware Developer's Journal, No. 13, pp. 56-65. http://hilpisch.com/YH_Efficient_Analytics_Article.pdf.%E2%80%A2Merton, Robert (1973): %E2%80%9CTheory of Rational Option Pricing.%E2%80%9D Bell Journal of Eco%E2%80%90nomics and Management Science, Vol. 4, pp. 141-183.","This part of the book represents its core. It introduces the most important Pythonlibraries, techniques, and approaches for financial analytics and application develop%E2%80%90ment. The sheer number of topics covered in this part makes it necessary to focus mainlyon selected, and partly rather specific, examples and use cases.The chapters are organized according to certain topics such that this part can be usedas a reference to which the reader can come to look up examples and details related toa topic of interest. This core part of the book consists of the following chapters:%E2%80%A2Chapter 4 on Python data types and structures%E2%80%A2Chapter 5 on 2D and 3D visualization with matplotlib%E2%80%A2Chapter 6 on the handling of financial time series data%E2%80%A2Chapter 7 on (performant) input/output operations%E2%80%A2Chapter 8 on performance techniques and libraries%E2%80%A2Chapter 9 on several mathematical tools needed in finance%E2%80%A2Chapter 10 on random number generation and simulation of stochastic processes%E2%80%A2Chapter 11 on statistical applications with Python%E2%80%A2Chapter 12 on the integration of Python and Excel%E2%80%A2Chapter 13on object-oriented programming with Pythonand the development of(simple) graphical user interfaces (GUIs)%E2%80%A2Chapter 14 on the integration of Python with web technologies as well as the de%E2%80%90velopment of web-based applications and web services","Bad programmers worry about the code. Good programmersworry about data structures and their relationships.- Linus TorvaldsThis chapter introduces basic data types and data structures of Python. Although thePython interpreter itself already brings a rich variety of data structures with it, NumPyand other libraries add to these in a valuable fashion.The chapter is organized as follows:Basic data typesThe first section introduces basic data types such as int, float, and string.Basic data structuresThe next section introduces the fundamental data structures of Python (e.g., listobjects) and illustrates control structures, functional programming paradigms, andanonymous functions.NumPy data structuresThe following section is devoted to the characteristics and capabilities of the NumPyndarray class and illustrates some of the benefits of this class for scientific andfinancial applications.Vectorization of codeAs the final section illustrates, thanks to NumPy's array class vectorized code is easilyimplemented, leading to more compact and also better-performing code.The spirit of this chapter is to provide a general introduction to Python specifics whenit comes to data types and structures. If you are equipped with a background fromanother programing language, say  C  or Matlab, you should be able to easily grasp the","1.The Cython librarybrings static typing and compiling features to Python that are comparable to those in  C .In fact, Cython is a hybrid language of Python and  C .differences that Python usage might bring along. The topics introduced here are allimportant and fundamental for the chapters to come.","Python is a dynamically typed language, which means that the Pythoninterpreter infersthe type of an object at runtime. In comparison, compiled languages like  C are generallystatically typed. In these cases, the type of an object has to be attached to the object beforecompile time.1","One of the most fundamental data types is the integer, or int:In%5B1%5D:a%3D10type(a)Out%5B1%5D: intThe built-in function typeprovides type information for all objects with standard andbuilt-in types as well as for newly created classes and objects. In the latter case, theinformation provided depends on the description the programmer has stored with theclass. There is a saying that %E2%80%9Ceverything in Python is an object.%E2%80%9D This means, for example,that even simple objects like the intobject we just defined have built-in methods. Forexample, you can get the number of bits needed to represent the int object in-memoryby calling the method bit_length:In%5B2%5D:a.bit_length()Out%5B2%5D: 4You will see that the number of bits needed increases the higher the integer value is thatwe assign to the object:In%5B3%5D:a%3D100000a.bit_length()Out%5B3%5D: 17In general, there are so many different methods that it is hard to memorize all methodsof all classes and objects. Advanced Pythonenvironments, like IPython, provide tabcompletion capabilities that show all methods attached to an object. You simply typethe object name followed by a dot (e.g., a.) and then press the Tab key, e.g., a.tab. Thisthen provides a collection of methods you can call on the object. Alternatively, the","2.Here and in the following discussion, terms like float, float object, etc. are used interchangeably, acknowl%E2%80%90edging that every float is also an object. The same holds true for other object types.Python built-in function dir gives a complete list of attributes and methods of anyobject.A specialty of Python is that integers can be arbitrarily large. Consider, for example, thegoogol number 10100. Pythonhas no problem with such large numbers, which are tech%E2%80%90nically long objects:In%5B4%5D:googol%3D10**100googolOut%5B4%5D: 100000000000000000000000000000000000000000000000000000000000000000000000        00000000000000000000000000000LIn%5B5%5D:googol.bit_length()Out%5B5%5D: 333","Pythonintegerscanbearbitrarilylarge.Theinterpretersimplyusesas many bits/bytes as needed to represent the numbers.It is important to note that mathematical operations on int objects return int objects.This can sometimes lead to confusion and/or hard-to-detect errors in mathematicalroutines. The following expression yields the expected result:In%5B6%5D:1%2B4Out%5B6%5D: 5However, the next case may return a somewhat surprising result:In%5B7%5D:1/4Out%5B7%5D: 0In%5B8%5D:type(1/4)Out%5B8%5D: int","For the last expression to return the generally desired result of 0.25, we must operate onfloatobjects, which brings us naturally to the next basic data type. Adding a dot to aninteger value, like in 1. or 1.0, causes Python to interpret the object as a float. Ex%E2%80%90pressions involving a float also return a float object in general:2","3.Cf. http://en.wikipedia.org/wiki/Double-precision_floating-point_format.In%5B9%5D:1./4Out%5B9%5D: 0.25In%5B10%5D:type(1./4)Out%5B10%5D: floatA float is a bit more involved in that the computerized representation of rational orreal numbers is in general not exact and depends on the specific technical approachtaken. To illustrate what this implies, let us define another float object:In%5B11%5D:b%3D0.35type(b)Out%5B11%5D: floatfloat objects like this one are always represented internally up to a certain degree ofaccuracy only. This becomes evident when adding 0.1 to b:In%5B12%5D:b%2B0.1Out%5B12%5D: 0.44999999999999996The reason for this is that floats are internally represented in binary format%3B that is, adecimal number 0 %3C n %3C 1 is represented by a series of the form n%3Dx2%2By4%2Bz8%2B.... Forcertain floating-point numbers the binary representation might involve a large numberof elements or might even be an infinite series. However, given a fixed number of bitsused to represent such a number-i.e., a fixed number of terms in the representationseries-inaccuracies are the consequence. Other numbers can be represented perfectlyand are therefore stored exactly even with a finite number of bits available. Considerthe following example:In%5B13%5D:c%3D0.5c.as_integer_ratio()Out%5B13%5D: (1, 2)One half, i.e., 0.5, is stored exactly because it has an exact (finite) binary representationas 0.5%3D12. However, for b %3D 0.35 we get something different than the expected rationalnumber 0.35%3D720:In%5B14%5D:b.as_integer_ratio()Out%5B14%5D: (3152519739159347, 9007199254740992)The precision is dependent on the number of bits used to represent the number. Ingeneral, all platforms that Pythonruns on use the IEEE 754 double-precision standard(i.e., 64 bits), for internal representation.3 This translates into a 15-digit relative accuracy.","Since this topic is of high importance for several application areas in finance, it is some%E2%80%90times necessary to ensure the exact, or at least best possible, representation of numbers.For example, the issue can be of importance when summing over a large set of numbers.In such a situation, a certain kind and/or magnitude of representation error might, inaggregate, lead to significant deviations from a benchmark value.The module decimal provides an arbitrary-precision object for floating-point numbersand several options to address precision issues when working with such numbers:In%5B15%5D:importdecimalfromdecimalimportDecimalIn%5B16%5D:decimal.getcontext()Out%5B16%5D: Context(prec%3D28, rounding%3DROUND_HALF_EVEN, Emin%3D-999999999, Emax%3D999999         999, capitals%3D1, flags%3D%5B%5D, traps%3D%5BOverflow, InvalidOperation, DivisionB         yZero%5D)In%5B17%5D:d%3DDecimal(1)/Decimal(11)dOut%5B17%5D: Decimal('0.09090909090909090909090909091')You can change the precision of the representation by changing the respective attributevalue of the Context object:In%5B18%5D:decimal.getcontext().prec%3D4# lower precision than defaultIn%5B19%5D:e%3DDecimal(1)/Decimal(11)eOut%5B19%5D: Decimal('0.09091')In%5B20%5D:decimal.getcontext().prec%3D50# higher precision than defaultIn%5B21%5D:f%3DDecimal(1)/Decimal(11)fOut%5B21%5D: Decimal('0.090909090909090909090909090909090909090909090909091')If needed, the precision can in this way be adjusted to the exact problem at hand andone can operate with floating-point objects that exhibit different degrees of accuracy:In%5B22%5D:g%3Dd%2Be%2BfgOut%5B22%5D: Decimal('0.27272818181818181818181818181909090909090909090909')","Themoduledecimalprovidesanarbitrary-precisionfloating-pointnumberobject.Infinance,itmightsometimesbenecessarytoensurehighprecisionandtogobeyondthe64-bitdouble-precisionstandard.","Now that we can represent natural and floating-point numbers, we turn to text. Thebasic data type to represent text in Python is the string. The string object has a numberof really helpful built-in methods. In fact, Pythonis generally considered to be a goodchoice when it comes to working with text files of any kind and any size. A stringobjectis generally defined by single or double quotation marks or by converting another objectusing the str function (i.e., using the object's standard or user-defined stringrepresentation):In%5B23%5D:t%3D'this is a string object'With regard to the built-in methods, you can, for example, capitalize the first word inthis object:In%5B24%5D:t.capitalize()Out%5B24%5D: 'This is a string object'Or you can split it into its single-word components to get a listobject of all the words(more on list objects later):In%5B25%5D:t.split()Out%5B25%5D: %5B'this', 'is', 'a', 'string', 'object'%5DYou can also search for a word and get the position (i.e., index value) of the first letterof the word back in a successful case:In%5B26%5D:t.find('string')Out%5B26%5D: 10If the word is not in the string object, the method returns -1:In%5B27%5D:t.find('Python')Out%5B27%5D: -1Replacing characters in a string is a typical task that is easily accomplished with thereplace method:In%5B28%5D:t.replace(' ','%7C')Out%5B28%5D: 'this%7Cis%7Ca%7Cstring%7Cobject'The stripping of strings-i.e., deletion of certain leading/lagging characters-is alsooften necessary:In%5B29%5D:'http://www.python.org'.strip('htp:/')Out%5B29%5D: 'www.python.org'Table 4-1 lists a number of helpful methods of the string object.","4.It is not possible to go into details here, but there is a wealth of information available on the Internet aboutregular expressions in general and for Pythonin particular. For an introduction to this topic, refer to Fitz%E2%80%90gerald, Michael (2012): Introducing Regular Expressions. O'Reilly, Sebastopol, CA.Table 4-1. Selected string methods","capitalize()Copy of the string with first letter capitalizedcount(sub%5B,start%5B,end%5D%5D)Count of the number of occurrences of substringdecode(%5Bencoding%5B,errors%5D%5D)Decoded version of the string, using encoding (e.g., UTF-8)encode(%5Bencoding%5B,errors%5D%5D)Encoded version of the stringfind(sub%5B,start%5B,end%5D%5D)(Lowest) index where substring is foundjoin(seq)Concatenation of strings in sequence seqreplace(old,new%5B,count%5D)Replaces old by new the first count timessplit(%5Bsep%5B,maxsplit%5D%5D)List of words in string with sep as separatorsplitlines(%5Bkeepends%5D)Separated lines with line ends/breaks if keepends is Truestrip(chars)Copy of string with leading/lagging characters in chars removedupper()Copy with all letters capitalizedA powerful tool when working with string objects is regular expressions. Python pro%E2%80%90vides such functionality in the module re:In%5B30%5D:importreSuppose you are faced with a large text file, such as a comma-separated value (CSV) file,which contains certain time series and respective date-time information. More oftenthan not, the date-time information is delivered in a format that Python cannot interpretdirectly. However, the date-time information can generally be described by a regularexpression. Consider the following string object, containing three date-time elements,three integers, and three strings. Note that triple quotation marks allow the definitionof strings over multiple rows:In%5B31%5D:series%3D%22%22%22         '01/18/2014 13:00:00', 100, '1st'%3B         '01/18/2014 13:30:00', 110, '2nd'%3B         '01/18/2014 14:00:00', 120, '3rd'         %22%22%22The following regular expression describes the format of the date-time informationprovided in the string object:4In%5B32%5D:dt%3Dre.compile(%22'%5B0-9/:%5Cs%5D%2B'%22)# datetime","Equipped with this regular expression, we can go on and find all the date-time elements.In general, applying regular expressions to string objects also leads to performanceimprovements for typical parsing tasks:In%5B33%5D:result%3Ddt.findall(series)resultOut%5B33%5D: %5B%22'01/18/2014 13:00:00'%22, %22'01/18/2014 13:30:00'%22, %22'01/18/2014 14:00:0         0'%22%5D","Whenparsingstringobjects,considerusingregularexpressions,whichcanbringbothconvenienceandperformancetosuchoperations.The resulting stringobjects can then be parsed to generate Python datetime objects(cf. Appendix Cfor an overview of handling date and time data with Python). To parsethe stringobjects containing the date-time information, we need to provide informa%E2%80%90tion of how to parse-again as a string object:In%5B34%5D:fromdatetimeimportdatetimepydt%3Ddatetime.strptime(result%5B0%5D.replace(%22'%22,%22%22),'%25m/%25d/%25Y %25H:%25M:%25S')pydtOut%5B34%5D: datetime.datetime(2014, 1, 18, 13, 0)In%5B35%5D:printpydtOut%5B35%5D: 2014-01-18 13:00:00In%5B36%5D:printtype(pydt)Out%5B36%5D: %3Ctype 'datetime.datetime'%3ELater chapters provide more information on date-time data, the handling of such data,and datetimeobjects and their methods. This is just meant to be a teaser for this im%E2%80%90portant topic in finance.","As a general rule, data structures are objects that contain a possibly large number ofother objects. Among those that Python provides as built-in structures are:tupleA collection of arbitrary objects%3B only a few methods availablelistA collection of arbitrary objects%3B many methods available","dictA key-value store objectsetAn unordered collection object for other unique objects","A tupleis an advanced data structure, yet it's still quite simple and limited in its appli%E2%80%90cations. It is defined by providing objects in parentheses:In%5B37%5D:t%3D(1,2.5,'data')type(t)Out%5B37%5D: tupleYou can even drop the parentheses and provide multiple objects separated by commas:In%5B38%5D:t%3D1,2.5,'data'type(t)Out%5B38%5D: tupleLike almost all data structures in Python the tuplehas a built-in index, with the helpof which you can retrieve single or multiple elements of the tuple. It is important toremember that Python uses zero-based numbering, such that the third element of a tupleis at index position 2:In%5B39%5D:t%5B2%5DOut%5B39%5D: 'data'In%5B40%5D:type(t%5B2%5D)Out%5B40%5D: str","IncontrasttosomeotherprogramminglanguageslikeMatlab,Pythonuseszero-basednumberingschemes.Forexample,thefirstelement of a tuple object has index value 0.There are only two special methods that this object type provides: count and index. Thefirst counts the number of occurrences of a certain object and the second gives the indexvalue of the first appearance of it:","In%5B41%5D:t.count('data')Out%5B41%5D: 1In%5B42%5D:t.index(1)Out%5B42%5D: 0tuple objects are not very flexible since, once defined, they cannot be changed easily.","Objects of type list are much more flexible and powerful in comparison to tupleobjects. From a finance point of view, you can achieve a lot working only with listobjects, such as storing stock price quotes and appending new data. A list object isdefined through brackets and the basic capabilities and behavior are similar to those oftuple objects:In%5B43%5D:l%3D%5B1,2.5,'data'%5Dl%5B2%5DOut%5B43%5D: 'data'list objects can also be defined or converted by using the function list. The followingcode generates a new list object by converting the tuple object from the previousexample:In%5B44%5D:l%3Dlist(t)lOut%5B44%5D: %5B1, 2.5, 'data'%5DIn%5B45%5D:type(l)Out%5B45%5D: listIn addition to the characteristics of tuple objects, list objects are also expandable andreducible via different methods. In other words, whereas string and tuple objects areimmutablesequence objects (with indexes) that cannot be changed once created, listobjects are mutableand can be changed via different operations. You can append listobjects to an existing list object, and more:In%5B46%5D:l.append(%5B4,3%5D)# append list at the endlOut%5B46%5D: %5B1, 2.5, 'data', %5B4, 3%5D%5DIn%5B47%5D:l.extend(%5B1.0,1.5,2.0%5D)# append elements of listlOut%5B47%5D: %5B1, 2.5, 'data', %5B4, 3%5D, 1.0, 1.5, 2.0%5DIn%5B48%5D:l.insert(1,'insert')# insert object before index positionl","Out%5B48%5D: %5B1, 'insert', 2.5, 'data', %5B4, 3%5D, 1.0, 1.5, 2.0%5DIn%5B49%5D:l.remove('data')# remove first occurrence of objectlOut%5B49%5D: %5B1, 'insert', 2.5, %5B4, 3%5D, 1.0, 1.5, 2.0%5DIn%5B50%5D:p%3Dl.pop(3)# removes and returns object at indexprintl,pOut%5B50%5D: %5B1, 'insert', 2.5, 1.0, 1.5, 2.0%5D %5B4, 3%5DSlicing is also easily accomplished. Here, slicing refers to an operation that breaks downa data set into smaller parts (of interest):In%5B51%5D:l%5B2:5%5D# 3rd to 5th elementsOut%5B51%5D: %5B2.5, 1.0, 1.5%5DTable 4-2 provides a summary of selected operations and methods of the list object.Table 4-2. Selected operations and methods of list objects","l%5Bi%5D %3D x%5Bi%5DReplaces ith element by xl%5Bi:j:k%5D %3D s%5Bi:j:k%5DReplaces every kth element from i to j - 1 by sappend(x)Appends x to objectcount(x)Number of occurrences of object xdel l%5Bi:j:k%5D%5Bi:j:k%5DDeletes elements with index values i to j %E2%80%93 1extend(s)Appends all elements of s to objectindex(x%5B,i%5B,j%5D%5D)First index of x between elements i and j %E2%80%93 1insert(i,x)%2B%2BInserts x at/before index iremove(i)Removes element with index ipop(i)Removes element with index i and return itreverse()Reverses all items in placesort(%5Bcmp%5B,key%5B,reverse%5D%5D%5D)Sorts all items in place","Although a topic in itself, control structures like for loops are maybe best introducedin Python based on list objects. This is due to the fact that looping in general takesplace over listobjects, which is quite different to what is often the standard in otherlanguages. Take the following example. The for loop loops over the elements of thelist object lwith index values 2 to 4 and prints the square of the respective elements.Note the importance of the indentation (whitespace) in the second line:In%5B52%5D:forelementinl%5B2:5%5D:printelement**2","Out%5B52%5D: 6.25         1.0         2.25This provides a really high degree of flexibility in comparison to the typical counter-based looping. Counter-based looping is also an option with Python, but is accom%E2%80%90plished based on the (standard) list object range:In%5B53%5D:r%3Drange(0,8,1)# start, end, step widthrOut%5B53%5D: %5B0, 1, 2, 3, 4, 5, 6, 7%5DIn%5B54%5D:type(r)Out%5B54%5D: listFor comparison, the same loop is implemented using range as follows:In%5B55%5D:foriinrange(2,5):printl%5Bi%5D**2Out%5B55%5D: 6.25         1.0         2.25","In Python you can loop over arbitrary list objects, no matter whatthecontentoftheobjectis.Thisoftenavoidstheintroductionofacounter.Python also provides the typical (conditional) control elements if, elif, and else. Theiruse is comparable in other languages:In%5B56%5D:foriinrange(1,10):ifi%252%3D%3D0:# %25 is for moduloprint%22%25d is even%22%25ielifi%253%3D%3D0:print%22%25d is multiple of 3%22%25ielse:print%22%25d is odd%22%25iOut%5B56%5D: 1 is odd         2 is even         3 is multiple of 3         4 is even         5 is odd         6 is even         7 is odd         8 is even         9 is multiple of 3","Similarly, while provides another means to control the flow:In%5B57%5D:total%3D0whiletotal%3C100:total%2B%3D1printtotalOut%5B57%5D: 100A specialty of Python is so-called listcomprehensions. Instead of looping over existinglist objects, this approach generates list objects via loops in a rather compact fashion:In%5B58%5D:m%3D%5Bi**2foriinrange(5)%5DmOut%5B58%5D: %5B0, 1, 4, 9, 16%5DIn a certain sense, this already provides a first means to generate %E2%80%9Csomething like%E2%80%9D vec%E2%80%90torized code in that loops are rather more implicit than explicit (vectorization of codeis discussed in more detail later in this chapter).","Pythonprovides a number of tools for functional programming support as well-i.e.,the application of a function to a whole set of inputs (in our case list objects). Amongthese tools are filter, map, and reduce. However, we need a function definition first.To start with something really simple, consider a function fthat returns the square ofthe input x:In%5B59%5D:deff(x):returnx**2f(2)Out%5B59%5D: 4Of course, functions can be arbitrarily complex, with multiple input/parameter objectsand even multiple outputs, (return objects). However, consider the following function:In%5B60%5D:defeven(x):returnx%252%3D%3D0even(3)Out%5B60%5D: FalseThe return object is a Boolean. Such a function can be applied to a whole list objectby using map:In%5B61%5D:map(even,range(10))Out%5B61%5D: %5BTrue, False, True, False, True, False, True, False, True, False%5DTo this end, we can also provide a function definition directly as an argument to map,by using lambda or anonymous functions:","In%5B62%5D:map(lambdax:x**2,range(10))Out%5B62%5D: %5B0, 1, 4, 9, 16, 25, 36, 49, 64, 81%5DFunctions can also be used to filter a list object. In the following example, the filterreturns elements of a list object that match the Boolean condition as defined by theeven function:In%5B63%5D:filter(even,range(15))Out%5B63%5D: %5B0, 2, 4, 6, 8, 10, 12, 14%5DFinally, reducehelps when we want to apply a function to all elements of a list objectthat returns a single value only. An example is the cumulative sum of all elements in alist object (assuming that summation is defined for the objects contained in the list):In%5B64%5D:reduce(lambdax,y:x%2By,range(10))Out%5B64%5D: 45An alternative, nonfunctional implementation could look like the following:In%5B65%5D:defcumsum(l):total%3D0foreleminl:total%2B%3Delemreturntotalcumsum(range(10))Out%5B65%5D: 45","It can be considered good practice to avoid loops on the Python levelasfaraspossible.listcomprehensionsandfunctionalprogrammingtoolslikemap,filter,andreduceprovidemeanstowritecodewithoutloopsthatisbothcompactandingeneralmorereadable.lambdaoranonymousfunctionsarealsopowerfultoolsinthiscontext.","dict objects are dictionaries, and also mutable sequences, that allow data retrieval bykeys that can, for example, be string objects. They are so-called key-value stores. Whilelistobjects are ordered and sortable, dictobjects are unordered and unsortable. Anexample best illustrates further differences to listobjects. Curly brackets are whatdefine dict objects:In%5B66%5D:d%3D%7B'Name':'Angela Merkel','Country':'Germany','Profession':'Chancelor',","'Age':60%7Dtype(d)Out%5B66%5D: dictIn%5B67%5D:printd%5B'Name'%5D,d%5B'Age'%5DOut%5B67%5D: Angela Merkel 60Again, this class of objects has a number of built-in methods:In%5B68%5D:d.keys()Out%5B68%5D: %5B'Country', 'Age', 'Profession', 'Name'%5DIn%5B69%5D:d.values()Out%5B69%5D: %5B'Germany', 60, 'Chancelor', 'Angela Merkel'%5DIn%5B70%5D:d.items()Out%5B70%5D: %5B('Country', 'Germany'),          ('Age', 60),          ('Profession', 'Chancelor'),          ('Name', 'Angela Merkel')%5DIn%5B71%5D:birthday%3DTrueifbirthdayisTrue:d%5B'Age'%5D%2B%3D1printd%5B'Age'%5DOut%5B71%5D: 61There are several methods to get iterator objects from the dictobject. The objectsbehave like list objects when iterated over:In%5B72%5D:foritemind.iteritems():printitemOut%5B72%5D: ('Country', 'Germany')         ('Age', 61)         ('Profession', 'Chancelor')         ('Name', 'Angela Merkel')In%5B73%5D:forvalueind.itervalues():printtype(value)Out%5B73%5D: %3Ctype 'str'%3E         %3Ctype 'int'%3E         %3Ctype 'str'%3E         %3Ctype 'str'%3ETable 4-3 provides a summary of selected operations and methods of the dict object.","Table 4-3. Selected operations and methods of dict objects","d%5Bk%5D%5Bk%5DItem of d with key kd%5Bk%5D %3D x%5Bk%5DSets item key k to xdel d%5Bk%5D%5Bk%5DDeletes item with key kclear()Removes all itemscopy()Makes a copyhas_key(k)True if k is a keyitems()Copy of all key-value pairsiteritems()Iterator over all itemsiterkeys()Iterator over all keysitervalues()Iterator over all valueskeys()Copy of all keyspoptiem(k)Returns and removes item with key kupdate(%5Be%5D)Updates items with items from evalues()Copy of all values","The last data structure we will consider is the setobject. Although set theory is a cor%E2%80%90nerstone of mathematics and also finance theory, there are not too many practical ap%E2%80%90plications for set objects. The objects are unordered collections of other objects,containing every element only once:In%5B74%5D:s%3Dset(%5B'u','d','ud','du','d','du'%5D)sOut%5B74%5D: %7B'd', 'du', 'u', 'ud'%7DIn%5B75%5D:t%3Dset(%5B'd','dd','uu','u'%5D)With setobjects, you can implement operations as you are used to in mathematical settheory. For example, you can generate unions, intersections, and differences:In%5B76%5D:s.union(t)# all of s and tOut%5B76%5D: %7B'd', 'dd', 'du', 'u', 'ud', 'uu'%7DIn%5B77%5D:s.intersection(t)# both in s and tOut%5B77%5D: %7B'd', 'u'%7DIn%5B78%5D:s.difference(t)# in s but not tOut%5B78%5D: %7B'du', 'ud'%7DIn%5B79%5D:t.difference(s)# in t but not s","Out%5B79%5D: %7B'dd', 'uu'%7DIn%5B80%5D:s.symmetric_difference(t)# in either one but not bothOut%5B80%5D: %7B'dd', 'du', 'ud', 'uu'%7DOne application of set objects is to get rid of duplicates in a list object. For example:In%5B81%5D:fromrandomimportrandintl%3D%5Brandint(0,10)foriinrange(1000)%5D# 1,000 random integers between 0 and 10len(l)# number of elements in lOut%5B81%5D: 1000In%5B82%5D:l%5B:20%5DOut%5B82%5D: %5B8, 3, 4, 9, 1, 7, 5, 5, 6, 7, 4, 4, 7, 1, 8, 5, 0, 7, 1, 9%5DIn%5B83%5D:s%3Dset(l)sOut%5B83%5D: %7B0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10%7D","The previous section shows that Python provides some quite useful and flexible generaldata structures. In particular, list objects can be considered a real workhorse with manyconvenient characteristics and application areas. However, scientific and financial ap%E2%80%90plications generally have a need for high-performing operations on special data struc%E2%80%90tures. One of the most important data structures in this regard is the array. Arraysgenerally structure other (fundamental) objects in rows and columns.Assume for the moment that we work with numbers only, although the concept gen%E2%80%90eralizes to other types of data as well. In the simplest case, a one-dimensional array thenrepresents, mathematically speaking, a vector of, in general, real numbers, internallyrepresented by float objects. It then consists of a single row or column of elements only.In a more common case, an array represents an i %C3%97 jmatrixof elements. This conceptgeneralizes to i %C3%97 j %C3%97 kcubes of elements in three dimensions as well as to general n-dimensional arrays of shape i %C3%97 j %C3%97 k %C3%97 l %C3%97 %E2%80%A6 .Mathematical disciplines like linear algebra and vector space theory illustrate that suchmathematical structures are of high importance in a number of disciplines and fields.It can therefore prove fruitful to have available a specialized class of data structuresexplicitly designed to handle arrays conveniently and efficiently. This is where thePython library NumPy comes into play, with its ndarray class.","Before we turn to NumPy, let us first construct arrays with the built-in data structurespresented in the previous section. listobjects are particularly suited to accomplishingthis task. A simple list can already be considered a one-dimensional array:In%5B84%5D:v%3D%5B0.5,0.75,1.0,1.5,2.0%5D# vector of numbersSince listobjects can contain arbitrary other objects, they can also contain other listobjects. In that way, two- and higher-dimensional arrays are easily constructed by nestedlist objects:In%5B85%5D:m%3D%5Bv,v,v%5D# matrix of numbersmOut%5B85%5D: %5B%5B0.5, 0.75, 1.0, 1.5, 2.0%5D,          %5B0.5, 0.75, 1.0, 1.5, 2.0%5D,          %5B0.5, 0.75, 1.0, 1.5, 2.0%5D%5DWe can also easily select rows via simple indexing or single elements via double indexing(whole columns, however, are not so easy to select):In%5B86%5D:m%5B1%5DOut%5B86%5D: %5B0.5, 0.75, 1.0, 1.5, 2.0%5DIn%5B87%5D:m%5B1%5D%5B0%5DOut%5B87%5D: 0.5Nesting can be pushed further for even more general structures:In%5B88%5D:v1%3D%5B0.5,1.5%5Dv2%3D%5B1,2%5Dm%3D%5Bv1,v2%5Dc%3D%5Bm,m%5D# cube of numberscOut%5B88%5D: %5B%5B%5B0.5, 1.5%5D, %5B1, 2%5D%5D, %5B%5B0.5, 1.5%5D, %5B1, 2%5D%5D%5DIn%5B89%5D:c%5B1%5D%5B1%5D%5B0%5DOut%5B89%5D: 1Note that combining objects in the way just presented generally works with referencepointers to the original objects. What does that mean in practice%3F Let us have a look atthe following operations:In%5B90%5D:v%3D%5B0.5,0.75,1.0,1.5,2.0%5Dm%3D%5Bv,v,v%5DmOut%5B90%5D: %5B%5B0.5, 0.75, 1.0, 1.5, 2.0%5D,          %5B0.5, 0.75, 1.0, 1.5, 2.0%5D,          %5B0.5, 0.75, 1.0, 1.5, 2.0%5D%5D","Now change the value of the first element of the vobject and see what happens to the mobject:In%5B91%5D:v%5B0%5D%3D'Python'mOut%5B91%5D: %5B%5B'Python', 0.75, 1.0, 1.5, 2.0%5D,          %5B'Python', 0.75, 1.0, 1.5, 2.0%5D,          %5B'Python', 0.75, 1.0, 1.5, 2.0%5D%5DThis can be avoided by using the deepcopy function of the copy module:In%5B92%5D:fromcopyimportdeepcopyv%3D%5B0.5,0.75,1.0,1.5,2.0%5Dm%3D3*%5Bdeepcopy(v),%5DmOut%5B92%5D: %5B%5B0.5, 0.75, 1.0, 1.5, 2.0%5D,          %5B0.5, 0.75, 1.0, 1.5, 2.0%5D,          %5B0.5, 0.75, 1.0, 1.5, 2.0%5D%5DIn%5B93%5D:v%5B0%5D%3D'Python'mOut%5B93%5D: %5B%5B0.5, 0.75, 1.0, 1.5, 2.0%5D,          %5B0.5, 0.75, 1.0, 1.5, 2.0%5D,          %5B0.5, 0.75, 1.0, 1.5, 2.0%5D%5D","Obviously, composing array structures with list objects works, somewhat. But it is notreally convenient, and the listclass has not been built with this specific goal in mind.It has rather been built with a much broader and more general scope. From this pointof view, some kind of specialized class could therefore be really beneficial to handlearray-type structures.Such a specialized class is numpy.ndarray, which has been built with the specific goalof handling n-dimensional arrays both conveniently and efficiently-i.e., in a highlyperforming manner. The basic handling of instances of this class is again best illustratedby examples:In%5B94%5D:importnumpyasnpIn%5B95%5D:a%3Dnp.array(%5B0,0.5,1.0,1.5,2.0%5D)type(a)Out%5B95%5D: numpy.ndarrayIn%5B96%5D:a%5B:2%5D# indexing as with list objects in 1 dimensionOut%5B96%5D: array(%5B 0. ,  0.5%5D)A major feature of the numpy.ndarray class is the multitude of built-in methods. Forinstance:","In%5B97%5D:a.sum()# sum of all elementsOut%5B97%5D: 5.0In%5B98%5D:a.std()# standard deviationOut%5B98%5D: 0.70710678118654757In%5B99%5D:a.cumsum()# running cumulative sumOut%5B99%5D: array(%5B 0. ,  0.5,  1.5,  3. ,  5. %5D)Another major feature is the (vectorized) mathematical operationsdefined on ndarrayobjects:In%5B100%5D:a*2Out%5B100%5D: array(%5B 0.,  1.,  2.,  3.,  4.%5D)In%5B101%5D:a**2Out%5B101%5D: array(%5B 0.  ,  0.25,  1.  ,  2.25,  4.  %5D)In%5B102%5D:np.sqrt(a)Out%5B102%5D: array(%5B 0.        ,  0.70710678,  1.        ,  1.22474487,  1.41421356          %5D)The transition to more than one dimension is seamless, and all features presented sofar carry over to the more general cases. In particular, the indexing system is madeconsistent across all dimensions:In%5B103%5D:b%3Dnp.array(%5Ba,a*2%5D)bOut%5B103%5D: array(%5B%5B 0. ,  0.5,  1. ,  1.5,  2. %5D,                 %5B 0. ,  1. ,  2. ,  3. ,  4. %5D%5D)In%5B104%5D:b%5B0%5D# first rowOut%5B104%5D: array(%5B 0. ,  0.5,  1. ,  1.5,  2. %5D)In%5B105%5D:b%5B0,2%5D# third element of first rowOut%5B105%5D: 1.0In%5B106%5D:b.sum()Out%5B106%5D: 15.0In contrast to our listobject-based approach to constructing arrays, the numpy.ndarray class knows axes explicitly. Selecting either rows or columns from a matrix is es%E2%80%90sentially the same:In%5B107%5D:b.sum(axis%3D0)# sum along axis 0, i.e. column-wise sumOut%5B107%5D: array(%5B 0. ,  1.5,  3. ,  4.5,  6. %5D)In%5B108%5D:b.sum(axis%3D1)# sum along axis 1, i.e. row-wise sum","Out%5B108%5D: array(%5B  5.,  10.%5D)There are a number of ways to initialize (instantiate) a numpy.ndarrayobject. One is aspresented before, via np.array. However, this assumes that all elements of the array arealready available. In contrast, one would maybe like to have the numpy.ndarray objectsinstantiated first to populate them later with results generated during the execution ofcode. To this end, we can use the following functions:In%5B109%5D:c%3Dnp.zeros((2,3,4),dtype%3D'i',order%3D'C')# also: np.ones()cOut%5B109%5D: array(%5B%5B%5B0, 0, 0, 0%5D,                  %5B0, 0, 0, 0%5D,                  %5B0, 0, 0, 0%5D%5D,                 %5B%5B0, 0, 0, 0%5D,                  %5B0, 0, 0, 0%5D,                  %5B0, 0, 0, 0%5D%5D%5D, dtype%3Dint32)In%5B110%5D:d%3Dnp.ones_like(c,dtype%3D'f16',order%3D'C')# also: np.zeros_like()dOut%5B110%5D: array(%5B%5B%5B 1.0,  1.0,  1.0,  1.0%5D,                  %5B 1.0,  1.0,  1.0,  1.0%5D,                  %5B 1.0,  1.0,  1.0,  1.0%5D%5D,                 %5B%5B 1.0,  1.0,  1.0,  1.0%5D,                  %5B 1.0,  1.0,  1.0,  1.0%5D,                  %5B 1.0,  1.0,  1.0,  1.0%5D%5D%5D, dtype%3Dfloat128)With all these functions we provide the following information:shapeEither an int, a sequence of ints, or a reference to another numpy.ndarraydtype (optional)A numpy.dtype-these are NumPy-specific data types for numpy.ndarray objectsorder (optional)The order in which to store elements in memory:  C  for  C -like (i.e., row-wise) or Ffor Fortran-like (i.e., column-wise)Here, it becomes obvious how NumPy specializes the construction of arrays with thenumpy.ndarray class, in comparison to the list-based approach:%E2%80%A2The shape/length/size of the array is homogenous across any given dimension.%E2%80%A2It only allows for a single data type (numpy.dtype) for the whole array.The role of the orderparameter is discussed later in the chapter. Table 4-4 provides anoverview of numpy.dtype objects (i.e., the basic data types NumPy allows).","Table 4-4. NumPy dtype objects","tBit fieldt4 (4 bits)bBooleanb (true or false)iIntegeri8 (64 bit)uUnsigned integeru8 (64 bit)fFloating pointf8 (64 bit)cComplex floating pointc16 (128 bit)OObject0 (pointer to object)S, aStringS24 (24 characters)UUnicodeU24 (24 Unicode characters)VOtherV12 (12-byte data block)NumPyprovides a generalization of regular arrays that loosens at least the dtype restric%E2%80%90tion, but let us stick with regular arrays for a moment and see what the specializationbrings in terms of performance.As a simple exercise, suppose we want to generate a matrix/array of shape 5,000 %C3%97 5,000elements, populated with (pseudo)random, standard normally distributed numbers.We then want to calculate the sum of all elements. First, the pure Pythonapproach,where we make heavy use of list comprehensions and functional programming meth%E2%80%90ods as well as lambda functions:In%5B111%5D:importrandomI%3D5000In%5B112%5D:%25timemat%3D%5B%5Brandom.gauss(0,1)forjinrange(I)%5Dforiinrange(I)%5D# a nested list comprehensionOut%5B112%5D: CPU times: user 36.5 s, sys: 408 ms, total: 36.9 s          Wall time: 36.4 sIn%5B113%5D:%25timereduce(lambdax,y:x%2By,      %5C%5Breduce(lambdax,y:x%2By,row) %5Cforrowinmat%5D)Out%5B113%5D: CPU times: user 4.3 s, sys: 52 ms, total: 4.35 s          Wall time: 4.07 s          678.5908519876674Let us now turn to NumPy and see how the same problem is solved there. For convenience,the NumPy sublibrary random offers a multitude of functions to initialize a numpy.ndarrayobject and populate it at the same time with (pseudo)random numbers:","In%5B114%5D:%25timemat%3Dnp.random.standard_normal((I,I))Out%5B114%5D: CPU times: user 1.83 s, sys: 40 ms, total: 1.87 s          Wall time: 1.87 sIn%5B115%5D:%25timemat.sum()Out%5B115%5D: CPU times: user 36 ms, sys: 0 ns, total: 36 ms          Wall time: 34.6 ms          349.49777911439384We observe the following:SyntaxAlthough we use several approaches to compact the pure Pythoncode, the NumPyversion is even more compact and readable.PerformanceThe generation of the numpy.ndarray object is roughly 20 times faster and thecalculation of the sum is roughly 100 times faster than the respective operations inpure Python.","TheuseofNumPyforarray-basedoperationsandalgorithmsgenerallyresults in compact, easily readable code and significant performanceimprovements over pure Python code.","The specialization of the numpy.ndarrayclass obviously brings a number of really val%E2%80%90uable benefits with it. However, a too-narrow specialization might turn out to be toolarge a burden to carry for the majority of array-based algorithms and applications.Therefore, NumPyprovides structured arrays that allow us to have different NumPy datatypes per column, at least. What does %E2%80%9Cper column%E2%80%9D mean%3F Consider the following ini%E2%80%90tialization of a structured array object:In%5B116%5D:dt%3Dnp.dtype(%5B('Name','S10'),('Age','i4'),('Height','f'),('Children/Pets','i4',2)%5D)s%3Dnp.array(%5B('Smith',45,1.83,(0,1)),('Jones',53,1.72,(2,2))%5D,dtype%3Ddt)sOut%5B116%5D: array(%5B('Smith', 45, 1.8300000429153442, %5B0, 1%5D),                 ('Jones', 53, 1.7200000286102295, %5B2, 2%5D)%5D,                dtype%3D%5B('Name', 'S10'), ('Age', '%3Ci4'), ('Height', '%3Cf4'), ('Chi          ldren/Pets', '%3Ci4', (2,))%5D)In a sense, this construction comes quite close to the operation for initializing tables ina SQL database. We have column names and column data types, with maybe some","additional information (e.g., maximum number of characters per string object). Thesingle columns can now be easily accessed by their names:In%5B117%5D:s%5B'Name'%5DOut%5B117%5D: array(%5B'Smith', 'Jones'%5D,                dtype%3D'%7CS10')In%5B118%5D:s%5B'Height'%5D.mean()Out%5B118%5D: 1.7750001Having selected a specific row and record, respectively, the resulting objects mainlybehave like dict objects, where one can retrieve values via keys:In%5B119%5D:s%5B1%5D%5B'Age'%5DOut%5B119%5D: 53In summary, structured arrays are a generalization of the regular numpy.ndarrayobjecttypes in that the data type only has to be the same per column, as one is used to in thecontext of tables in SQL databases. One advantage of structured arrays is that a singleelement of a column can be another multidimensional object and does not have toconform to the basic NumPy data types.","NumPyprovides,inadditiontoregulararrays,structuredarraysthatallow the description and handling of rather complex array-orienteddata structures with a variety of different data types and even struc%E2%80%90tures per (named) column. They bring SQL table-like data structurestoPython,withallthebenefitsofregularnumpy.ndarrayobjects(syntax, methods, performance).","Vectorization of code is a strategy to get more compact code that is possibly executedfaster. The fundamental idea is to conduct an operation on or to apply a function to acomplex object %E2%80%9Cat once%E2%80%9D and not by iterating over the single elements of the object. InPython, the functional programming tools map, filter, and reduceprovide means forvectorization. In a sense, NumPy has vectorization built in deep down in its core.","As we learned in the previous section, simple mathematical operations can be imple%E2%80%90mented on numpy.ndarray objects directly. For example, we can add two NumPy arrayselement-wise as follows:","In%5B120%5D:r%3Dnp.random.standard_normal((4,3))s%3Dnp.random.standard_normal((4,3))In%5B121%5D:r%2BsOut%5B121%5D: array(%5B%5B-1.94801686, -0.6855251 ,  2.28954806%5D,                 %5B 0.33847593, -1.97109602,  1.30071653%5D,                 %5B-1.12066585,  0.22234207, -2.73940339%5D,                 %5B 0.43787363,  0.52938941, -1.38467623%5D%5D)NumPy also supports what is called broadcasting. This allows us to combine objects ofdifferent shape within a single operation. We have already made use of this before.Consider the following example:In%5B122%5D:2*r%2B3Out%5B122%5D: array(%5B%5B 2.54691692,  1.65823523,  8.14636725%5D,                 %5B 4.94758114,  0.25648128,  1.89566919%5D,                 %5B 0.41775907,  0.58038395,  2.06567484%5D,                 %5B 0.67600205,  3.41004636,  1.07282384%5D%5D)In this case, the robject is multiplied by 2 element-wise and then 3 is added element-wise-the 3 is broadcasted or stretched to the shape of the r object. It works with dif%E2%80%90ferently shaped arrays as well, up to a certain point:In%5B123%5D:s%3Dnp.random.standard_normal(3)r%2BsOut%5B123%5D: array(%5B%5B 0.23324118, -1.09764268,  1.90412565%5D,                 %5B 1.43357329, -1.79851966, -1.22122338%5D,                 %5B-0.83133775, -1.63656832, -1.13622055%5D,                 %5B-0.70221625, -0.22173711, -1.63264605%5D%5D)This broadcasts the one-dimensional array of size 3 to a shape of (4, 3). The same doesnot work, for example, with a one-dimensional array of size 4:In%5B124%5D:s%3Dnp.random.standard_normal(4)r%2BsOut%5B124%5D: ValueError          operands could not be broadcast together with shapes (4,3) (4,)However, transposing the r object makes the operation work again. In the followingcode, the transpose method transforms the ndarray object with shape (4, 3) into anobject of the same type with shape (3, 4):In%5B125%5D:r.transpose()%2BsOut%5B125%5D: array(%5B%5B-0.63380522,  0.5964174 ,  0.88641996, -0.86931849%5D,                 %5B-1.07814606, -1.74913253,  0.9677324 ,  0.49770367%5D,                 %5B 2.16591995, -0.92953858,  1.71037785, -0.67090759%5D%5D)In%5B126%5D:np.shape(r.T)Out%5B126%5D: (3, 4)","As a general rule, custom-defined Python functions work with numpy.ndarrays as well.If the implementation allows, arrays can be used with functions just as intor floatobjects can. Consider the following function:In%5B127%5D:deff(x):return3*x%2B5We can pass standard Pythonobjects as well as numpy.ndarrayobjects (for which theoperations in the function have to be defined, of course):In%5B128%5D:f(0.5)# float objectOut%5B128%5D: 6.5In%5B129%5D:f(r)# NumPy arrayOut%5B129%5D: array(%5B%5B  4.32037538,   2.98735285,  12.71955087%5D,                 %5B  7.9213717 ,   0.88472192,   3.34350378%5D,                 %5B  1.1266386 ,   1.37057593,   3.59851226%5D,                 %5B  1.51400308,   5.61506954,   2.10923576%5D%5D)What NumPy does is to simply apply the function f to the object element-wise. In thatsense, by using this kind of operation we do not avoid loops%3B we only avoid them on thePython level and delegate the looping to NumPy. On the NumPy level, looping over thenumpy.ndarrayobject is taken care of by highly optimized code, most of it written in  C and therefore generally much faster than pure Python. This explains the %E2%80%9Csecret%E2%80%9D behindthe performance benefits of using NumPy for array-based use cases.When working with arrays, one has to take care to call the right functions on the re%E2%80%90spective objects. For example, the sin function from the standard mathmodule ofPython does not work with NumPy arrays:In%5B130%5D:importmathmath.sin(r)Out%5B130%5D: TypeError          only length-1 arrays can be converted to Python scalarsThe function is designed to handle, for example, float objects-i.e., single numbers,not arrays. NumPyprovides the respective counterparts as so-called ufuncs, or universalfunctions:In%5B131%5D:np.sin(r)# array as inputOut%5B131%5D: array(%5B%5B-0.22460878, -0.62167738,  0.53829193%5D,                 %5B 0.82702259, -0.98025745, -0.52453206%5D,                 %5B-0.96114497, -0.93554821, -0.45035471%5D,                 %5B-0.91759955,  0.20358986, -0.82124413%5D%5D)In%5B132%5D:np.sin(np.pi)# float as inputOut%5B132%5D: 1.2246467991473532e-16","5.Cf. http://docs.scipy.org/doc/numpy/reference/ufuncs.html for an overview.NumPy provides a large number of such ufuncs that generalize typical mathematicalfunctions to numpy.ndarray objects.5","Becarefulwhenusingthefromlibraryimport*approachtoimporting.SuchanapproachcancausetheNumPyreferencetotheufuncnumpy.sintobereplacedbythereferencetothemathfunctionmath.sin.Youshould,asarule,importbothlibrariesbynametoavoidconfusion:importnumpyasnp%3Bimportmath.Thenyoucanuse math.sin alongside np.sin.","When we first initialized numpy.ndarrayobjects by using numpy.zero, we provided anoptional argument for the memory layout. This argument specifies, roughly speaking,which elements of an array get stored in memory next to each other. When workingwith small arrays, this has hardly any measurable impact on the performance of arrayoperations. However, when arrays get large the story is somewhat different, dependingon the operations to be implemented on the arrays.To illustrate this important point for memory-wise handling of arrays in science andfinance, consider the following construction of multidimensional numpy.ndarrayobjects:In%5B133%5D:x%3Dnp.random.standard_normal((5,10000000))y%3D2*x%2B3# linear equation y %3D a * x %2B b C %3Dnp.array((x,y),order%3D'C')F%3Dnp.array((x,y),order%3D'F')x%3D0.0%3By%3D0.0# memory cleanupIn%5B134%5D: C %5B:2%5D.round(2)Out%5B134%5D: array(%5B%5B%5B-0.51, -1.14, -1.07, ...,  0.2 , -0.18,  0.1 %5D,                  %5B-1.22,  0.68,  1.83, ...,  1.23, -0.27, -0.16%5D,                  %5B 0.45,  0.15,  0.01, ..., -0.75,  0.91, -1.12%5D,                  %5B-0.16,  1.4 , -0.79, ..., -0.33,  0.54,  1.81%5D,                  %5B 1.07, -1.07, -0.37, ..., -0.76,  0.71,  0.34%5D%5D,                 %5B%5B 1.98,  0.72,  0.86, ...,  3.4 ,  2.64,  3.21%5D,                  %5B 0.55,  4.37,  6.66, ...,  5.47,  2.47,  2.68%5D,                  %5B 3.9 ,  3.29,  3.03, ...,  1.5 ,  4.82,  0.76%5D,                  %5B 2.67,  5.8 ,  1.42, ...,  2.34,  4.09,  6.63%5D,                  %5B 5.14,  0.87,  2.27, ...,  1.48,  4.43,  3.67%5D%5D%5D)","Let's look at some really fundamental examples and use cases for both types of ndarrayobjects:In%5B135%5D:%25timeit C .sum()Out%5B135%5D: 10 loops, best of 3: 123 ms per loopIn%5B136%5D:%25timeitF.sum()Out%5B136%5D: 10 loops, best of 3: 123 ms per loopWhen summing up all elements of the arrays, there is no performance difference be%E2%80%90tween the two memory layouts. However, consider the following example with the C-like memory layout:In%5B137%5D:%25timeit C %5B0%5D.sum(axis%3D0)Out%5B137%5D: 10 loops, best of 3: 102 ms per loopIn%5B138%5D:%25timeit C %5B0%5D.sum(axis%3D1)Out%5B138%5D: 10 loops, best of 3: 61.9 ms per loopSumming five large vectors and getting back a single large results vector obviously isslower in this case than summing 10,000,000 small ones and getting back an equalnumber of results. This is due to the fact that the single elements of the small vectors-i.e., the rows-are stored next to each other. With the Fortran-like memory layout, therelative performance changes considerably:In%5B139%5D:%25timeitF.sum(axis%3D0)Out%5B139%5D: 1 loops, best of 3: 801 ms per loopIn%5B140%5D:%25timeitF.sum(axis%3D1)Out%5B140%5D: 1 loops, best of 3: 2.23 s per loopIn%5B141%5D:F%3D0.0%3B C %3D0.0# memory cleanupIn this case, operating on a few large vectors performs better than operating on a largenumber of small ones. The elements of the few large vectors are stored in memory nextto each other, which explains the relative performance advantage. However, overall theoperations are absolutely much slower when compared to the  C -like variant.","Python provides, in combination with NumPy, a rich set of flexible data structures. Froma finance point of view, the following can be considered the most important ones:","Basic data typesIn finance, the classes int, float, and string provide the atomic data types.Standard data structuresThe classes tuple, list, dict, and set have many application areas in finance, withlist being the most flexible workhorse in general.ArraysA large class of finance-related problems and algorithms can be cast to an arraysetting%3B NumPyprovides the specialized class numpy.ndarray, which provides bothconvenience and compactness of code as well as high performance.This chapter shows that both the basic data structures and the NumPy ones allow forhighly vectorized implementation of algorithms. Depending on the specific shape ofthe data structures, care should be taken with regard to the memory layout of arrays.Choosing the right approach here can speed up code execution by a factor of twoor more.","This chapter focuses on those issues that might be of particular importance for financealgorithms and applications. However, it can only represent a starting point for theexploration of data structures and data modeling in Python. There are a number ofvaluable resources available to go deeper from here.Here are some Internet resources to consult:%E2%80%A2The Python documentation is always a good starting point: http://www.python.org/doc/.%E2%80%A2For details on NumPy arrays as well as related methods and functions, see http://docs.scipy.org/doc/.%E2%80%A2The SciPy lecture notes are also a good source to get started: http://scipy-lectures.github.io/.Good references in book form are:%E2%80%A2Goodrich, Michael et al. (2013): Data Structures and Algorithms in Python.JohnWiley %26 Sons, Hoboken, NJ.%E2%80%A2Langtangen, Hans Petter (2009): A Primer on Scientific Programming with Python.Springer Verlag, Berlin, Heidelberg.","Use a picture. It's worth a thousand words.- Arthur Brisbane (1911)This chapter is about basic visualization capabilities of the matplotliblibrary. Althoughthere are many other visualization libraries available, matplotlib has established itselfas the benchmark and, in many situations, a robust and reliable visualization tool. It isboth easy to use for standard plots and flexible when it comes to more complex plotsand customizations. In addition, it is tightly integrated with NumPyand the data struc%E2%80%90tures that it provides.This chapter mainly covers the following topics:2D plottingFrom the most simple to some more advanced plots with two scales or differentsubplots%3B typical financial plots, like candlestick charts, are also covered.3D plottingA selection of 3D plots useful for financial applications are presented.This chapter cannot be comprehensive with regard to data visualization with Pythonand matplotlib, but it provides a number of examples for the most basic and mostimportant capabilities for finance. Other examples are also found in later chapters. Forinstance, Chapter 6 shows how to visualize time series data with the pandas library.","To begin with, we have to import the respective libraries. The main plotting functionsare found in the sublibrary matplotlib.pyplot:In%5B1%5D:importnumpyasnpimportmatplotlibasmpl","importmatplotlib.pyplotasplt%25matplotlibinline","In all that follows, we will plot data stored in NumPyndarrayobjects. However, matplotlibis of course able to plot data stored in different Pythonformats, like list objects,as well. First, we need data that we can plot. To this end, we generate 20 standard nor%E2%80%90mally distributed (pseudo)random numbers as a NumPyndarray:In%5B2%5D:np.random.seed(1000)y%3Dnp.random.standard_normal(20)The most fundamental, but nevertheless quite powerful, plotting function is plotfromthe pyplot sublibrary. In principle, it needs two sets of numbers:%E2%80%A2",": a list or an array containing the x coordinates (values of the abscissa)%E2%80%A2",": a list or an array containing the y coordinates (values of the ordinate)The number of x and yvalues provided must match, of course. Consider the followingtwo lines of code, whose output is presented in Figure 5-1:In%5B3%5D:x%3Drange(len(y))plt.plot(x,y)Figure 5-1. Plot given x and y valuesplotnotices when you pass an ndarrayobject. In this case, there is no need to providethe %E2%80%9Cextra%E2%80%9D information of the x values. If you only provide the y values, plot takes theindex values as the respective x values. Therefore, the following single line of codegenerates exactly the same output (cf. Figure 5-2):In%5B4%5D:plt.plot(y)","Figure 5-2. Plot given data as 1D array","You can simply pass NumPyndarray objects to matplotlib functions.It is able to interpret the data structure for simplified plotting. How%E2%80%90ever, be careful to not pass a too large and/or complex array.Since the majority of the ndarray methods return again an ndarray object, you can alsopass your object with a method (or even multiple methods, in some cases) attached. Bycalling the cumsummethod on the ndarrayobject with the sample data, we get the cu%E2%80%90mulative sum of this data and, as to be expected, a different output (cf. Figure 5-3):In%5B5%5D:plt.plot(y.cumsum())Figure 5-3. Plot given a 1D array with method attachedIn general, the default plotting style does not satisfy typical requirements for reports,publications, etc. For example, you might want to customize the font used (e.g., forcompatibility with LaTeX fonts), to have labels at the axes, or to plot a grid for betterreadability. Therefore, matplotliboffers a large number of functions to customize the","plotting style. Some are easily accessible%3B for others one has to go a bit deeper. Easilyaccessible, for example, are those functions that manipulate the axes and those that addgrids and labels (cf. Figure 5-4):In%5B6%5D:plt.plot(y.cumsum())plt.grid(True)# adds a gridplt.axis('tight')# adjusts the axis rangesFigure 5-4. Plot with grid and tight axesOther options for plt.axis are given in Table 5-1, the majority of which have to bepassed as a string object.Table 5-1. Options for plt.axis","EmptyReturns current axis limitsoffTurns axis lines and labels offequalLeads to equal scalingscaledEqual scaling via dimension changestightMakes all data visible (tightens limits)imageMakes all data visible (with data limits)%5Bxmin,xmax,ymin,ymax%5DSets limits to given (list of) valuesIn addition, you can directly set the minimum and maximum values of each axis byusing plt.xlim and plt.ylim. The following code provides an example whose outputis shown in Figure 5-5:In%5B7%5D:plt.plot(y.cumsum())plt.grid(True)plt.xlim(-1,20)plt.ylim(np.min(y.cumsum())-1,np.max(y.cumsum())%2B1)","Figure 5-5. Plot with custom axis limitsFor the sake of better readability, a plot usually contains a number of labels-e.g., a titleand labels describing the nature of xand y values. These are added by the functionsplt.title, plt.xlabel, and plt.ylabel, respectively. By default, plot plots continu%E2%80%90ous lines, even if discrete data points are provided. The plotting of discrete points isaccomplished by choosing a different style option. Figure 5-6overlays (red) points anda (blue) line with line width of 1.5 points:In%5B8%5D:plt.figure(figsize%3D(7,4))# the figsize parameter defines the# size of the figure in (width, height)plt.plot(y.cumsum(),'b',lw%3D1.5)plt.plot(y.cumsum(),'ro')plt.grid(True)plt.axis('tight')plt.xlabel('index')plt.ylabel('value')plt.title('A Simple Plot')Figure 5-6. Plot with typical labels","By default, plt.plot supports the color abbreviations in Table 5-2.Table 5-2. Standard color abbreviations","bBluegGreenrRedcCyanmMagentayYellowkBlackwWhiteIn terms of line and/or point styles, plt.plot supports the characters listed in Table 5-3.Table 5-3. Standard style characters","-Solid line style--Dashed line style-.Dash-dot line style:Dotted line style.Point marker,Pixel markeroCircle markervTriangle_down marker%5ETriangle_up marker%3CTriangle_left marker%3ETriangle_right marker1Tri_down marker2Tri_up marker3Tri_left marker4Tri_right markersSquare markerpPentagon marker*Star markerhHexagon1 markerHHexagon2 marker%2BPlus marker","xX markerDDiamond markerdThin diamond marker%7CVline markerAny color abbreviation can be combined with any style character. In this way, you canmake sure that different data sets are easily distinguished. As we will see, the plottingstyle will also be reflected in the legend.","Plotting one-dimensional data can be considered a special case. In general, data sets willconsist of multiple separate subsets of data. The handling of such data sets follows thesame rules with matplotlibas with one-dimensional data. However, a number of ad%E2%80%90ditional issues might arise in such a context. For example, two data sets might have sucha different scaling that they cannot be plotted using the same y- and/or x-axis scaling.Another issue might be that you may want to visualize two different data sets in differentways, e.g., one by a line plot and the other by a bar plot.To begin with, let us first generate a two-dimensional sample data set. The code thatfollows generates first a NumPyndarray of shape 20 %C3%97 2 with standard normally dis%E2%80%90tributed (pseudo)random numbers. On this array, the method cumsumis called to cal%E2%80%90culate the cumulative sum of the sample data along axis 0 (i.e., the first dimension):In%5B9%5D:np.random.seed(2000)y%3Dnp.random.standard_normal((20,2)).cumsum(axis%3D0)In general, you can also pass such two-dimensional arrays to plt.plot. It will thenautomatically interpret the contained data as separate data sets (along axis 1, i.e., thesecond dimension). A respective plot is shown in Figure 5-7:In%5B10%5D:plt.figure(figsize%3D(7,4))plt.plot(y,lw%3D1.5)# plots two linesplt.plot(y,'ro')# plots two dotted linesplt.grid(True)plt.axis('tight')plt.xlabel('index')plt.ylabel('value')plt.title('A Simple Plot')","Figure 5-7. Plot with two data setsIn such a case, further annotations might be helpful to better read the plot. You can addindividual labels to each data set and have them listed in the legend. plt.legendacceptsdifferent locality parameters. 0 stands for best location, in the sense that as little data aspossible is hidden by the legend. Figure 5-8 shows the plot of the two data sets, this timewith a legend. In the generating code, we now do not pass the ndarrayobject as a wholebut rather access the two data subsets separately (y%5B:, 0%5D and y%5B:, 0%5D), which allowsus to attach individual labels to them:In%5B11%5D:plt.figure(figsize%3D(7,4))plt.plot(y%5B:,0%5D,lw%3D1.5,label%3D'1st')plt.plot(y%5B:,1%5D,lw%3D1.5,label%3D'2nd')plt.plot(y,'ro')plt.grid(True)plt.legend(loc%3D0)plt.axis('tight')plt.xlabel('index')plt.ylabel('value')plt.title('A Simple Plot')Further location options for plt.legend include those presented in Table 5-4.Table 5-4. Options for plt.legend","EmptyAutomatic0Best possible1Upper right2Upper left3Lower left4Lower right5Right","6Center left7Center right8Lower center9Upper center10CenterFigure 5-8. Plot with labeled data setsMultiple data sets with a similar scaling, like simulated paths for the same financial riskfactor, can be plotted using a single y-axis. However, often data sets show rather differentscalings and the plotting of such data with a single y scale generally leads to a significantloss of visual information. To illustrate the effect, we scale the first of the two data subsetsby a factor of 100 and plot the data again (cf. Figure 5-9):In%5B12%5D:y%5B:,0%5D%3Dy%5B:,0%5D*100plt.figure(figsize%3D(7,4))plt.plot(y%5B:,0%5D,lw%3D1.5,label%3D'1st')plt.plot(y%5B:,1%5D,lw%3D1.5,label%3D'2nd')plt.plot(y,'ro')plt.grid(True)plt.legend(loc%3D0)plt.axis('tight')plt.xlabel('index')plt.ylabel('value')plt.title('A Simple Plot')","Figure 5-9. Plot with two differently scaled data setsInspection of Figure 5-9 reveals that the first data set is still %E2%80%9Cvisually readable,%E2%80%9D whilethe second data set now looks like a straight line with the new scaling of the y-axis. Ina sense, information about the second data set now gets %E2%80%9Cvisually lost.%E2%80%9D There are twobasic approaches to resolve this problem:%E2%80%A2Use of two y-axes (left/right)%E2%80%A2Use of two subplots (upper/lower, left/right)Let us first introduce a second y-axis into the plot. Figure 5-10 now has two differenty-axes. The left y-axis is for the first data set while the right y-axis is for the second.Consequently, there are also two legends:In%5B13%5D:fig,ax1%3Dplt.subplots()plt.plot(y%5B:,0%5D,'b',lw%3D1.5,label%3D'1st')plt.plot(y%5B:,0%5D,'ro')plt.grid(True)plt.legend(loc%3D8)plt.axis('tight')plt.xlabel('index')plt.ylabel('value 1st')plt.title('A Simple Plot')ax2%3Dax1.twinx()plt.plot(y%5B:,1%5D,'g',lw%3D1.5,label%3D'2nd')plt.plot(y%5B:,1%5D,'ro')plt.legend(loc%3D0)plt.ylabel('value 2nd')","Figure 5-10. Plot with two data sets and two y-axesThe key lines of code are those that help manage the axes. These are the ones that follow:fig,ax1%3Dplt.subplots()# plot first data set using first (left) axisax2%3Dax1.twinx()# plot second data set using second (right) axisBy using the plt.subplots function, we get direct access to the underlying plottingobjects (the figure, subplots, etc.). It allows us, for example, to generate a second subplotthat shares the x-axis with the first subplot. In Figure 5-10we have, then, actually twosubplots that overlay each other.Next, consider the case of two separatesubplots. This option gives even more freedomto handle the two data sets, as Figure 5-11 illustrates:In%5B14%5D:plt.figure(figsize%3D(7,5))plt.subplot(211)plt.plot(y%5B:,0%5D,lw%3D1.5,label%3D'1st')plt.plot(y%5B:,0%5D,'ro')plt.grid(True)plt.legend(loc%3D0)plt.axis('tight')plt.ylabel('value')plt.title('A Simple Plot')plt.subplot(212)plt.plot(y%5B:,1%5D,'g',lw%3D1.5,label%3D'2nd')plt.plot(y%5B:,1%5D,'ro')plt.grid(True)plt.legend(loc%3D0)plt.axis('tight')plt.xlabel('index')plt.ylabel('value')","1.For an overview of which plot types are available, visit the matplotlib gallery.Figure 5-11. Plot with two subplotsThe placing of subplots in the a matplotlibfigureobject is accomplished here by theuse of a special coordinate system. plt.subplottakes as arguments three integers fornumrows, numcols, and fignum (either separated by commas or not). numrows specifiesthe number of rows, numcols the number of columns, and fignumthe number of thesub-plot, starting with 1 and ending with numrows * numcols. For example, a figurewith nine equally sized subplots would have numrows%3D3, numcols%3D3, and fignum%3D1,2,...,9. The lower-right subplot would have the following %E2%80%9Ccoordinates%E2%80%9D:plt.subplot(3, 3, 9).Sometimes, it might be necessary or desired to choose two different plot types to visu%E2%80%90alize such data. With the subplot approach you have the freedom to combine arbitrarykinds of plots that matplotliboffers.1Figure 5-12 combines a line/point plot with a barchart:In%5B15%5D:plt.figure(figsize%3D(9,4))plt.subplot(121)plt.plot(y%5B:,0%5D,lw%3D1.5,label%3D'1st')plt.plot(y%5B:,0%5D,'ro')plt.grid(True)plt.legend(loc%3D0)plt.axis('tight')plt.xlabel('index')plt.ylabel('value')plt.title('1st Data Set')plt.subplot(122)plt.bar(np.arange(len(y)),y%5B:,1%5D,width%3D0.5,color%3D'g',label%3D'2nd')","plt.grid(True)plt.legend(loc%3D0)plt.axis('tight')plt.xlabel('index')plt.title('2nd Data Set')Figure 5-12. Plot combining line/point subplot with bar subplot","When it comes to two-dimensional plotting, line and point plots are probably the mostimportant ones in finance%3B this is because many data sets embody time series data, whichgenerally is visualized by such plots. Chapter 6addresses financial times series data indetail. However, for the moment we want to stick with the two-dimensional data setand illustrate some alternative, and for financial applications useful, visualizationapproaches.The first is the scatter plot, where the values of one data set serve as the x values for theother data set. Figure 5-13 shows such a plot. Such a plot type is used, for example, whenyou want to plot the returns of one financial time series against those of another one.For this example we will use a new two-dimensional data set with some more data:In%5B16%5D:y%3Dnp.random.standard_normal((1000,2))In%5B17%5D:plt.figure(figsize%3D(7,5))plt.plot(y%5B:,0%5D,y%5B:,1%5D,'ro')plt.grid(True)plt.xlabel('1st')plt.ylabel('2nd')plt.title('Scatter Plot')","Figure 5-13. Scatter plot via plot functionmatplotlib also provides a specific function to generate scatter plots. It basically worksin the same way, but provides some additional features. Figure 5-14 shows the corre%E2%80%90sponding scatter plot to Figure 5-13, this time generated using the scatter function:In%5B18%5D:plt.figure(figsize%3D(7,5))plt.scatter(y%5B:,0%5D,y%5B:,1%5D,marker%3D'o')plt.grid(True)plt.xlabel('1st')plt.ylabel('2nd')plt.title('Scatter Plot')Figure 5-14. Scatter plot via scatter function","The scatterplotting function, for example, allows the addition of a third dimension,which can be visualized through different colors and be described by the use of a colorbar. To this end, we generate a third data set with random data, this time with integersbetween 0 and 10:In%5B19%5D:c%3Dnp.random.randint(0,10,len(y))Figure 5-15 shows a scatter plot where there is a third dimension illustrated by differentcolors of the single dots and with a color bar as a legend for the colors:In%5B20%5D:plt.figure(figsize%3D(7,5))plt.scatter(y%5B:,0%5D,y%5B:,1%5D,c%3Dc,marker%3D'o')plt.colorbar()plt.grid(True)plt.xlabel('1st')plt.ylabel('2nd')plt.title('Scatter Plot')Figure 5-15. Scatter plot with third dimensionAnother type of plot, the histogram, is also often used in the context of financial returns.Figure 5-16 puts the frequency values of the two data sets next to each other in thesame plot:In%5B21%5D:plt.figure(figsize%3D(7,4))plt.hist(y,label%3D%5B'1st','2nd'%5D,bins%3D25)plt.grid(True)plt.legend(loc%3D0)plt.xlabel('value')plt.ylabel('frequency')plt.title('Histogram')","Figure 5-16. Histogram for two data setsSince the histogram is such an important plot type for financial applications, let us takea closer look at the use of plt.hist. The following example illustrates the parametersthat are supported:plt.hist(x, bins%3D10, range%3DNone, normed%3DFalse, weights%3DNone, cumulative%3DFalse,bottom%3DNone, histtype%3D'bar', align%3D'mid', orientation%3D'vertical', rwidth%3DNone,log%3DFalse, color%3DNone, label%3DNone, stacked%3DFalse, hold%3DNone, **kwargs)Table 5-5 provides a description of the main parameters of the plt.hist function.Table 5-5. Parameters for plt.hist","xlist object(s), ndarray objectbinsNumber of binsrangeLower and upper range of binsnormedNorming such that integral value is 1weightsWeights for every value in xcumulativeEvery bin contains the counts of the lower binshisttypeOptions (strings): bar, barstacked, step, stepfilledalignOptions (strings): left, mid, rightorientationOptions (strings): horizontal, verticalrwidthRelative width of the barslogLog scalecolorColor per data set (array-like)labelString or sequence of strings for labelsstackedStacks multiple data sets","Figure 5-17shows a similar plot%3B this time, the data of the two data sets is stacked in thehistogram:In%5B22%5D:plt.figure(figsize%3D(7,4))plt.hist(y,label%3D%5B'1st','2nd'%5D,color%3D%5B'b','g'%5D,stacked%3DTrue,bins%3D20)plt.grid(True)plt.legend(loc%3D0)plt.xlabel('value')plt.ylabel('frequency')plt.title('Histogram')Figure 5-17. Stacked histogram for two data setsAnother useful plot type is the boxplot. Similar to the histogram, the boxplot allows botha concise overview of the characteristics of a data set and easy comparison of multipledata sets. Figure 5-18 shows such a plot for our data set:In%5B23%5D:fig,ax%3Dplt.subplots(figsize%3D(7,4))plt.boxplot(y)plt.grid(True)plt.setp(ax,xticklabels%3D%5B'1st','2nd'%5D)plt.xlabel('data set')plt.ylabel('value')plt.title('Boxplot')This last example uses the function plt.setp, which sets properties for a (set of) plottinginstance(s). For example, considering a line plot generated by:line%3Dplt.plot(data,'r')the following code:plt.setp(line,linestyle%3D'--')changes the style of the line to %E2%80%9Cdashed.%E2%80%9D This way, you can easily change parametersafter the plotting instance (%E2%80%9Cartist object%E2%80%9D) has been generated.","Figure 5-18. Boxplot for two data setsAs a final illustration in this section, we consider a mathematically inspired plot thatcan also be found as an example in the gallery for matplotlib. It plots a function andillustrates graphically the area below the function between a lower and an upper limit-in other words, the integral value of the function between the lower and upper limits.Figure 5-19 shows the resulting plot and illustrates that matplotlibseamlessly handlesLaTeX type setting for the inclusion of mathematical formulae into plots:In%5B24%5D:frommatplotlib.patchesimportPolygondeffunc(x):return0.5*np.exp(x)%2B1a,b%3D0.5,1.5# integral limitsx%3Dnp.linspace(0,2)y%3Dfunc(x)fig,ax%3Dplt.subplots(figsize%3D(7,5))plt.plot(x,y,'b',linewidth%3D2)plt.ylim(ymin%3D0)# Illustrate the integral value, i.e. the area under the function# between the lower and upper limitsIx%3Dnp.linspace(a,b)Iy%3Dfunc(Ix)verts%3D%5B(a,0)%5D%2Blist(zip(Ix,Iy))%2B%5B(b,0)%5Dpoly%3DPolygon(verts,facecolor%3D'0.7',edgecolor%3D'0.5')ax.add_patch(poly)plt.text(0.5*(a%2Bb),1,r%22%24%5Cint_a%5Eb f(x)%5Cmathrm%7Bd%7Dx%24%22,horizontalalignment%3D'center',fontsize%3D20)plt.figtext(0.9,0.075,'%24x%24')plt.figtext(0.075,0.9,'%24f(x)%24')","ax.set_xticks((a,b))ax.set_xticklabels(('%24a%24','%24b%24'))ax.set_yticks(%5Bfunc(a),func(b)%5D)ax.set_yticklabels(('%24f(a)%24','%24f(b)%24'))plt.grid(True)Figure 5-19. Exponential function, integral area, and LaTeX labelsLet us go through the generation of this plot step by step. The first step is the definitionof the function to be integrated:deffunc(x):return0.5*np.exp(x)%2B1The second step is the definition of the integral limits and the generation of needednumerical values:a,b%3D0.5,1.5# integral limitsx%3Dnp.linspace(0,2)y%3Dfunc(x)Third, we plot the function itself:fig,ax%3Dplt.subplots(figsize%3D(7,5))plt.plot(x,y,'b',linewidth%3D2)plt.ylim(ymin%3D0)Fourth and central, we generate the shaded area (%E2%80%9Cpatch%E2%80%9D) by the use of the Polygonfunction illustrating the integral area:Ix%3Dnp.linspace(a,b)Iy%3Dfunc(Ix)verts%3D%5B(a,0)%5D%2Blist(zip(Ix,Iy))%2B%5B(b,0)%5Dpoly%3DPolygon(verts,facecolor%3D'0.7',edgecolor%3D'0.5')ax.add_patch(poly)","The fifth step is the addition of the mathematical formula and some axis labels to theplot, using the plt.text and plt.figtextfunctions. LaTeXcode is passed between twodollar signs (%24 ... %24). The first two parameters of both functions are coordinate valuesto place the respective text:plt.text(0.5*(a%2Bb),1,r%22%24%5Cint_a%5Eb f(x)%5Cmathrm%7Bd%7Dx%24%22,horizontalalignment%3D'center',fontsize%3D20)plt.figtext(0.9,0.075,'%24x%24')plt.figtext(0.075,0.9,'%24f(x)%24')Finally, we set the individual x and y tick labels at their respective positions. Note thatalthough we place variable names rendered in LaTeX, the correct numerical values areused for the placing. We also add a grid, which in this particular case is only drawn forthe selected ticks highlighted before:ax.set_xticks((a,b))ax.set_xticklabels(('%24a%24','%24b%24'))ax.set_yticks(%5Bfunc(a),func(b)%5D)ax.set_yticklabels(('%24f(a)%24','%24f(b)%24'))plt.grid(True)","matplotlib also provides a small selection of special finance plots. These, like thecandlestick plot, are mainly used to visualize historical stock price data or similar fi%E2%80%90nancial time series data. Those plotting capabilities are found in the matplotlib.finance sublibrary:In%5B25%5D:importmatplotlib.financeasmpfAs a convenience function, this sublibrary allows for easy retrieval of historical stockprice data from the Yahoo! Finance website (cf. http://finance.yahoo.com). All you needare start and end dates and the respective ticker symbol. The following retrieves datafor the German DAX index whose ticker symbol is %5EGDAXI:In%5B26%5D:start%3D(2014,5,1)end%3D(2014,6,30)quotes%3Dmpf.quotes_historical_yahoo('%5EGDAXI',start,end)","Nowadays,acoupleofPythonlibrariesprovideconveniencefunc%E2%80%90tions to retrieve data from Yahoo! Finance. Be aware that, althoughthisisaconvenientwaytovisualizefinancialdatasets,thedataqualityisnotsufficienttobaseanyimportantinvestmentdecisiononit.Forexample,stocksplits,leadingto%E2%80%9Cpricedrops,%E2%80%9Dareoftennot correctly accounted for in the data provided by Yahoo! Finance.Thisholdstrueforanumberofotherfreelyavailabledatasourcesas well.quotesnow contains time series data for the DAX index starting with Date (in epochtime format), then Open, High, Low, Close, and Volume:In%5B27%5D:quotes%5B:2%5DOut%5B27%5D: %5B(735355.0,           9611.7900000000009,           9556.0200000000004,           9627.3799999999992,           9533.2999999999993,           88062300.0),          (735358.0,           9536.3799999999992,           9529.5,           9548.1700000000001,           9407.0900000000001,           61911600.0)%5DThe plotting functions of matplotlib.financeunderstand exactly this format and thedata set can be passed, for example, to the candlestick function as it is. Figure 5-20shows the result. Daily positive returns are indicated by blue rectangles, and negativereturns by redones. As you notice, matplotlibtakes care of the right labeling of the x-axis given the date information in the data set:In%5B28%5D:fig,ax%3Dplt.subplots(figsize%3D(8,5))fig.subplots_adjust(bottom%3D0.2)mpf.candlestick(ax,quotes,width%3D0.6,colorup%3D'b',colordown%3D'r')plt.grid(True)ax.xaxis_date()# dates on the x-axisax.autoscale_view()plt.setp(plt.gca().get_xticklabels(),rotation%3D30)","Figure 5-20. Candlestick chart for financial dataIn the preceding code, plt.setp(plt.gca().get_xticklabels(), rotation%3D30)grabs the x-axis labels and rotates them by 30 degrees. To this end, the function plt.gcais used, which returns the current figureobject. The method call of get_xticklabelsthen provides the tick labels for the x-axis of the figure.Table 5-6 provides a description of the different parameters the mpf.candlestickfunction takes.Table 5-6. Parameters for mpf.candlestick","axAn Axes instance to plot toquotesFinancial data to plot (sequence of time, open, close, high, low sequences)widthFraction of a day for the rectangle widthcolorupThe color of the rectangle where close %3E%3D opencolordownThe color of the rectangle where close %3C openalphaThe rectangle alpha levelA rather similar plot type is provided by the plot_day_summary function, which is usedin the same fashion as the candlestick function and with similar parameters. Here,opening and closing values are not illustrated by a colored rectangle but rather by twosmall horizontal lines, as Figure 5-21 shows:In%5B29%5D:fig,ax%3Dplt.subplots(figsize%3D(8,5))mpf.plot_day_summary(ax,quotes,colorup%3D'b',colordown%3D'r')plt.grid(True)ax.xaxis_date()plt.title('DAX Index')","plt.ylabel('index level')plt.setp(plt.gca().get_xticklabels(),rotation%3D30)Figure 5-21. Daily summary chart for financial dataOften, stock price data is combined with volume data in a single plot to also provideinformation with regard to market activity. The following code, with the result shownin Figure 5-22, illustrates such a use case based on historical data for the stock ofYahoo! Inc.:In%5B30%5D:quotes%3Dnp.array(mpf.quotes_historical_yahoo('YHOO',start,end))In%5B31%5D:fig,(ax1,ax2)%3Dplt.subplots(2,sharex%3DTrue,figsize%3D(8,6))mpf.candlestick(ax1,quotes,width%3D0.6,colorup%3D'b',colordown%3D'r')ax1.set_title('Yahoo Inc.')ax1.set_ylabel('index level')ax1.grid(True)ax1.xaxis_date()plt.bar(quotes%5B:,0%5D-0.25,quotes%5B:,5%5D,width%3D0.5)ax2.set_ylabel('volume')ax2.grid(True)ax2.autoscale_view()plt.setp(plt.gca().get_xticklabels(),rotation%3D30)","Figure 5-22. Plot combining candlestick and volume bar chart","There are not too many fields in finance that really benefit from visualization in threedimensions. However, one application area is volatility surfaces showing implied vola%E2%80%90tilities simultaneously for a number of times-of-maturity and strikes. In what follows,we artificially generate a plot that resembles a volatility surface. To this end, we consider:%E2%80%A2Strike values between 50 and 150%E2%80%A2Times-to-maturity between 0.5 and 2.5 yearsThis provides our two-dimensional coordinate system. We can use NumPy's meshgridfunction to generate such a system out of two one-dimensional ndarray objects:In%5B32%5D:strike%3Dnp.linspace(50,150,24)ttm%3Dnp.linspace(0.5,2.5,24)strike,ttm%3Dnp.meshgrid(strike,ttm)This transforms both 1D arrays into 2D arrays, repeating the original axis values asoften as needed:In%5B33%5D:strike%5B:2%5DOut%5B33%5D: array(%5B%5B  50.        ,   54.34782609,   58.69565217,   63.04347826,                   67.39130435,   71.73913043,   76.08695652,   80.43478261,                   84.7826087 ,   89.13043478,   93.47826087,   97.82608696,                  102.17391304,  106.52173913,  110.86956522,  115.2173913 ,                  119.56521739,  123.91304348,  128.26086957,  132.60869565,","                  136.95652174,  141.30434783,  145.65217391,  150.        %5D,                %5B  50.        ,   54.34782609,   58.69565217,   63.04347826,                   67.39130435,   71.73913043,   76.08695652,   80.43478261,                   84.7826087 ,   89.13043478,   93.47826087,   97.82608696,                  102.17391304,  106.52173913,  110.86956522,  115.2173913 ,                  119.56521739,  123.91304348,  128.26086957,  132.60869565,                  136.95652174,  141.30434783,  145.65217391,  150.        %5D%5D)Now, given the new ndarray objects, we generate the fake implied volatilities by a simple,scaled quadratic function:In%5B34%5D:iv%3D(strike-100)**2/(100*strike)/ttm# generate fake implied volatilitiesThe plot resulting from the following code is shown in Figure 5-23:In%5B35%5D:frommpl_toolkits.mplot3dimportAxes3Dfig%3Dplt.figure(figsize%3D(9,6))ax%3Dfig.gca(projection%3D'3d')surf%3Dax.plot_surface(strike,ttm,iv,rstride%3D2,cstride%3D2,cmap%3Dplt.cm.coolwarm,linewidth%3D0.5,antialiased%3DTrue)ax.set_xlabel('strike')ax.set_ylabel('time-to-maturity')ax.set_zlabel('implied volatility')fig.colorbar(surf,shrink%3D0.5,aspect%3D5)Figure 5-23. 3D surface plot for (fake) implied volatilities","Table 5-7 provides a description of the different parameters the plot_surfacefunctioncan take.Table 5-7. Parameters for plot_surface","X, Y, ZData values as 2D arraysrstrideArray row stride (step size)cstrideArray column stride (step size)colorColor of the surface patchescmapA colormap for the surface patchesfacecolorsFace colors for the individual patchesnormAn instance of Normalize to map values to colorsvminMinimum value to mapvmaxMaximum value to mapshadeWhether to shade the face colorsAs with two-dimensional plots, the line style can be replaced by single points or, as inwhat follows, single triangles. Figure 5-24plots the same data as a 3D scatter plot, butnow also with a different viewing angle, using the view_init function to set it:In%5B36%5D:fig%3Dplt.figure(figsize%3D(8,5))ax%3Dfig.add_subplot(111,projection%3D'3d')ax.view_init(30,60)ax.scatter(strike,ttm,iv,zdir%3D'z',s%3D25,c%3D'b',marker%3D'%5E')ax.set_xlabel('strike')ax.set_ylabel('time-to-maturity')ax.set_zlabel('implied volatility')","Figure 5-24. 3D scatter plot for (fake) implied volatilities","matplotlib can be considered both the benchmark and the workhorse when it comesto data visualization in Python. It is tightly integrated with NumPy and the basic func%E2%80%90tionality is easily and conveniently accessed. However, on the other hand, matplotlibis a rather mighty library with a somewhat complex API. This makes it impossible togive a broader overview of all the capabilities of matplotlib in this chapter.This chapter introduces the basic functions of matplotlib for 2D and 3D plotting usefulin most financial contexts. Other chapters provide further examples of how to use thisfundamental library for visualization.","The major resources for matplotlib can be found on the Web:%E2%80%A2The home page of matplotlibis, of course, the best starting point: http://matplotlib.org.%E2%80%A2There's a gallery with many useful examples: http://matplotlib.org/gallery.html.%E2%80%A2A tutorial for 2D plotting is found here: http://matplotlib.org/users/pyplot_tutorial.html.%E2%80%A2Another one for 3D plotting is here: http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html.","It has become kind of a standard routine to consult the gallery, to look there for anappropriate visualization example, and to start with the corresponding example code.Using, for example, IPython Notebook, only a single command is required to get startedonce you have found the right example.","The only reason for time is so that everything doesn't happen at once.- Albert EinsteinOne of the most important types of data one encounters in finance are financial timeseries. This is data indexed by date and/or time. For example, prices of stocks representfinancial time series data. Similarly, the USD-EUR exchange rate represents a financialtime series%3B the exchange rate is quoted in brief intervals of time, and a collection ofsuch quotes then is a time series of exchange rates.There is no financial discipline that gets by without considering time an importantfactor. This mainly is the same as with physics and other sciences. The major tool tocope with time series data in Python is the library pandas. Wes McKinney, the mainauthor of pandas, started developing the library when working as an analyst at AQRCapital Management, a large hedge fund. It is safe to say that pandashas been designedfrom the ground up to work with financial time series. As this chapter demonstrates,the main inspiration for the fundamental classes, such as the DataFrame and Seriesclasses, is drawn from the R statistical analysis language, which without doubt has astrength in that kind of modeling and analysis.The chapter is mainly based on a couple of examples drawn from a financial context. Itproceeds along the following lines:First and second stepsWe start exploring the capabilities of pandasby using very simple and small datasets%3B we then proceed by using a NumPyndarrayobject and transforming this to aDataFrame object. As we go, basic analytics and visualization capabilities areillustrated.","1.Considering only daily closing prices, you have approximately 30 %C3%97 252 %3D 7,560 closing prices for a singlestock over a period of 30 years. It is not uncommon to have more than 10,000 daily (bid/ask) ticks for a singlestock.Data from the Webpandas allows us to conveniently retrieve data from the Web-e.g., from Yahoo!Finance-and to analyze such data in many ways.Using data from CSV filesComma-separated value (CSV) files represent a global standard for the exchange offinancial time series data%3B pandas makes reading data from such files an efficienttask. Using data for two indices, we implement a regression analysis with pandas.High-frequency dataIn recent years, available financial data has increasingly shifted from daily quotesto tick data. Daily tick data volumes for a stock price regularly surpass those volumesof daily data collected over 30 years.1All financial time series data contains date and/or time information, by definition.Appendix Cprovides an overview of how to handle such data with Python, NumPy, andpandas as well as of how to convert typical date-time object types into each other.","In a sense, pandasis built %E2%80%9Con top%E2%80%9D of NumPy. So, for example, NumPy universal functionswill generally work on pandasobjects as well. We therefore import both to begin with:In%5B1%5D:importnumpyasnpimportpandasaspd","On a rather fundamental level, the DataFrameclass is designed to manage indexed andlabeled data, not too different from a SQL database table or a worksheet in a spreadsheetapplication. Consider the following creation of a DataFrame object:In%5B2%5D:df%3Dpd.DataFrame(%5B10,20,30,40%5D,columns%3D%5B'numbers'%5D,index%3D%5B'a','b','c','d'%5D)dfOut%5B2%5D:    numbers        a       10        b       20        c       30        d       40","This simple example already shows some major features of the DataFrame class whenit comes to storing data:DataData itself can be provided in different shapes and types (list, tuple, ndarray, anddict objects are candidates).LabelsData is organized in columns, which can have custom names.IndexThere is an index that can take on different formats (e.g., numbers, strings, timeinformation).Working with such a DataFrame object is in general pretty convenient and efficient, e.g.,compared to regular ndarrayobjects, which are more specialized and more restrictedwhen you want to do something link enlarge an existing object. The following are simpleexamples showing how typical operations on a DataFrame object work:In%5B3%5D:df.index# the index valuesOut%5B3%5D: Index(%5Bu'a', u'b', u'c', u'd'%5D, dtype%3D'object')In%5B4%5D:df.columns# the column namesOut%5B4%5D: Index(%5Bu'numbers'%5D, dtype%3D'object')In%5B5%5D:df.ix%5B'c'%5D# selection via indexOut%5B5%5D: numbers    30        Name: c, dtype: int64In%5B6%5D:df.ix%5B%5B'a','d'%5D%5D# selection of multiple indicesOut%5B6%5D:    numbers        a       10        d       40In%5B7%5D:df.ix%5Bdf.index%5B1:3%5D%5D# selection via Index objectOut%5B7%5D:    numbers        b       20        c       30In%5B8%5D:df.sum()# sum per columnOut%5B8%5D: numbers    100        dtype: int64In%5B9%5D:df.apply(lambdax:x**2)# square of every elementOut%5B9%5D:    numbers        a      100        b      400        c      900        d     1600","In general, you can implement the same vectorized operations on a DataFrame objectas on a NumPyndarray object:In%5B10%5D:df**2# again square, this time NumPy-likeOut%5B10%5D:    numbers         a      100         b      400         c      900         d     1600Enlarging the DataFrame object in both dimensions is possible:In%5B11%5D:df%5B'floats'%5D%3D(1.5,2.5,3.5,4.5)# new column is generateddfOut%5B11%5D:    numbers  floats         a       10     1.5         b       20     2.5         c       30     3.5         d       40     4.5In%5B12%5D:df%5B'floats'%5D# selection of columnOut%5B12%5D: a    1.5         b    2.5         c    3.5         d    4.5         Name: floats, dtype: float64A whole DataFrame object can also be taken to define a new column. In such a case,indices are aligned automatically:In%5B13%5D:df%5B'names'%5D%3Dpd.DataFrame(%5B'Yves','Guido','Felix','Francesc'%5D,index%3D%5B'd','a','b','c'%5D)dfOut%5B13%5D:    numbers  floats     names         a       10     1.5     Guido         b       20     2.5     Felix         c       30     3.5  Francesc         d       40     4.5      YvesAppending data works similarly. However, in the following example we see a side effectthat is usually to be avoided-the index gets replaced by a simple numbered index:In%5B14%5D:df.append(%7B'numbers':100,'floats':5.75,'names':'Henry'%7D,ignore_index%3DTrue)# temporary object%3B df not changedOut%5B14%5D:    numbers  floats     names         0       10    1.50     Guido         1       20    2.50     Felix         2       30    3.50  Francesc","         3       40    4.50      Yves         4      100    5.75     HenryIt is often better to append a DataFrame object, providing the appropriate index infor%E2%80%90mation. This preserves the index:In%5B15%5D:df%3Ddf.append(pd.DataFrame(%7B'numbers':100,'floats':5.75,'names':'Henry'%7D,index%3D%5B'z',%5D))dfOut%5B15%5D:    floats     names  numbers         a    1.50     Guido       10         b    2.50     Felix       20         c    3.50  Francesc       30         d    4.50      Yves       40         z    5.75     Henry      100One of the strengths of pandas is working with missing data. To this end, consider thefollowing code that adds a new column, but with a slightly different index. We use therather flexible join method here:In%5B16%5D:df.join(pd.DataFrame(%5B1,4,9,16,25%5D,index%3D%5B'a','b','c','d','y'%5D,columns%3D%5B'squares',%5D))# temporary objectOut%5B16%5D:    floats     names  numbers  squares         a    1.50     Guido       10        1         b    2.50     Felix       20        4         c    3.50  Francesc       30        9         d    4.50      Yves       40       16         z    5.75     Henry      100      NaNWhat you can see here is that pandas by default accepts only values for those indicesthat already exist. We lose the value for the index y and have a NaN value (i.e., %E2%80%9CNot aNumber%E2%80%9D) at index position z. To preserve both indices, we can provide an additionalparameter to tell pandashow to join. In our case, we use how%3D%22outer%22to use the unionof all values from both indices:In%5B17%5D:df%3Ddf.join(pd.DataFrame(%5B1,4,9,16,25%5D,index%3D%5B'a','b','c','d','y'%5D,columns%3D%5B'squares',%5D),how%3D'outer')dfOut%5B17%5D:    floats     names  numbers  squares         a    1.50     Guido       10        1         b    2.50     Felix       20        4         c    3.50  Francesc       30        9         d    4.50      Yves       40       16         y     NaN       NaN      NaN       25         z    5.75     Henry      100      NaN","Indeed, the index is now the union of the two original indices. All missing data points,given the new enlarged index, are replaced by NaN values. Other options for the joinoperation include innerfor the intersection of the index values, left (default) for theindex values of the object on which the method is called, and right for the index valuesof the object to be joined.Although there are missing values, the majority of method calls will still work. Forexample:In%5B18%5D:df%5B%5B'numbers','squares'%5D%5D.mean()# column-wise meanOut%5B18%5D: numbers    40         squares    11         dtype: float64In%5B19%5D:df%5B%5B'numbers','squares'%5D%5D.std()# column-wise standard deviationOut%5B19%5D: numbers    35.355339         squares     9.669540         dtype: float64","From now on, we will work with numerical data. We will add further features as we go,like a DatetimeIndex to manage time series data. To have a dummy data set to workwith, generate a numpy.ndarrywith, for example, nine rows and four columns of pseu%E2%80%90dorandom, standard normally distributed numbers:In%5B20%5D:a%3Dnp.random.standard_normal((9,4))a.round(6)Out%5B20%5D: array(%5B%5B-0.737304,  1.065173,  0.073406,  1.301174%5D,                %5B-0.788818, -0.985819,  0.403796, -1.753784%5D,                %5B-0.155881, -1.752672,  1.037444, -0.400793%5D,                %5B-0.777546,  1.730278,  0.417114,  0.184079%5D,                %5B-1.76366 , -0.375469,  0.098678, -1.553824%5D,                %5B-1.134258,  1.401821,  1.227124,  0.979389%5D,                %5B 0.458838, -0.143187,  1.565701, -2.085863%5D,                %5B-0.103058, -0.36617 , -0.478036, -0.03281 %5D,                %5B 1.040318, -0.128799,  0.786187,  0.414084%5D%5D)Although you can construct DataFrameobjects more directly (as we have seen before),using an ndarray object is generally a good choice since pandas will retain the basicstructure and will %E2%80%9Conly%E2%80%9D add meta-information (e.g., index values). It also represents atypical use case for financial applications and scientific research in general. For example:In%5B21%5D:df%3Dpd.DataFrame(a)df","Out%5B21%5D:           0         1         2         3         0 -0.737304  1.065173  0.073406  1.301174         1 -0.788818 -0.985819  0.403796 -1.753784         2 -0.155881 -1.752672  1.037444 -0.400793         3 -0.777546  1.730278  0.417114  0.184079         4 -1.763660 -0.375469  0.098678 -1.553824         5 -1.134258  1.401821  1.227124  0.979389         6  0.458838 -0.143187  1.565701 -2.085863         7 -0.103058 -0.366170 -0.478036 -0.032810         8  1.040318 -0.128799  0.786187  0.414084Table 6-1 lists the parameters that the DataFrame function takes. In the table, %E2%80%9Carray-like%E2%80%9D means a data structure similar to an ndarray object-a list, for example. Indexis an instance of the pandasIndex class.Table 6-1. Parameters of DataFrame function","datandarray/dict/DataFrameData for DataFrame%3B dict can contain Series, ndarrays, listsindexIndex/array-likeIndex to use%3B defaults to range(n)columnsIndex/array-likeColumn headers to use%3B defaults to range(n)dtypedtype, default NoneData type to use/force%3B otherwise, it is inferredcopybool, default NoneCopy data from inputsAs with structured arrays, and as we have already seen, DataFrameobjects have columnnames that can be defined directly by assigning a list with the right number of elements.This illustrates that you can define/change the attributes of the DataFrameobject asyou go:In%5B22%5D:df.columns%3D%5B%5B'No1','No2','No3','No4'%5D%5DdfOut%5B22%5D:         No1       No2       No3       No4         0 -0.737304  1.065173  0.073406  1.301174         1 -0.788818 -0.985819  0.403796 -1.753784         2 -0.155881 -1.752672  1.037444 -0.400793         3 -0.777546  1.730278  0.417114  0.184079         4 -1.763660 -0.375469  0.098678 -1.553824         5 -1.134258  1.401821  1.227124  0.979389         6  0.458838 -0.143187  1.565701 -2.085863         7 -0.103058 -0.366170 -0.478036 -0.032810         8  1.040318 -0.128799  0.786187  0.414084The column names provide an efficient mechanism to access data in the DataFrameobject, again similar to structured arrays:In%5B23%5D:df%5B'No2'%5D%5B3%5D# value in column No2 at index position 3Out%5B23%5D: 1.7302783624820191","To work with financial time series data efficiently, you must be able to handle timeindices well. This can also be considered a major strength of pandas. For example,assume that our nine data entries in the four columns correspond to month-end data,beginning in January 2015. A DatetimeIndex object is then generated with date_rangeas follows:In%5B24%5D:dates%3Dpd.date_range('2015-1-1',periods%3D9,freq%3D'M')datesOut%5B24%5D: %3Cclass 'pandas.tseries.index.DatetimeIndex'%3E         %5B2015-01-31, ..., 2015-09-30%5D         Length: 9, Freq: M, Timezone: NoneTable 6-2 lists the parameters that the date_range function takes.Table 6-2. Parameters of date_range function","startstring/datetimeleft bound for generating datesendstring/datetimeright bound for generating datesperiodsinteger/Nonenumber of periods (if start or end is None)freqstring/DateOffsetfrequency string, e.g., 5D for 5 daystzstring/Nonetime zone name for localized indexnormalizebool, default Nonenormalize start and end to midnightnamestring, default Nonename of resulting indexSo far, we have only encountered indices composed of stringand int objects. For timeseries data, however, a DatetimeIndex object generated with the date_range functionis of course what is needed.As with the columns, we assign the newly generated DatetimeIndex as the new Indexobject to the DataFrame object:In%5B25%5D:df.index%3DdatesdfOut%5B25%5D:                  No1       No2       No3       No4         2015-01-31 -0.737304  1.065173  0.073406  1.301174         2015-02-28 -0.788818 -0.985819  0.403796 -1.753784         2015-03-31 -0.155881 -1.752672  1.037444 -0.400793         2015-04-30 -0.777546  1.730278  0.417114  0.184079         2015-05-31 -1.763660 -0.375469  0.098678 -1.553824         2015-06-30 -1.134258  1.401821  1.227124  0.979389         2015-07-31  0.458838 -0.143187  1.565701 -2.085863         2015-08-31 -0.103058 -0.366170 -0.478036 -0.032810         2015-09-30  1.040318 -0.128799  0.786187  0.414084","When it comes to the generation of DatetimeIndex objects with the help of thedate_rangefunction, there are a number of choices for the frequency parameter freq.Table 6-3 lists all the options.Table 6-3. Frequency parameter values for date_range function","BBusiness day frequency C Custom business day frequency (experimental)DCalendar day frequencyWWeekly frequencyMMonth end frequencyBMBusiness month end frequencyMSMonth start frequencyBMSBusiness month start frequencyQQuarter end frequencyBQBusiness quarter end frequencyQSQuarter start frequencyBQSBusiness quarter start frequencyAYear end frequencyBABusiness year end frequencyASYear start frequencyBASBusiness year start frequencyHHourly frequencyTMinutely frequencySSecondly frequencyLMilliseondsUMicrosecondsIn this subsection, we start with a NumPyndarray object and end with an enriched versionin the form of a pandasDataFrameobject. But does this procedure work the other wayaround as well%3F Yes, it does:In%5B26%5D:np.array(df).round(6)Out%5B26%5D: array(%5B%5B-0.737304,  1.065173,  0.073406,  1.301174%5D,                %5B-0.788818, -0.985819,  0.403796, -1.753784%5D,                %5B-0.155881, -1.752672,  1.037444, -0.400793%5D,                %5B-0.777546,  1.730278,  0.417114,  0.184079%5D,                %5B-1.76366 , -0.375469,  0.098678, -1.553824%5D,                %5B-1.134258,  1.401821,  1.227124,  0.979389%5D,                %5B 0.458838, -0.143187,  1.565701, -2.085863%5D,","                %5B-0.103058, -0.36617 , -0.478036, -0.03281 %5D,                %5B 1.040318, -0.128799,  0.786187,  0.414084%5D%5D)","YoucangenerateaDataFrameobjectingeneralfromanndarrayobject.ButyoucanalsoeasilygenerateanndarrayobjectoutofaDataFrame by using the function array of NumPy.","Like NumPy arrays, the pandasDataFrameclass has built in a multitude of conveniencemethods. For example, you can easily get the column-wise sums, means, and cumulativesums as follows:In%5B27%5D:df.sum()Out%5B27%5D: No1   -3.961370         No2    0.445156         No3    5.131414         No4   -2.948346         dtype: float64In%5B28%5D:df.mean()Out%5B28%5D: No1   -0.440152         No2    0.049462         No3    0.570157         No4   -0.327594         dtype: float64In%5B29%5D:df.cumsum()Out%5B29%5D:                  No1       No2       No3       No4         2015-01-31 -0.737304  1.065173  0.073406  1.301174         2015-02-28 -1.526122  0.079354  0.477201 -0.452609         2015-03-31 -1.682003 -1.673318  1.514645 -0.853403         2015-04-30 -2.459549  0.056960  1.931759 -0.669323         2015-05-31 -4.223209 -0.318508  2.030438 -2.223147         2015-06-30 -5.357467  1.083313  3.257562 -1.243758         2015-07-31 -4.898629  0.940126  4.823263 -3.329621         2015-08-31 -5.001687  0.573956  4.345227 -3.362430         2015-09-30 -3.961370  0.445156  5.131414 -2.948346There is also a shortcut to a number of often-used statistics for numerical data sets, thedescribe method:In%5B30%5D:df.describe()Out%5B30%5D:             No1       No2       No3       No4         count  9.000000  9.000000  9.000000  9.000000         mean  -0.440152  0.049462  0.570157 -0.327594         std    0.847907  1.141676  0.642904  1.219345","         min   -1.763660 -1.752672 -0.478036 -2.085863         25%25   -0.788818 -0.375469  0.098678 -1.553824         50%25   -0.737304 -0.143187  0.417114 -0.032810         75%25   -0.103058  1.065173  1.037444  0.414084         max    1.040318  1.730278  1.565701  1.301174You can also apply the majority of NumPy universal functions to DataFrame objects:In%5B31%5D:np.sqrt(df)Out%5B31%5D:                  No1       No2       No3       No4         2015-01-31       NaN  1.032072  0.270935  1.140690         2015-02-28       NaN       NaN  0.635449       NaN         2015-03-31       NaN       NaN  1.018550       NaN         2015-04-30       NaN  1.315400  0.645844  0.429045         2015-05-31       NaN       NaN  0.314131       NaN         2015-06-30       NaN  1.183985  1.107756  0.989641         2015-07-31  0.677376       NaN  1.251280       NaN         2015-08-31       NaN       NaN       NaN       NaN         2015-09-30  1.019960       NaN  0.886672  0.643494","Ingeneral,youcanapplyNumPyuniversalfunctionstopandasDataFrameobjectswhenevertheycouldbeappliedtoanndarrayobjectcontaining the same data.pandas is quite error tolerant, in the sense that it captures errors and just puts a NaNvalue where the respective mathematical operation fails. Not only this, but as brieflyshown already, you can also work with such incomplete data sets as if they were completein a number of cases:In%5B32%5D:np.sqrt(df).sum()Out%5B32%5D: No1    1.697335         No2    3.531458         No3    6.130617         No4    3.202870         dtype: float64In such cases, pandas just leaves out the NaN values and only works with the otheravailable values. Plotting of data is also only one line of code away in general (cf.Figure 6-1):In%5B33%5D:%25matplotlibinlinedf.cumsum().plot(lw%3D2.0)","Figure 6-1. Line plot of a DataFrame objectBasically, pandasprovides a wrapper around matplotplib (cf. Chapter 5), specificallydesigned for DataFrame objects. Table 6-4 lists the parameters that the plotmethodtakes.Table 6-4. Parameters of plot method","xLabel/position, default NoneOnly used when column values are x-ticksyLabel/position, default NoneOnly used when column values are y-tickssubplotsBoolean, default FalsePlot columns in subplotssharexBoolean, default TrueSharing of the x-axisshareyBoolean, default FalseSharing of the y-axisuse_indexBoolean, default TrueUse of DataFrame.index as x-ticksstackedBoolean, default FalseStack (only for bar plots)sort_columnsBoolean, default FalseSort columns alphabetically before plottingtitleString, default NoneTitle for the plotgridBoolean, default FalseHorizontal and vertical grid lineslegendBoolean, default TrueLegend of labelsaxmatplotlib axis objectmatplotlib axis object to use for plottingstyleString or list/dictionaryline plotting style (for each column)kind%22line%E2%80%9C/%E2%80%9Dbar%E2%80%9C/%E2%80%9Dbarh%E2%80%9C/%E2%80%9Dkde%E2%80%9C/%E2%80%9Ddensity%22type of plotlogxBoolean, default FalseLogarithmic scaling of x-axislogyBoolean, default FalseLogarithmic scaling of y-axisxticksSequence, default Indexx-ticks for the plotyticksSequence, default Valuesy-ticks for the plotxlim2-tuple, listBoundaries for x-axisylim2-tuple, listBoundaries for y-axis","rotInteger, default NoneRotation of x-tickssecondary_yBoolean/sequence, default FalseSecondary y-axismark_rightBoolean, default TrueAutomatic labeling of secondary axiscolormapString/colormap object, default NoneColormap to use for plottingkwdsKeywordsOptions to pass to matplotlib","So far, we have worked mainly with the pandasDataFrame class:In%5B34%5D:type(df)Out%5B34%5D: pandas.core.frame.DataFrameBut there is also a dedicated Series class. We get a Series object, for example, whenselecting a single column from our DataFrame object:In%5B35%5D:df%5B'No1'%5DOut%5B35%5D: 2015-01-31   -0.737304         2015-02-28   -0.788818         2015-03-31   -0.155881         2015-04-30   -0.777546         2015-05-31   -1.763660         2015-06-30   -1.134258         2015-07-31    0.458838         2015-08-31   -0.103058         2015-09-30    1.040318         Freq: M, Name: No1, dtype: float64In%5B36%5D:type(df%5B'No1'%5D)Out%5B36%5D: pandas.core.series.SeriesThe main DataFrame methods are available for Series objects as well, and we can, forinstance, plot the results as before (cf. Figure 6-2):In%5B37%5D:importmatplotlib.pyplotaspltdf%5B'No1'%5D.cumsum().plot(style%3D'r',lw%3D2.)plt.xlabel('date')plt.ylabel('value')","Figure 6-2. Line plot of a Series object","pandas has powerful and flexible grouping capabilities. They work similarly to groupingin SQL as well as pivot tables in Microsoft Excel. To have something to group by, we adda column indicating the quarter the respective data of the index belongs to:In%5B38%5D:df%5B'Quarter'%5D%3D%5B'Q1','Q1','Q1','Q2','Q2','Q2','Q3','Q3','Q3'%5DdfOut%5B38%5D:                  No1       No2       No3       No4 Quarter         2015-01-31 -0.737304  1.065173  0.073406  1.301174      Q1         2015-02-28 -0.788818 -0.985819  0.403796 -1.753784      Q1         2015-03-31 -0.155881 -1.752672  1.037444 -0.400793      Q1         2015-04-30 -0.777546  1.730278  0.417114  0.184079      Q2         2015-05-31 -1.763660 -0.375469  0.098678 -1.553824      Q2         2015-06-30 -1.134258  1.401821  1.227124  0.979389      Q2         2015-07-31  0.458838 -0.143187  1.565701 -2.085863      Q3         2015-08-31 -0.103058 -0.366170 -0.478036 -0.032810      Q3         2015-09-30  1.040318 -0.128799  0.786187  0.414084      Q3Now, we can group by the %E2%80%9CQuarter%E2%80%9D column and can output statistics for the singlegroups:In%5B39%5D:groups%3Ddf.groupby('Quarter')For example, we can easily get the mean, max, and size of every group bucket as follows:In%5B40%5D:groups.mean()Out%5B40%5D:               No1       No2       No3       No4         Quarter         Q1      -0.560668 -0.557773  0.504882 -0.284468         Q2      -1.225155  0.918877  0.580972 -0.130118         Q3       0.465366 -0.212719  0.624617 -0.568196In%5B41%5D:groups.max()","Out%5B41%5D:               No1       No2       No3       No4         Quarter         Q1      -0.155881  1.065173  1.037444  1.301174         Q2      -0.777546  1.730278  1.227124  0.979389         Q3       1.040318 -0.128799  1.565701  0.414084In%5B42%5D:groups.size()Out%5B42%5D: Quarter         Q1         3         Q2         3         Q3         3         dtype: int64Grouping can also be done with multiple columns. To this end, we add another column,indicating whether the month of the index date is odd or even:In%5B43%5D:df%5B'Odd_Even'%5D%3D%5B'Odd','Even','Odd','Even','Odd','Even','Odd','Even','Odd'%5DThis additional information can now be used for a grouping based on two columnssimultaneously:In%5B44%5D:groups%3Ddf.groupby(%5B'Quarter','Odd_Even'%5D)In%5B45%5D:groups.size()Out%5B45%5D: Quarter  Odd_Even         Q1       Even        1                  Odd         2         Q2       Even        2                  Odd         1         Q3       Even        1                  Odd         2         dtype: int64In%5B46%5D:groups.mean()Out%5B46%5D:                        No1       No2       No3       No4         Quarter Odd_Even         Q1      Even     -0.788818 -0.985819  0.403796 -1.753784                 Odd      -0.446592 -0.343749  0.555425  0.450190         Q2      Even     -0.955902  1.566050  0.822119  0.581734                 Odd      -1.763660 -0.375469  0.098678 -1.553824         Q3      Even     -0.103058 -0.366170 -0.478036 -0.032810                 Odd       0.749578 -0.135993  1.175944 -0.835890This concludes the introduction into pandas and the use of DataFrame objects. Subse%E2%80%90quent sections apply this tool set to real-world financial data.","The Web today provides a wealth of financial information for free. Web giants such asGoogle or Yahoo! have comprehensive financial data offerings. Although the quality of","2.For a similar example using matplotlib only, see Chapter 5.the data sometimes does not fulfill professional requirements, for example with regardto the handling of stock splits, such data is well suited to illustrate the %E2%80%9Cfinancial power%E2%80%9Dof pandas.To this end, we will use the pandasbuilt-in function DataReaderto retrieve stock pricedata from Yahoo! Finance, analyze the data, and generate different plots of it.2Therequired function is stored in a submodule of pandas:In%5B47%5D:importpandas.io.dataaswebAt the time of this writing, pandas supports the following data sources:%E2%80%A2Yahoo! Finance (yahoo)%E2%80%A2Google Finance (google)%E2%80%A2St. Louis FED (fred)%E2%80%A2Kenneth French's data library (famafrench)%E2%80%A2World Bank (via pandas.io.wb)We can retrieve stock price information for the German DAX index, for example, fromYahoo! Finance with a single line of code:In%5B48%5D:DAX%3Dweb.DataReader(name%3D'%5EGDAXI',data_source%3D'yahoo',start%3D'2000-1-1')DAX.info()Out%5B48%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E         DatetimeIndex: 3760 entries, 2000-01-03 00:00:00 to 2014-09-26 00:00:00         Data columns (total 6 columns):         Open         3760 non-null float64         High         3760 non-null float64         Low          3760 non-null float64         Close        3760 non-null float64         Volume       3760 non-null int64         Adj Close    3760 non-null float64         dtypes: float64(5), int64(1)Table 6-5 presents the parameters that the DataReader function takes.","Table 6-5. Parameters of DataReader function","nameStringName of data set-generally, the ticker symboldata_sourceE.g., %E2%80%9Cyahoo%E2%80%9DData sourcestartString/datetime/NoneLeft boundary of range (default %222010/1/1%E2%80%9C)endString/datetime/NoneRight boundary of range (default today)The tail method provides us with the five last rows of the data set:In%5B49%5D:DAX.tail()Out%5B49%5D:                Open     High      Low    Close    Volume  Adj Close         Date         2014-09-22  9748.53  9812.77  9735.69  9749.54  73981000    9749.54         2014-09-23  9713.40  9719.66  9589.03  9595.03  88196000    9595.03         2014-09-24  9598.77  9669.45  9534.77  9661.97  85850600    9661.97         2014-09-25  9644.36  9718.11  9482.54  9510.01  97697000    9510.01         2014-09-26  9500.55  9545.34  9454.88  9490.55  83499600    9490.55To get a better overview of the index's history, a plot is again generated easily with theplot method (cf. Figure 6-3):In%5B50%5D:DAX%5B'Close'%5D.plot(figsize%3D(8,5))Figure 6-3. Historical DAX index levelsRetrieving data and visualizing it is one thing. Implementing more complex analyticstasks is another. Like NumPyndarrays, pandasallows for vectorized mathematical op%E2%80%90erations on whole, and even complex, DataFrame objects. Take the log returns based onthe daily closing prices as an example. Adding a column with the respective informationcould be achieved with the following code, which first generates a new, empty columnand then iterates over all indexes to calculate the single log return values step by step:","In%5B51%5D:%25%25timeDAX%5B'Ret_Loop'%5D%3D0.0foriinrange(1,len(DAX)):DAX%5B'Ret_Loop'%5D%5Bi%5D%3Dnp.log(DAX%5B'Close'%5D%5Bi%5D/DAX%5B'Close'%5D%5Bi-1%5D)Out%5B51%5D: CPU times: user 452 ms, sys: 12 ms, total: 464 ms         Wall time: 449 msIn%5B52%5D:DAX%5B%5B'Close','Ret_Loop'%5D%5D.tail()Out%5B52%5D:               Close  Ret_Loop         Date         2014-09-22  9749.54 -0.005087         2014-09-23  9595.03 -0.015975         2014-09-24  9661.97  0.006952         2014-09-25  9510.01 -0.015853         2014-09-26  9490.55 -0.002048Alternatively, you can use vectorized code to reach the same result without looping. Tothis end, the shift method is useful%3B it shifts Series or whole DataFrameobjects relativeto their index, forward as well as backward. To accomplish our goal, we need to shiftthe Close column by one day, or more generally, one index position:In%5B53%5D:%25timeDAX%5B'Return'%5D%3Dnp.log(DAX%5B'Close'%5D/DAX%5B'Close'%5D.shift(1))Out%5B53%5D: CPU times: user 4 ms, sys: 0 ns, total: 4 ms         Wall time: 1.52 msIn%5B54%5D:DAX%5B%5B'Close','Ret_Loop','Return'%5D%5D.tail()Out%5B54%5D:               Close  Ret_Loop    Return         Date         2014-09-22  9749.54 -0.005087 -0.005087         2014-09-23  9595.03 -0.015975 -0.015975         2014-09-24  9661.97  0.006952  0.006952         2014-09-25  9510.01 -0.015853 -0.015853         2014-09-26  9490.55 -0.002048 -0.002048This not only provides the same results with more compact and readable code, but alsois the much faster alternative.","In general, you can use the same vectorization approaches with pandasDataFrame objects as you would whenever you could do such anoperation with two NumPyndarray objects containing the same data.One column with the log return data is enough for our purposes, so we can delete theother one:In%5B55%5D:delDAX%5B'Ret_Loop'%5D","Now let us have a look at the newly generated return data. Figure 6-4illustrates twostylized facts of equity returns:Volatility clusteringVolatility is not constant over time%3B there are periods of high volatility (both highlypositive and negative returns) as well as periods of low volatility.Leverage effectGenerally, volatility and stock market returns are negatively correlated%3B when mar%E2%80%90kets come down volatility rises, and vice versa.Here is the code that generates this plot:In%5B56%5D:DAX%5B%5B'Close','Return'%5D%5D.plot(subplots%3DTrue,style%3D'b',figsize%3D(8,5))Figure 6-4. The DAX index and daily log returnsWhile volatility is something of particular importance for options traders, (technical)stock traders might be more interested in moving averages, or so-called trends. A mov%E2%80%90ing average is easily calculated with the rolling_mean function of pandas (there areother %E2%80%9Crolling%E2%80%9D functions as well, like rolling_max, rolling_min, and rolling_corr):In%5B57%5D:DAX%5B'42d'%5D%3Dpd.rolling_mean(DAX%5B'Close'%5D,window%3D42)DAX%5B'252d'%5D%3Dpd.rolling_mean(DAX%5B'Close'%5D,window%3D252)In%5B58%5D:DAX%5B%5B'Close','42d','252d'%5D%5D.tail()Out%5B58%5D:               Close          42d         252d         Date         2014-09-22  9749.54  9464.947143  9429.476468         2014-09-23  9595.03  9463.780952  9433.168651         2014-09-24  9661.97  9465.300000  9437.122381         2014-09-25  9510.01  9461.880476  9440.479167         2014-09-26  9490.55  9459.425000  9443.769008","A typical stock price chart with the two trends included then looks like Figure 6-5:In%5B59%5D:DAX%5B%5B'Close','42d','252d'%5D%5D.plot(figsize%3D(8,5))Figure 6-5. The DAX index and moving averagesReturning to the more options trader-like perspective, the moving historical standarddeviation of the log returns-i.e. the moving historical volatility-might be more ofinterest:In%5B60%5D:importmathDAX%5B'Mov_Vol'%5D%3Dpd.rolling_std(DAX%5B'Return'%5D,window%3D252)*math.sqrt(252)# moving annual volatilityFigure 6-6 further supports the hypothesis of the leverage effect by clearly showing thatthe historical moving volatility tends to increase when markets come down, and todecrease when they rise:In%5B61%5D:DAX%5B%5B'Close','Mov_Vol','Return'%5D%5D.plot(subplots%3DTrue,style%3D'b',figsize%3D(8,7))","Figure 6-6. The DAX index and moving, annualized volatility","The previous section introduces the leverage effect as a stylized fact of equity marketreturns. So far, the support that we provided is based on the inspection of financial dataplots only. Using pandas, we can also base such analysis on a more formal, statisticalground. The simplest approach is to use (linear) ordinary least-squares regression(OLS).In what follows, the analysis uses two different data sets available on the Web:EURO STOXX 50Historical daily closing values of the EURO STOXX 50 index, composed of Euro%E2%80%90pean blue-chip stocksVSTOXXHistorical daily closing data for the VSTOXX volatility index, calculated on thebasis of volatilities implied by options on the EURO STOXX 50 indexIt is noteworthy that we now (indirectly) use implied volatilities, which relate to ex%E2%80%90pectations with regard to the future volatility development, while the previous DAXanalysis used historical volatility measures. For details, see the %E2%80%9CVSTOXX AdvancedServices%E2%80%9D tutorial pages provided by Eurex.We begin with a few imports:In%5B62%5D:importpandasaspdfromurllibimporturlretrieve","3.See Chapter 7 for more information on input-output operations with Python.For the analysis, we retrieve files from the Web and save them in a folder called data.If there is no such folder already, you might want to create one first via mkdir data. Weproceed by retrieving the most current available information with regard to both indices:In%5B63%5D:es_url%3D'http://www.stoxx.com/download/historical_values/hbrbcpe.txt'vs_url%3D'http://www.stoxx.com/download/historical_values/h_vstoxx.txt'urlretrieve(es_url,'./data/es.txt')urlretrieve(vs_url,'./data/vs.txt')!ls-o./data/*.txt# Windows: use dirOut%5B63%5D: -rw------- 1 yhilpisch      0 Sep 28 11:14 ./data/es50.txt         -rw------- 1 yhilpisch 641180 Sep 28 11:14 ./data/es.txt         -rw------- 1 yhilpisch 330564 Sep 28 11:14 ./data/vs.txtReading the EURO STOXX 50 data directly with pandas is not the best route in thiscase. A little data cleaning beforehand will give a better data structure for the import.Two issues have to be addressed, relating to the header and the structure:%E2%80%A2There are a couple of additional header lines that we do not need for the import.%E2%80%A2From December 27, 2001 onwards, the data set %E2%80%9Csuddenly%E2%80%9D has an additional sem%E2%80%90icolon at the end of each data row.The following code reads the whole data set and removes all blanks:3In%5B64%5D:lines%3Dopen('./data/es.txt','r').readlines()lines%3D%5Bline.replace(' ','')forlineinlines%5DWith regard to the header, we can inspect it easily by printing the first couple of linesof the downloaded data set:In%5B65%5D:lines%5B:6%5DOut%5B65%5D: %5B'PriceIndices-EUROCurrency%5Cn',          'Date%3BBlue-Chip%3BBlue-Chip%3BBroad%3BBroad%3BExUK%3BExEuroZone%3BBlue-Chip%3BBroad%5C         n',          '%3BEurope%3BEuro-Zone%3BEurope%3BEuro-Zone%3B%3B%3BNordic%3BNordic%5Cn',          '%3BSX5P%3BSX5E%3BSXXP%3BSXXE%3BSXXF%3BSXXA%3BDK5F%3BDKXF%5Cn',          '31.12.1986%3B775.00%3B900.82%3B82.76%3B98.58%3B98.06%3B69.06%3B645.26%3B65.56%5Cn',          '01.01.1987%3B775.00%3B900.82%3B82.76%3B98.58%3B98.06%3B69.06%3B645.26%3B65.56%5Cn'%5DThe above-mentioned format change can be seen between lines 3,883 and 3,990 of thefile. From December 27, there suddenly appears an additional semicolon at the end ofeach data row:In%5B66%5D:forlineinlines%5B3883:3890%5D:printline%5B41:%5D,","Out%5B66%5D: 317.10%3B267.23%3B5268.36%3B363.19         322.55%3B272.18%3B5360.52%3B370.94         322.69%3B272.95%3B5360.52%3B370.94         327.57%3B277.68%3B5479.59%3B378.69%3B         329.94%3B278.87%3B5585.35%3B386.99%3B         326.77%3B272.38%3B5522.25%3B380.09%3B         332.62%3B277.08%3B5722.57%3B396.12%3BTo make the data set easier to import, we do the following:1.Generate a new text file.2.Delete unneeded header lines.3.Write an appropriate new header line to the new file.4.Add a helper column, DEL (to catch the trailing semicolons).5.Write all data rows to the new file.With these adjustments, the data set can be imported and the helper column deletedafter the import. But first, the cleaning code:In%5B67%5D:new_file%3Dopen('./data/es50.txt','w')# opens a new filenew_file.writelines('date'%2Blines%5B3%5D%5B:-1%5D%2B'%3BDEL'%2Blines%5B3%5D%5B-1%5D)# writes the corrected third line of the original file# as first line of new filenew_file.writelines(lines%5B4:%5D)# writes the remaining lines of the orignial filenew_file.close()Let us see how the new header looks:In%5B68%5D:new_lines%3Dopen('./data/es50.txt','r').readlines()new_lines%5B:5%5DOut%5B68%5D: %5B'date%3BSX5P%3BSX5E%3BSXXP%3BSXXE%3BSXXF%3BSXXA%3BDK5F%3BDKXF%3BDEL%5Cn',          '31.12.1986%3B775.00%3B900.82%3B82.76%3B98.58%3B98.06%3B69.06%3B645.26%3B65.56%5Cn',          '01.01.1987%3B775.00%3B900.82%3B82.76%3B98.58%3B98.06%3B69.06%3B645.26%3B65.56%5Cn',          '02.01.1987%3B770.89%3B891.78%3B82.57%3B97.80%3B97.43%3B69.37%3B647.62%3B65.81%5Cn',          '05.01.1987%3B771.89%3B898.33%3B82.82%3B98.60%3B98.19%3B69.16%3B649.94%3B65.82%5Cn'%5DIt looks appropriate for the import with the read_csv function of pandas, so wecontinue:In%5B69%5D:es%3Dpd.read_csv('./data/es50.txt',index_col%3D0,parse_dates%3DTrue,sep%3D'%3B',dayfirst%3DTrue)In%5B70%5D:np.round(es.tail())Out%5B70%5D:             SX5P  SX5E  SXXP  SXXE  SXXF  SXXA  DK5F  DKXF  DEL         date         2014-09-22  3096  3257   347   326   403   357  9703   565  NaN         2014-09-23  3058  3206   342   321   398   353  9602   558  NaN","         2014-09-24  3086  3244   344   323   401   355  9629   560  NaN         2014-09-25  3059  3202   341   320   397   353  9538   556  NaN         2014-09-26  3064  3220   342   321   398   353  9559   557  NaNThe helper column has fulfilled its purpose and can now be deleted:In%5B71%5D:deles%5B'DEL'%5Des.info()Out%5B71%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E         DatetimeIndex: 7153 entries, 1986-12-31 00:00:00 to 2014-09-26 00:00:00         Data columns (total 8 columns):         SX5P    7153 non-null float64         SX5E    7153 non-null float64         SXXP    7153 non-null float64         SXXE    7153 non-null float64         SXXF    7153 non-null float64         SXXA    7153 non-null float64         DK5F    7153 non-null float64         DKXF    7153 non-null float64         dtypes: float64(8)Equipped with the knowledge about the structure of the EURO STOXX 50 data set, wecan also use the advanced capabilities of the read_csv function to make the importmore compact and efficient:In%5B72%5D:cols%3D%5B'SX5P','SX5E','SXXP','SXXE','SXXF','SXXA','DK5F','DKXF'%5Des%3Dpd.read_csv(es_url,index_col%3D0,parse_dates%3DTrue,sep%3D'%3B',dayfirst%3DTrue,header%3DNone,skiprows%3D4,names%3Dcols)In%5B73%5D:es.tail()Out%5B73%5D:                SX5P     SX5E    SXXP    SXXE    SXXF    SXXA     DK5F           DKXF         2014-09-22  3096.02  3257.48  346.69  325.68  403.16  357.08  9703.33         564.81         2014-09-23  3057.89  3205.93  341.89  320.72  397.96  352.56  9602.32         558.35         2014-09-24  3086.12  3244.01  344.35  323.42  400.58  354.72  9628.84         559.83         2014-09-25  3059.01  3202.31  341.44  319.77  396.90  352.58  9537.95         555.51         2014-09-26  3063.71  3219.58  342.30  321.39  398.33  352.71  9558.51         556.57Fortunately, the VSTOXX data set is already in a form such that it can be imported a bitmore easily into a DataFrame object:In%5B74%5D:vs%3Dpd.read_csv('./data/vs.txt',index_col%3D0,header%3D2,parse_dates%3DTrue,sep%3D',',dayfirst%3DTrue)vs.info()","Out%5B74%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E         DatetimeIndex: 4010 entries, 1999-01-04 00:00:00 to 2014-09-26 00:00:00         Data columns (total 9 columns):         V2TX    4010 non-null float64         V6I1    3591 non-null float64         V6I2    4010 non-null float64         V6I3    3960 non-null float64         V6I4    4010 non-null float64         V6I5    4010 non-null float64         V6I6    3995 non-null float64         V6I7    4010 non-null float64         V6I8    3999 non-null float64         dtypes: float64(9)Table 6-6 contains the parameters of this important import function. There are a mul%E2%80%90titude of parameters, the majority of which default to None%3B object, of course, is non%E2%80%90default and has to be specified in any case.Table 6-6. Parameters of read_csv function","objectStringFile path, URL, or other sourcesepString, default %22,%22Delimiter to uselineterminatorString (one character)String for line breaksquotecharStringCharacter for quotesquotingIntegerControls recognition of quotesescapecharStringString for escapingdtpyedtype/dictdict of dtype(s) for column(s)compression%22gzip%22/%22bz2%22For decompression of datadialectString/csv.DialectCSV dialect, default ExcelheaderIntegerNumber of header rowsskiprowsIntegerNumber of rows to skipindex_colIntegerNumber of index columns (sequence for multi-index)namesArray-likeColumn names if no header rowsprefixStringString to add to column numbers if no header namesna_valuesList/dictAdditional strings to recognize as NA, NaNtrue_valuesListValues to consider as Truefalse_valuesListValues to consider as Falsekeep_default_naBoolean, default TrueIf True, NaN is added to na_valuesparse_datesBoolean/list, default FalseWhether to parse dates in index columns or multiple columnskeep_date_colBoolean, default FalseKeeps original date columnsdayfirstBoolean, default FalseFor European date convention DD/MMthousandsStringThousands operator","commentStringRest of line as comment (not to be parsed)decimalStringString to indicate decimal, e.g., %22.%22 or %22,%22nrowsIntegerNumber of rows of file to readiteratorBoolean, default FalseReturn TextFileReader objectchunksizeIntegerReturn TextFileReader object for iterationskipfooterIntegerNumber of lines to skip at bottomconvertersDictionaryFunction to convert/translate column dataverboseBoolean, default FalseReport number of NA values in nonnumeric columnsdelimiterStringAlternative to sep, can contain regular expressionsencodingStringEncoding to use, e.g., %22UTF-8%22squeezeBoolean, default FalseReturn one-column data sets as Seriesna_filterBoolean, default FalseDetect missing value markers automaticallyusecolsArray-likeSelection of columns to usemangle_dupe_colsBoolean, default FalseName duplicate columns differentlytupleize_colsBoolean, default FalseLeave a list of tuples on columns as isTo implement the regression analysis, we only need one column from each data set. Wetherefore generate a new DataFrameobject within which we combine the two columnsof interest, namely those for the major indexes. Since VSTOXX data is only availablefrom the beginning of January 1999, we only take data from that date on:In%5B75%5D:importdatetimeasdtdata%3Dpd.DataFrame(%7B'EUROSTOXX':es%5B'SX5E'%5D%5Bes.index%3Edt.datetime(1999,1,1)%5D%7D)data%3Ddata.join(pd.DataFrame(%7B'VSTOXX':vs%5B'V2TX'%5D%5Bvs.index%3Edt.datetime(1999,1,1)%5D%7D))We also fill missing values with the last available values from the time series. We call thefillna method, providing ffill (for forward fill) as the method parameter. Anotheroption would be bfill (for backward fill), which would however lead to a %E2%80%9Cforesight%E2%80%9Dissue:In%5B76%5D:data%3Ddata.fillna(method%3D'ffill')data.info()Out%5B76%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E         DatetimeIndex: 4034 entries, 1999-01-04 00:00:00 to 2014-09-26 00:00:00         Data columns (total 2 columns):         EUROSTOXX    4034 non-null float64         VSTOXX       4034 non-null float64         dtypes: float64(2)In%5B77%5D:data.tail()","Out%5B77%5D:             EUROSTOXX   VSTOXX         2014-09-22    3257.48  15.8303         2014-09-23    3205.93  17.7684         2014-09-24    3244.01  15.9504         2014-09-25    3202.31  17.5658         2014-09-26    3219.58  17.6012Again, a graphical representation of the new data set might provide some insights. In%E2%80%90deed, as Figure 6-7 shows, there seems to be a negative correlation between the twoindexes:In%5B78%5D:data.plot(subplots%3DTrue,grid%3DTrue,style%3D'b',figsize%3D(8,6))Figure 6-7. The EURO STOXX 50 index and the VSTOXX volatility indexHowever, to put this on more formal ground, we want to work again with the log returnsof the two financial time series. Figure 6-8 shows these graphically:In%5B79%5D:rets%3Dnp.log(data/data.shift(1))rets.head()Out%5B79%5D:             EUROSTOXX    VSTOXX         1999-01-04        NaN       NaN         1999-01-05   0.017228  0.489248         1999-01-06   0.022138 -0.165317         1999-01-07  -0.015723  0.256337         1999-01-08  -0.003120  0.021570In%5B80%5D:rets.plot(subplots%3DTrue,grid%3DTrue,style%3D'b',figsize%3D(8,6))","Figure 6-8. Log returns of EURO STOXX 50 and VSTOXXWe have everything together to implement the regression analysis. In what follows, theEURO STOXX 50 returns are taken as the independent variable while the VSTOXXreturns are taken as the dependent variable:In%5B81%5D:xdat%3Drets%5B'EUROSTOXX'%5Dydat%3Drets%5B'VSTOXX'%5Dmodel%3Dpd.ols(y%3Dydat,x%3Dxdat)modelOut%5B81%5D: -------------------------Summary of Regression Analysis----------------         ---------         Formula: Y ~ %3Cx%3E %2B %3Cintercept%3E         Number of Observations:         4033         Number of Degrees of Freedom:   2         R-squared:         0.5322         Adj R-squared:     0.5321         Rmse:              0.0389         F-stat (1, 4031):  4586.3942, p-value:     0.0000         Degrees of Freedom: model 1, resid 4031         -----------------------Summary of Estimated Coefficients---------------         ---------               Variable       Coef    Std Err     t-stat    p-value    CI 2.5%25          CI 97.5%25         -----------------------------------------------------------------------         ---------","                      x    -2.7529     0.0406     -67.72     0.0000    -2.8326           -2.6732              intercept    -0.0001     0.0006      -0.12     0.9043    -0.0013            0.0011         ---------------------------------End of Summary------------------------         ---------Obviously, there is indeed a highly negative correlation. We can access the results asfollows:In%5B82%5D:model.betaOut%5B82%5D: x           -2.752894         intercept   -0.000074         dtype: float64This input, in combination with the raw log return data, is used to generate the plot inFigure 6-9, which provides strong support for the leverage effect:In%5B83%5D:plt.plot(xdat,ydat,'r.')ax%3Dplt.axis()# grab axis valuesx%3Dnp.linspace(ax%5B0%5D,ax%5B1%5D%2B0.01)plt.plot(x,model.beta%5B1%5D%2Bmodel.beta%5B0%5D*x,'b',lw%3D2)plt.grid(True)plt.axis('tight')plt.xlabel('EURO STOXX 50 returns')plt.ylabel('VSTOXX returns')Figure 6-9. Scatter plot of log returns and regression lineAs a final cross-check, we can calculate the correlation between the two financial timeseries directly:In%5B84%5D:rets.corr()Out%5B84%5D:            EUROSTOXX    VSTOXX         EUROSTOXX   1.000000 -0.729538         VSTOXX     -0.729538  1.000000","4.Note that the data provider only provides this type of data for a couple of days back from the current date.Therefore, you might need to use different (i.e., more current) dates to implement the same example.Although the correlation is strongly negative on the whole data set, it varies considerablyover time, as shown in Figure 6-10. The figure uses correlation on a yearly basis, i.e., for252 trading days:In%5B85%5D:pd.rolling_corr(rets%5B'EUROSTOXX'%5D,rets%5B'VSTOXX'%5D,window%3D252).plot(grid%3DTrue,style%3D'b')Figure 6-10. Rolling correlation between EURO STOXX 50 and VSTOXX","By now, you should have a feeling for the strengths of pandas when it comes to financialtime series data. One aspect in this regard has become prevalent in the financial analyticssphere and represents quite a high burden for some market players: high-frequencydata. This brief section illustrates how to cope with tick data instead of daily financialdata. To begin with, a couple of imports:In%5B86%5D:importnumpyasnpimportpandasaspdimportdatetimeasdtfromurllibimporturlretrieve%25matplotlibinlineThe Norwegian online broker Netfondsprovides tick data for a multitude of stocks, inparticular for American names. The web-based API has basically the following format:In%5B87%5D:url1%3D'http://hopey.netfonds.no/posdump.php%3F'url2%3D'date%3D%25s%25s%25s%26paper%3DAAPL.O%26csv_format%3Dcsv'url%3Durl1%2Burl2We want to download, combine, and analyze a week's worth of tick data for the AppleInc. stock, a quite actively traded name. Let us start with the dates of interest:4","In%5B88%5D:year%3D'2014'month%3D'09'days%3D%5B'22','23','24','25'%5D# dates might need to be updatedIn%5B89%5D:AAPL%3Dpd.DataFrame()fordayindays:AAPL%3DAAPL.append(pd.read_csv(url%25(year,month,day),index_col%3D0,header%3D0,parse_dates%3DTrue))AAPL.columns%3D%5B'bid','bdepth','bdeptht','offer','odepth','odeptht'%5D# shorter colummn namesThe data set now consists of almost 100,000 rows:In%5B90%5D:AAPL.info()Out%5B90%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E         DatetimeIndex: 95871 entries, 2014-09-22 10:00:01 to 2014-09-25 22:19:25         Data columns (total 6 columns):         bid        95871 non-null float64         bdepth     95871 non-null float64         bdeptht    95871 non-null float64         offer      95871 non-null float64         odepth     95871 non-null float64         odeptht    95871 non-null float64         dtypes: float64(6)Figure 6-11shows the bid columns graphically. One can identify a number of periodswithout any trading activity-i.e., times when the markets are closed:In%5B91%5D:AAPL%5B'bid'%5D.plot()Over the course of a single trading day when markets are open, there is of course usuallya high activity level. Figure 6-12 shows the trading activity for the first day in the sampleand three hours of the third. Times are for the Norwegian time zone and you can seeeasily when pre-trading starts, when US stock markets are open, and when they close:In%5B92%5D:to_plot%3DAAPL%5B%5B'bid','bdeptht'%5D%5D%5B(AAPL.index%3Edt.datetime(2014,9,22,0,0))%26(AAPL.index%3Cdt.datetime(2014,9,23,2,59))%5D# adjust dates to given data setto_plot.plot(subplots%3DTrue,style%3D'b',figsize%3D(8,5))","Figure 6-11. Apple stock tick data for a weekFigure 6-12. Apple stock tick data and volume for a trading dayUsually, financial tick data series lead to a DatetimeIndex that is highly irregular. Inother words, time intervals between two observation points are highly heterogeneous.Against this background, a resamplingof such data sets might sometimes be useful oreven in order depending on the task at hand. pandas provides a method for this purposefor the DataFrameobject. In what follows, we simply take the mean for the resamplingprocedure%3B this might be consistent for some columns (e.g., %E2%80%9Cbid%E2%80%9D) but not for others(e.g., %E2%80%9Cbdepth%E2%80%9D):In%5B93%5D:AAPL_resam%3DAAPL.resample(rule%3D'5min',how%3D'mean')np.round(AAPL_resam.head(),2)Out%5B93%5D:                         bid  bdepth  bdeptht   offer  odepth  odeptht         2014-09-22 10:00:00  100.49  366.67   366.67  100.95     200      200         2014-09-22 10:05:00  100.49  100.00   100.00  100.84     200      200         2014-09-22 10:10:00  100.54  150.00   150.00  100.74     100      100","         2014-09-22 10:15:00  100.59  200.00   200.00  100.75    1500     1500         2014-09-22 10:20:00  100.50  100.00   100.00  100.75    1500     1500The resulting plot in Figure 6-13looks a bit smoother. Here, we have also filled emptytime intervals with the most recent available values (before the empty time interval):In%5B94%5D:AAPL_resam%5B'bid'%5D.fillna(method%3D'ffill').plot()Figure 6-13. Resampled Apple stock tick dataTo conclude this section, we apply a custom-defined Pythonfunction to our new dataset. The function we choose is arbitrary and does not make any economic sense here%3Bit just mirrors the stock performance at a certain stock price level (compare Figure 6-14to Figure 6-13):In%5B95%5D:defreversal(x):return2*95-xIn%5B96%5D:AAPL_resam%5B'bid'%5D.fillna(method%3D'ffill').apply(reversal).plot()Finally, let's clean up disk space by erasing all data sets saved to disk:In%5B97%5D:!rm./data/*# Windows: del /data/*","Figure 6-14. Resampled Apple stock tick data with function applied to it","Financial time series data is one of the most common and important forms of data infinance. The library pandas is generally the tool of choice when it comes to workingwith such data sets. Modeled after the data.frame class of R, the pandasDataFrameclassprovides a wealth of attributes and methods to attack almost any kind of (financial)analytics problem you might face. Convenience is another benefit of using pandas: evenif you might be able to generate the same result by using NumPy and/or matplotlibonly,pandas generally has some neat shortcuts based on a powerful and flexible API.In addition, pandasmakes it really easy to retrieve data from a variety of web sources,like Yahoo! Finance or Google. Compared to %E2%80%9Cpure%E2%80%9D NumPy or matplotlib, it automatesthe management of financial time series data in many respects and also provides higherflexibility when it comes to combining data sets and enlarging existing ones.","At the time of this writing, the definitive resource in printed form for pandas is the bookby the main author of the library:%E2%80%A2McKinney, Wes (2012): Data Analysis with Python. O'Reilly, Sebastopol, CA.Of course, the Web-especially the website of pandasitself-also provides a wealth ofinformation:%E2%80%A2Again, it is good to start on the home page of the library: http://pandas.pydata.org.%E2%80%A2There is rather comprehensive online documentation available at http://pandas.pydata.org/pandas-docs/stable/.%E2%80%A2The documentation in PDF format with 1,500%2B pages illustrates how much func%E2%80%90tionality pandas has to offer: http://pandas.pydata.org/pandas-docs/stable/pandas.pdf.","1.Here, we do not distinguish between different levels of RAM and processor caches. The optimal use of currentmemory architectures is a topic in itself.","It is a capital mistake to theorize before one has data.- Sherlock HolmesAs a general rule, the majority of data, be it in a finance context or any other applicationarea, is stored on hard disk drives (HDDs) or some other form of permanent storagedevice, like solid state disks (SSDs) or hybrid disk drives. Storage capacities have beensteadily increasing over the years, while costs per storage unit (e.g., megabytes) havebeen steadily falling.At the same time, stored data volumes have been increasing at a much faster pace thanthe typical random access memory (RAM) available even in the largest machines. Thismakes it necessary not only to store data to disk for permanent storage, but also tocompensate for lack of sufficient RAM by swapping data from RAM to disk and back.Input/output (I/O) operations are therefore generally very important tasks when itcomes to finance applications and data-intensive applications in general. Often theyrepresent the bottleneck for performance-critical computations, since I/O operationscannot in general shuffle data fast enough to the RAM1and from the RAM to the disk.In a sense, CPUs are often %E2%80%9Cstarving%E2%80%9D due to slow I/O operations.Although the majority of today's financial and corporate analytics efforts are confrontedwith %E2%80%9Cbig%E2%80%9D data (e.g., of petascale size), single analytics tasks generally use data (sub)setsthat fall in the %E2%80%9Cmid%E2%80%9D data category. A recent study concluded:Our measurements as well as other recent work shows that the majority of real-worldanalytic jobs process less than 100 GB of input, but popular infrastructures such as Ha%E2%80%90doop/MapReduce were originally designed for petascale processing.- Appuswamy et al. (2013)","In terms of frequency, single financial analytics tasks generally process data of not morethan a couple of gigabytes (GB) in size-and this is a sweet spot for Python and thelibraries of its scientific stack, like NumPy, pandas, and PyTables. Data sets of such a sizecan also be analyzed in-memory, leading to generally high speeds with today's CPUsand GPUs. However, the data has to be read into RAM and the results have to be writtento disk, meanwhile ensuring today's performance requirements are met.This chapter addresses the following areas:Basic I/OPythonhas built-in functions to serialize and store any object on disk and to readit from disk into RAM%3B apart from that, Pythonis strong when it comes to workingwith text files and SQL databases. NumPyalso provides dedicated functions for faststorage and retrieval of ndarray objects.I/O with pandasThe pandas library provides a plentitude of convenience functions and methods toread data stored in different formats (e.g., CSV, JSON) and to write data to files indiverse formats.I/O with PyTablesPyTables uses the HDF5standard to accomplish fast I/O operations for large datasets%3B speed often is only bound by the hardware used.","Python itself comes with a multitude of I/O capabilites, some optimized for perfor%E2%80%90mance, others more for flexibility. In general, however, they are easily used in interactiveas well as in large-scale deployment settings.","For later use, for documentation, or for sharing with others, one might want to storePython objects on disk. One option is to use the pickle module. This module canserialize the majority of Python objects. Serialization refers to the conversion of an object(hierarchy) to a byte stream%3B deserialization is the opposite operation. In the examplethat follows, we work again with (pseudo)random data, this time stored in a listobject:In%5B1%5D:path%3D'/flash/data/'In%5B2%5D:importnumpyasnpfromrandomimportgaussIn%5B3%5D:a%3D%5Bgauss(1.5,2)foriinrange(1000000)%5D# generation of normally distributed randoms","The task now is to write this list object to disk for later retrieval. pickleaccomplishesthis task:In%5B4%5D:importpickleIn%5B5%5D:pkl_file%3Dopen(path%2B'data.pkl','w')# open file for writing# Note: existing file might be overwrittenThe two major functions we need are dump, for writing objects, and load, for loadingthem into the memory:In%5B6%5D:%25timepickle.dump(a,pkl_file)Out%5B6%5D: CPU times: user 4.3 s, sys: 43 ms, total: 4.35 s        Wall time: 4.36 sIn%5B7%5D:pkl_fileOut%5B7%5D: %3Copen file '/flash/data/data.pkl', mode 'w' at 0x3df0540%3EIn%5B8%5D:pkl_file.close()We can now inspect the size of the file on disk. The list object with 1,000,000 floatstakes about 20 megabytes (MB) of disk space:In%5B9%5D:ll%24path*Out%5B9%5D: -rw-r--r-- 1 root 20970325 28. Sep 15:16 /flash/data/data.pklNow that we have data on disk, we can read it into memory via pickle.load:In%5B10%5D:pkl_file%3Dopen(path%2B'data.pkl','r')# open file for readingIn%5B11%5D:%25timeb%3Dpickle.load(pkl_file)Out%5B11%5D: CPU times: user 3.37 s, sys: 18 ms, total: 3.38 s         Wall time: 3.39 sIn%5B12%5D:b%5B:5%5DOut%5B12%5D: %5B-3.6459230447943165,          1.4637510875573307,          2.5483218463404067,          0.9822259685028746,          3.594915396586916%5DLet us compare this with the first five floats of the original object:In%5B13%5D:a%5B:5%5DOut%5B13%5D: %5B-3.6459230447943165,          1.4637510875573307,          2.5483218463404067,          0.9822259685028746,          3.594915396586916%5D","To ensure that objects a and b are indeed the same, NumPy provides the function allclose:In%5B14%5D:np.allclose(np.array(a),np.array(b))Out%5B14%5D: TrueIn principle, this is the same as calculating the difference of two ndarrayobjects andchecking whether it is 0:In%5B15%5D:np.sum(np.array(a)-np.array(b))Out%5B15%5D: 0.0However, allclose takes as a parameter a tolerance level, which by default is set to 1e-5.Storing and retrieving a single object with pickle obviously is quite simple. What abouttwo objects%3FIn%5B16%5D:pkl_file%3Dopen(path%2B'data.pkl','w')# open file for writingIn%5B17%5D:%25timepickle.dump(np.array(a),pkl_file)Out%5B17%5D: CPU times: user 799 ms, sys: 47 ms, total: 846 ms         Wall time: 846 msIn%5B18%5D:%25timepickle.dump(np.array(a)**2,pkl_file)Out%5B18%5D: CPU times: user 742 ms, sys: 41 ms, total: 783 ms         Wall time: 784 msIn%5B19%5D:pkl_file.close()In%5B20%5D:ll%24path*Out%5B20%5D: -rw-r--r-- 1 root 44098737 28. Sep 15:16 /flash/data/data.pklWhat has happened%3F Mainly the following:%E2%80%A2We have written an ndarray version of the original object to disk.%E2%80%A2We have also written a squared ndarray version to disk, into the same file.%E2%80%A2Both operations were faster than the original operation (due to the use of ndarrayobjects).%E2%80%A2The file is approximately double the size as before, since we have stored double theamount of data.Let us read the two ndarray objects back into memory:In%5B21%5D:pkl_file%3Dopen(path%2B'data.pkl','r')# open file for readingpickle.loaddoes the job. However, notice that it only returns a single ndarray object:In%5B22%5D:x%3Dpickle.load(pkl_file)x","Out%5B22%5D: array(%5B-3.64592304,  1.46375109,  2.54832185, ...,  2.87048515,                 0.66186994, -1.38532837%5D)Calling pickle.load for the second time returns the second object:In%5B23%5D:y%3Dpickle.load(pkl_file)yOut%5B23%5D: array(%5B 13.29275485,   2.14256725,   6.49394423, ...,   8.23968501,                  0.43807181,   1.9191347 %5D)In%5B24%5D:pkl_file.close()Obviously, pickle stores objects according to the first in, first out (FIFO) principle.There is one major problem with this: there is no meta-information available to the userto know beforehand what is stored in a picklefile. A sometimes helpful workaroundis to not store single objects, but a dict object containing all the other objects:In%5B25%5D:pkl_file%3Dopen(path%2B'data.pkl','w')# open file for writingpickle.dump(%7B'x':x,'y':y%7D,pkl_file)pkl_file.close()Using this approach allows us to read the whole set of objects at once and, for example,to iterate over the dict object's key values:In%5B26%5D:pkl_file%3Dopen(path%2B'data.pkl','r')# open file for writingdata%3Dpickle.load(pkl_file)pkl_file.close()forkeyindata.keys():printkey,data%5Bkey%5D%5B:4%5DOut%5B26%5D: y %5B 13.29275485   2.14256725   6.49394423   0.96476785%5D         x %5B-3.64592304  1.46375109  2.54832185  0.98222597%5DIn%5B27%5D:!rm-f%24path*This approach, however, requires us to write and read all objects at once. This is acompromise one can probably live with in many circumstances given the much higherconvenience it brings along.","Text processing can be considered a strength of Python. In fact, many corporate andscientific users use Python for exactly this task. With Pythonyou have a multitude ofoptions to work with string objects, as well as with text files in general.Suppose we have generated quite a large set of data that we want to save and share as acomma-separated value (CSV) file. Although they have a special structure, such files arebasically plain text files:In%5B28%5D:rows%3D5000a%3Dnp.random.standard_normal((rows,5))# dummy data","In%5B29%5D:a.round(4)Out%5B29%5D: array(%5B%5B 1.381 , -1.1236,  1.0622, -1.3997, -0.7374%5D,                %5B 0.15  ,  0.967 ,  1.8391,  0.5633,  0.0569%5D,                %5B-0.9504,  0.4779,  1.8636, -1.9152, -0.3005%5D,                ...,                %5B 0.8843, -1.3932, -0.0506,  0.2717, -1.4921%5D,                %5B-1.0352,  1.0368,  0.4562, -0.0667, -1.3391%5D,                %5B 0.9952, -0.6398,  0.8467, -1.6951,  1.122 %5D%5D)To make the case a bit more realistic, we add date-time information to the mix and usethe pandasdate_range function to generate a series of hourly date-time points (fordetails, see Chapter 6 and Appendix C):In%5B30%5D:importpandasaspdt%3Dpd.date_range(start%3D'2014/1/1',periods%3Drows,freq%3D'H')# set of hourly datetime objectsIn%5B31%5D:tOut%5B31%5D: %3Cclass 'pandas.tseries.index.DatetimeIndex'%3E         %5B2014-01-01 00:00:00, ..., 2014-07-28 07:00:00%5D         Length: 5000, Freq: H, Timezone: NoneTo write the data, we need to open a new file object on disk:In%5B32%5D:csv_file%3Dopen(path%2B'data.csv','w')# open file for writingThe first line of a CSVfile generally contains the names for each data column stored inthe file, so we write this first:In%5B33%5D:header%3D'date,no1,no2,no3,no4,no5%5Cn'csv_file.write(header)The actual data is then written row by row, merging the date-time information with the(pseudo)random numbers:In%5B34%5D:fort_,(no1,no2,no3,no4,no5)inzip(t,a):s%3D'%25s,%25f,%25f,%25f,%25f,%25f%5Cn'%25(t_,no1,no2,no3,no4,no5)csv_file.write(s)csv_file.close()In%5B35%5D:ll%24path*Out%5B35%5D: -rw-r--r-- 1 root 337664 28. Sep 15:16 /flash/data/data.csvThe other way around works quite similarly. First, open the now-existing CSV file. Sec%E2%80%90ond, read its content line by line using the readline method of the file object:In%5B36%5D:csv_file%3Dopen(path%2B'data.csv','r')# open file for readingIn%5B37%5D:foriinrange(5):printcsv_file.readline(),Out%5B37%5D: date,no1,no2,no3,no4,no5         2014-01-01 00:00:00,1.381035,-1.123613,1.062245,-1.399746,-0.737369         2014-01-01 01:00:00,0.149965,0.966987,1.839130,0.563322,0.056906","2.Another first-class citizen in the database world is MySQL, with which Python also integrates very well. Whilemany web projects are implemented on the basis of the so-called LAMP stack, which generally stands forLinux, Apache Web server, MySQL, PHP, there are also a large number of stacks where Python replacesPHP for the P in the stack. For an overview of available database connectors, visit https://wiki.python.org/moin/DatabaseInterfaces.         2014-01-01 02:00:00,-0.950360,0.477881,1.863646,-1.915203,-0.300522         2014-01-01 03:00:00,-0.503429,-0.895489,-0.240227,-0.327176,0.123498You can also read all the content at once by using the readlines method:In%5B38%5D:csv_file%3Dopen(path%2B'data.csv','r')content%3Dcsv_file.readlines()forlineincontent%5B:5%5D:printline,Out%5B38%5D: date,no1,no2,no3,no4,no5         2014-01-01 00:00:00,1.381035,-1.123613,1.062245,-1.399746,-0.737369         2014-01-01 01:00:00,0.149965,0.966987,1.839130,0.563322,0.056906         2014-01-01 02:00:00,-0.950360,0.477881,1.863646,-1.915203,-0.300522         2014-01-01 03:00:00,-0.503429,-0.895489,-0.240227,-0.327176,0.123498We finish with some closing operations in this example:In%5B39%5D:csv_file.close()!rm-f%24path*","Python can work with any kind of SQL database and in general also with any kind ofNoSQLdatabase. One database that is delivered with Pythonby default is SQLite3. Withit, the basic Python approach to SQL databases can be easily illustrated:2In%5B40%5D:importsqlite3assq3SQL queries are formulated as string objects. The syntax, data types, etc. of coursedepend on the database in use:In%5B41%5D:query%3D'CREATE TABLE numbs (Date date, No1 real, No2 real)'Open a database connection. In this case, we generate a new database file on disk:In%5B42%5D:con%3Dsq3.connect(path%2B'numbs.db')Then execute the query statement to create the table by using the method execute:In%5B43%5D:con.execute(query)Out%5B43%5D: %3Csqlite3.Cursor at 0xb8a4490%3ETo make the query effective, call the method commit:In%5B44%5D:con.commit()","Now that we have a database file with a table, we can populate that table with data. Eachrow consists of date-time information and two floats:In%5B45%5D:importdatetimeasdtA single data row can be written with the respective SQL statement, as follows:In%5B46%5D:con.execute('INSERT INTO numbs VALUES(%3F, %3F, %3F)',(dt.datetime.now(),0.12,7.3))Out%5B46%5D: %3Csqlite3.Cursor at 0xb8a4570%3EHowever, you usually have to (or want to) write a larger data set in bulk:In%5B47%5D:data%3Dnp.random.standard_normal((10000,2)).round(5)In%5B48%5D:forrowindata:con.execute('INSERT INTO numbs VALUES(%3F, %3F, %3F)',(dt.datetime.now(),row%5B0%5D,row%5B1%5D))con.commit()There is also a method called executemany. Since we have combined current date-timeinformation with our pseudorandom number data set, we cannot use it here. What wecan use, however, is fetchmany to retrieve a certain number of rows at once from thedatabase:In%5B49%5D:con.execute('SELECT * FROM numbs').fetchmany(10)Out%5B49%5D: %5B(u'2014-09-28 15:16:19.486021', 0.12, 7.3),          (u'2014-09-28 15:16:19.762476', 0.30736, -0.21114),          (u'2014-09-28 15:16:19.762640', 0.95078, 0.50106),          (u'2014-09-28 15:16:19.762702', 0.95896, 0.15812),          (u'2014-09-28 15:16:19.762774', -0.42919, -1.45132),          (u'2014-09-28 15:16:19.762825', -0.99502, -0.91755),          (u'2014-09-28 15:16:19.762862', 0.25416, -0.85317),          (u'2014-09-28 15:16:19.762890', -0.55879, -0.36144),          (u'2014-09-28 15:16:19.762918', -1.61041, -1.29589),          (u'2014-09-28 15:16:19.762945', -2.04225, 0.43446)%5DOr we can just read a single data row at a time:In%5B50%5D:pointer%3Dcon.execute('SELECT * FROM numbs')In%5B51%5D:foriinrange(3):printpointer.fetchone()Out%5B51%5D: (u'2014-09-28 15:16:19.486021', 0.12, 7.3)         (u'2014-09-28 15:16:19.762476', 0.30736, -0.21114)         (u'2014-09-28 15:16:19.762640', 0.95078, 0.50106)In%5B52%5D:con.close()!rm-f%24path*SQLdatabases are a rather broad topic%3B indeed, too broad and complex to be covered inany significant way in this chapter. The basic messages only are:","3.Cf. http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html.%E2%80%A2Python integrates pretty well with almost any database technology.%E2%80%A2The basic SQLsyntax is mainly determined by the database in use%3B the rest is, as wesay, real Pythonic.","NumPy itself has functions to write and read ndarray objects in a convenient and per%E2%80%90formant fashion. This saves a lot of effort in some circumstances, such as when you haveto convert NumPydtypes into specific database types (e.g., for SQLite3). To illustrate thatNumPy can sometimes be an efficient replacement for a SQL-based approach, we replicatethe example from before, this time only using NumPy:In%5B53%5D:importnumpyasnpInstead of pandas, we use the arange function of NumPy to generate an arrayobject withdatetime objects stored:3In%5B54%5D:dtimes%3Dnp.arange('2015-01-01 10:00:00','2021-12-31 22:00:00',dtype%3D'datetime64%5Bm%5D')# minute intervalslen(dtimes)Out%5B54%5D: 3681360What is a table in a SQL database is a structured array with NumPy. We use a special dtypeobject mirroring the SQL table from before:In%5B55%5D:dty%3Dnp.dtype(%5B('Date','datetime64%5Bm%5D'),('No1','f'),('No2','f')%5D)data%3Dnp.zeros(len(dtimes),dtype%3Ddty)With the dates object, we populate the Date column:In%5B56%5D:data%5B'Date'%5D%3DdtimesThe other two columns are populated as before with pseudorandom numbers:In%5B57%5D:a%3Dnp.random.standard_normal((len(dtimes),2)).round(5)data%5B'No1'%5D%3Da%5B:,0%5Ddata%5B'No2'%5D%3Da%5B:,1%5DSaving of ndarrayobjects is highly optimized and therefore quite fast. Almost 60 MBof data takes less than 0.1 seconds to save on disk (here using an SSD):In%5B58%5D:%25timenp.save(path%2B'array',data)# suffix .npy is addedOut%5B58%5D: CPU times: user 0 ns, sys: 77 ms, total: 77 ms         Wall time: 77.1 msIn%5B59%5D:ll%24path*","Out%5B59%5D: -rw-r--r-- 1 root 58901888 28. Sep 15:16 /flash/data/array.npyReading is even faster:In%5B60%5D:%25timenp.load(path%2B'array.npy')Out%5B60%5D: CPU times: user 10 ms, sys: 29 ms, total: 39 ms         Wall time: 37.8 ms         array(%5B (datetime.datetime(2015, 1, 1, 9, 0), -1.4985100030899048,         0.9664400219917297),                (datetime.datetime(2015, 1, 1, 9, 1), -0.2501699924468994,         -0.9184499979019165),                (datetime.datetime(2015, 1, 1, 9, 2), 1.2026900053024292,         0.49570000171661377),                ...,                (datetime.datetime(2021, 12, 31, 20, 57), 0.8927800059318542,         -1.0334899425506592),                (datetime.datetime(2021, 12, 31, 20, 58), 1.0062999725341797,         -1.3476499915122986),                (datetime.datetime(2021, 12, 31, 20, 59), -0.08011999726295471,         0.4992400109767914)%5D,               dtype%3D%5B('Date', '%3CM8%5Bm%5D'), ('No1', '%3Cf4'), ('No2', '%3Cf4')%5D)A data set of 60 MB is not that large. Therefore, let us try a somewhat larger ndarrayobject:In%5B61%5D:data%3Dnp.random.standard_normal((10000,6000))In%5B62%5D:%25timenp.save(path%2B'array',data)Out%5B62%5D: CPU times: user 0 ns, sys: 631 ms, total: 631 ms         Wall time: 633 msIn%5B63%5D:ll%24path*Out%5B63%5D: -rw-r--r-- 1 root 480000080 28. Sep 15:16 /flash/data/array.npyIn this case, the file on disk is about 480 MB large and it is written in less than a second.This illustrates that writing to disk in this case is mainly hardware-bound, since 480MB/s represents roughly the advertised writing speed of better SSDs at the time of thiswriting (512 MB/s). Reading the file/object from disk is even faster (note that cachingtechniques might also play a role here):In%5B64%5D:%25timenp.load(path%2B'array.npy')Out%5B64%5D: CPU times: user 2 ms, sys: 216 ms, total: 218 ms         Wall time: 216 ms         array(%5B%5B 0.10989742, -0.48626177, -0.60849881, ..., -0.99051776,                  0.88124291, -1.34261656%5D,                %5B-0.42301145,  0.29831708,  1.29729826, ..., -0.73426192,                 -0.13484905,  0.91787421%5D,                %5B 0.12322789, -0.28728811,  0.85956891, ...,  1.47888978,                 -1.12452641, -0.528133  %5D,","                ...,                %5B 0.06507559, -0.37130379,  1.35427048, ..., -1.4457718 ,                  0.49509821,  0.0738847 %5D,                %5B 1.76525714, -0.07876135, -2.94133788, ..., -0.62581084,                  0.0933164 ,  1.55788205%5D,                %5B-1.18439949, -0.73210571, -0.45845113, ...,  0.0528656 ,                 -0.39526633, -0.5964333 %5D%5D)In%5B65%5D:data%3D0.0!rm-f%24path*In any case, you can expect that this form of data storage and retrieval is much, muchfaster as compared to SQL databases or using the standard pickle library for serializa%E2%80%90tion. Of course, you do not have the functionality of a SQL database available with thisapproach, but PyTables will help in this regard, as subsequent sections show.","One of the major strengths of the pandaslibrary is that it can read and write differentdata formats natively, including among others:%E2%80%A2CSV (comma-separated value)%E2%80%A2SQL (Structured Query Language)%E2%80%A2XLS/XSLX (Microsoft Excel files)%E2%80%A2JSON (JavaScriptObject Notation)%E2%80%A2HTML (HyperText Markup Language)Table 7-1 lists all the supported formats and the corresponding import and exportfunctions/methods of pandas. The parameters that the import functions take are listedand described in Table 6-6 (depending on the functions, some other conventions mightapply).Table 7-1. Parameters of DataFrame function","CSVread_csvto_csvText fileXLS/XLSXread_excelto_excelSpreadsheetHDFread_hdfto_hdfHDF5 databaseSQLread_sqlto_sqlSQL tableJSONread_jsonto_jsonJavaScript Object NotationMSGPACKread_msgpackto_msgpackPortable binary formatHTMLread_htmlto_htmlHTML codeGBQread_gbqto_gbqGoogle Big Query format","DTAread_statato_stataFormats 104, 105, 108, 113-115, 117Anyread_clipboardto_clipboardE.g., from HTML pageAnyread_pickleto_pickle(Structured) Python objectOur test case will again be a large set of floating-point numbers:In%5B66%5D:importnumpyasnpimportpandasaspddata%3Dnp.random.standard_normal((1000000,5)).round(5)# sample data setIn%5B67%5D:filename%3Dpath%2B'numbs'To this end, we will also revisit SQLite3and will compare the performance with alter%E2%80%90native approaches using pandas.","All that follows with regard to SQLite3 should be known by now:In%5B68%5D:importsqlite3assq3In%5B69%5D:query%3D'CREATE TABLE numbers (No1 real, No2 real,%5C                 No3 real, No4 real, No5 real)'In%5B70%5D:con%3Dsq3.Connection(filename%2B'.db')In%5B71%5D:con.execute(query)Out%5B71%5D: %3Csqlite3.Cursor at 0x9d59c00%3EThis time, executemany can be applied since we write from a single ndarray object:In%5B72%5D:%25%25timecon.executemany('INSERT INTO numbers VALUES (%3F, %3F, %3F, %3F, %3F)',data)con.commit()Out%5B72%5D: CPU times: user 13.9 s, sys: 229 ms, total: 14.2 s         Wall time: 14.9 sIn%5B73%5D:ll%24path*Out%5B73%5D: -rw-r--r-- 1 root 54446080 28. Sep 15:16 /flash/data/numbs.dbWriting the whole data set of 1,000,000 rows takes quite a while. The reading of thewhole table into a list object is much faster:In%5B74%5D:%25%25timetemp%3Dcon.execute('SELECT * FROM numbers').fetchall()printtemp%5B:2%5Dtemp%3D0.0Out%5B74%5D: %5B(-1.67378, -0.58292, -1.10616, 1.14929, -0.0393), (1.38006, 0.82665, 0         .34168, -1.1676, -0.53274)%5D","         CPU times: user 1.54 s, sys: 138 ms, total: 1.68 s         Wall time: 1.68 sReading SQLquery results directly into a NumPyndarray object is easily accomplished.Accordingly, you can also easily plot the results of such a query, as shown by the fol%E2%80%90lowing code and the output in Figure 7-1:In%5B75%5D:%25%25timequery%3D'SELECT * FROM numbers WHERE No1 %3E 0 AND No2 %3C 0'res%3Dnp.array(con.execute(query).fetchall()).round(3)Out%5B75%5D: CPU times: user 766 ms, sys: 34 ms, total: 800 ms         Wall time: 799 msIn%5B76%5D:res%3Dres%5B::100%5D# every 100th resultimportmatplotlib.pyplotasplt%25matplotlibinlineplt.plot(res%5B:,0%5D,res%5B:,1%5D,'ro')plt.grid(True)%3Bplt.xlim(-0.5,4.5)%3Bplt.ylim(-4.5,0.5)Figure 7-1. Plot of the query result","A generally more efficient approach, however, is the reading of either whole tables orquery results with pandas. When you are able to read a whole table into memory, ana%E2%80%90lytical queries can generally be executed much faster than when using the SQLdisk-based approach. The sublibrary pandas.io.sql contains functions to handle data stor%E2%80%90ed in SQL databases:In%5B77%5D:importpandas.io.sqlaspdsReading the whole table with pandas takes roughly the same amount of time as readingit into a NumPyndarray object. There as here, the bottleneck is the SQL database:In%5B78%5D:%25timedata%3Dpds.read_sql('SELECT * FROM numbers',con)","Out%5B78%5D: CPU times: user 2.16 s, sys: 60 ms, total: 2.22 s         Wall time: 2.23 sIn%5B79%5D:data.head()Out%5B79%5D:        No1      No2      No3      No4      No5         0 -1.67378 -0.58292 -1.10616  1.14929 -0.03930         1  1.38006  0.82665  0.34168 -1.16760 -0.53274         2  0.79329  0.11947  2.06403 -0.36208  1.77442         3 -0.33507 -0.00715 -1.01193  0.23157  1.30225         4 -0.35292  0.67483  1.59507 -1.21263  0.14745         %5B5 rows x 5 columns%5DThe data is now in-memory. This allows for much faster analytics. The SQL query thattakes a few seconds with SQLite3 finishes in less than 0.1 seconds with pandasin-memory:In%5B80%5D:%25timedata%5B(data%5B'No1'%5D%3E0)%26(data%5B'No2'%5D%3C0)%5D.head()Out%5B80%5D: CPU times: user 50 ms, sys: 0 ns, total: 50 ms         Wall time: 49.9 ms                 No1      No2      No3      No4      No5         6   1.17749 -1.13017 -0.24176 -0.64047  1.58002         8   0.18625 -0.99949  2.29854  0.91816 -0.92661         9   1.09481 -0.26301  1.11341  0.68716 -0.71524         18  0.31836 -0.33039 -1.50109  0.52961  0.96595         20  0.40261 -0.45917  0.37339 -1.09515  0.23972         %5B5 rows x 5 columns%5Dpandascan master even more complex queries, although it is neither meant nor able toreplace SQLdatabases when it comes to complex, relational data structures. The resultof the next query is shown in Figure 7-2:In%5B81%5D:%25%25timeres%3Ddata%5B%5B'No1','No2'%5D%5D%5B((data%5B'No1'%5D%3E0.5)%7C(data%5B'No1'%5D%3C-0.5))%26((data%5B'No2'%5D%3C-1)%7C(data%5B'No2'%5D%3E1))%5DOut%5B81%5D: CPU times: user 49 ms, sys: 0 ns, total: 49 ms         Wall time: 48.7 msIn%5B82%5D:plt.plot(res.No1,res.No2,'ro')plt.grid(True)%3Bplt.axis('tight')","Figure 7-2. Scatter plot of complex query resultAs expected, using the in-memory analytics capabilities of pandasleads to a significantspeedup, provided pandasis able to replicate the respective SQLstatement. This is notthe only advantage of using pandas, though pandas is tightly integrated with PyTables, which is the topic of the next section. Here, it suffices to know that the combinationof both can speed up I/O operations considerably. This is shown in the following:In%5B83%5D:h5s%3Dpd.HDFStore(filename%2B'.h5s','w')In%5B84%5D:%25timeh5s%5B'data'%5D%3DdataOut%5B84%5D: CPU times: user 43 ms, sys: 60 ms, total: 103 ms         Wall time: 161 msIn%5B85%5D:h5sOut%5B85%5D: %3Cclass 'pandas.io.pytables.HDFStore'%3E         File path: /flash/data/numbs.h5s         /data            frame        (shape-%3E%5B1000000,5%5D)In%5B86%5D:h5s.close()The whole DataFrame with all the data from the original SQL table is written in wellbelow 1 second. Reading is even faster, as is to be expected:In%5B87%5D:%25%25timeh5s%3Dpd.HDFStore(filename%2B'.h5s','r')temp%3Dh5s%5B'data'%5Dh5s.close()Out%5B87%5D: CPU times: user 13 ms, sys: 22 ms, total: 35 ms         Wall time: 32.7 msA brief check of whether the data sets are indeed the same:In%5B88%5D:np.allclose(np.array(temp),np.array(data))Out%5B88%5D: TrueIn%5B89%5D:temp%3D0.0","Also, a look at the two files now on disk, showing that the HDF5format consumes some%E2%80%90what less disk space:In%5B90%5D:ll%24path*Out%5B90%5D: -rw-r--r-- 1 root 54446080 28. Sep 15:16 /flash/data/numbs.db         -rw-r--r-- 1 root 48007368 28. Sep 15:16 /flash/data/numbs.h5sAs a summary, we can state the following with regard to our dummy data set, which isroughly 50 MB in size:%E2%80%A2Writing the data with SQLite3 takes multiple seconds, with pandas taking much lessthan a second.%E2%80%A2Reading the data from the SQLdatabase takes a bit more than a few seconds, withpandas taking less than 0.1 second.","One of the most widely used formats to exchange data is the CSVformat. Although it isnot really standardized, it can be processed by any platform and the vast majority ofapplications concerned with data and financial analytics. The previous section showshow to write and read data to and from CSV files step by step with standard Pythonfunctionality (cf. %E2%80%9CReading and Writing Text Files%E2%80%9D on page 177). pandasmakes thiswhole procedure a bit more convenient, the code more concise, and the execution ingeneral faster:In%5B91%5D:%25timedata.to_csv(filename%2B'.csv')Out%5B91%5D: CPU times: user 5.55 s, sys: 137 ms, total: 5.69 s         Wall time: 5.87 sReading the data now stored in the CSV file and plotting it is accomplished with theread_csv function (cf. Figure 7-3 for the result):In%5B92%5D:%25%25timepd.read_csv(filename%2B'.csv')%5B%5B'No1','No2','No3','No4'%5D%5D.hist(bins%3D20)Out%5B92%5D: CPU times: user 1.72 s, sys: 54 ms, total: 1.77 s         Wall time: 1.78 s","Figure 7-3. Histogram of four data sets","Although working with Excel spreadsheets is the topic of a later chapter, we want tobriefly demonstrate how pandas can write data in Excel format and read data fromExcel spreadsheets. We restrict the data set to 100,000 rows in this case:In%5B93%5D:%25timedata%5B:100000%5D.to_excel(filename%2B'.xlsx')Out%5B93%5D: CPU times: user 27.5 s, sys: 131 ms, total: 27.6 s         Wall time: 27.7 sGenerating the Excelspreadsheet with this small subset of the data takes quite a while.This illustrates what kind of overhead the spreadsheet structure brings along with it.Reading (and plotting) the data is a faster procedure (cf. Figure 7-4):In%5B94%5D:%25timepd.read_excel(filename%2B'.xlsx','Sheet1').cumsum().plot()Out%5B94%5D: CPU times: user 12.9 s, sys: 6 ms, total: 12.9 s         Wall time: 12.9 sFigure 7-4. Paths of random data from Excel file","Inspection of the generated files reveals that the DataFrame with HDFStorecombinationis the most compact alternative (using compression, as described later in this chapter,further increases the benefits). The same amount of data as a CSVfile-i.e., as a text file-is somewhat larger in size. This is one reason for the slower performance when work%E2%80%90ing with CSV files, the other being the very fact that they are %E2%80%9Conly%E2%80%9D general text files:In%5B95%5D:ll%24path*Out%5B95%5D: -rw-r--r-- 1 root 48831681 28. Sep 15:17 /flash/data/numbs.csv         -rw-r--r-- 1 root 54446080 28. Sep 15:16 /flash/data/numbs.db         -rw-r--r-- 1 root 48007368 28. Sep 15:16 /flash/data/numbs.h5s         -rw-r--r-- 1 root  4311424 28. Sep 15:17 /flash/data/numbs.xlsxIn%5B96%5D:rm-f%24path*","PyTables is a Python binding for the HDF5 database/file standard (cf. http://www.hdfgroup.org). It is specifically designed to optimize the performance of I/O op%E2%80%90erations and make best use of the available hardware. The library's import name istables. Similar to pandaswhen it comes to in-memory analytics, PyTables is neitherable nor meant to be a full replacement for SQL databases. However, it brings along somefeatures that further close the gap. For example, a PyTablesdatabase can have manytables, and it supports compression and indexing and also nontrivial queries on tables.In addition, it can store NumPy arrays efficiently and has its own flavor of array-like datastructures.We begin with a few imports:In%5B97%5D:importnumpyasnpimporttablesastbimportdatetimeasdtimportmatplotlib.pyplotasplt%25matplotlibinline","PyTables provides a file-based database format:In%5B98%5D:filename%3Dpath%2B'tab.h5'h5%3Dtb.open_file(filename,'w')For our example case, we generate a table with 2,000,000 rows of data:In%5B99%5D:rows%3D2000000The table itself has a datetime column, two int columns, and two float columns:In%5B100%5D:row_des%3D%7B'Date':tb.StringCol(26,pos%3D1),'No1':tb.IntCol(pos%3D2),","'No2':tb.IntCol(pos%3D3),'No3':tb.Float64Col(pos%3D4),'No4':tb.Float64Col(pos%3D5)%7DWhen creating the table, we choose no compression. A later example will add com%E2%80%90pression as well:In%5B101%5D:filters%3Dtb.Filters(complevel%3D0)# no compressiontab%3Dh5.create_table('/','ints_floats',row_des,title%3D'Integers and Floats',expectedrows%3Drows,filters%3Dfilters)In%5B102%5D:tabOut%5B102%5D: /ints_floats (Table(0,)) 'Integers and Floats'            description :%3D %7B            %22Date%22: StringCol(itemsize%3D26, shape%3D(), dflt%3D'', pos%3D0),            %22No1%22: Int32Col(shape%3D(), dflt%3D0, pos%3D1),            %22No2%22: Int32Col(shape%3D(), dflt%3D0, pos%3D2),            %22No3%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D3),            %22No4%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D4)%7D            byteorder :%3D 'little'            chunkshape :%3D (2621,)In%5B103%5D:pointer%3Dtab.rowNow we generate the sample data:In%5B104%5D:ran_int%3Dnp.random.randint(0,10000,size%3D(rows,2))ran_flo%3Dnp.random.standard_normal((rows,2)).round(5)The sample data set is written row-by-row to the table:In%5B105%5D:%25%25timeforiinrange(rows):pointer%5B'Date'%5D%3Ddt.datetime.now()pointer%5B'No1'%5D%3Dran_int%5Bi,0%5Dpointer%5B'No2'%5D%3Dran_int%5Bi,1%5Dpointer%5B'No3'%5D%3Dran_flo%5Bi,0%5Dpointer%5B'No4'%5D%3Dran_flo%5Bi,1%5Dpointer.append()# this appends the data and# moves the pointer one row forwardtab.flush()Out%5B105%5D: CPU times: user 15.7 s, sys: 3.53 s, total: 19.2 s          Wall time: 19.4 sAlways remember to commit your changes. What the commitmethod is for the SQLite3database, the flushmethod is for PyTables. We can now inspect the data on disk, firstlogically via our Table object and second physically via the file information:","In%5B106%5D:tabOut%5B106%5D: /ints_floats (Table(2000000,)) 'Integers and Floats'            description :%3D %7B            %22Date%22: StringCol(itemsize%3D26, shape%3D(), dflt%3D'', pos%3D0),            %22No1%22: Int32Col(shape%3D(), dflt%3D0, pos%3D1),            %22No2%22: Int32Col(shape%3D(), dflt%3D0, pos%3D2),            %22No3%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D3),            %22No4%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D4)%7D            byteorder :%3D 'little'            chunkshape :%3D (2621,)In%5B107%5D:ll%24path*Out%5B107%5D: -rw-r--r-- 1 root 100156256 28. Sep 15:18 /flash/data/tab.h5There is a more performant and Pythonic way to accomplish the same result, by theuse of NumPy structured arrays:In%5B108%5D:dty%3Dnp.dtype(%5B('Date','S26'),('No1','%3Ci4'),('No2','%3Ci4'),('No3','%3Cf8'),('No4','%3Cf8')%5D)sarray%3Dnp.zeros(len(ran_int),dtype%3Ddty)In%5B109%5D:sarrayOut%5B109%5D: array(%5B('', 0, 0, 0.0, 0.0), ('', 0, 0, 0.0, 0.0),          ('', 0, 0, 0.0, 0.0),                 ..., ('', 0, 0, 0.0, 0.0), ('', 0, 0, 0.0, 0.0),                 ('', 0, 0, 0.0, 0.0)%5D,                dtype%3D%5B('Date', 'S26'), ('No1', '%3Ci4'), ('No2', '%3Ci4'), ('No3',          '%3Cf8'), ('No4', '%3Cf8')%5D)In%5B110%5D:%25%25timesarray%5B'Date'%5D%3Ddt.datetime.now()sarray%5B'No1'%5D%3Dran_int%5B:,0%5Dsarray%5B'No2'%5D%3Dran_int%5B:,1%5Dsarray%5B'No3'%5D%3Dran_flo%5B:,0%5Dsarray%5B'No4'%5D%3Dran_flo%5B:,1%5DOut%5B110%5D: CPU times: user 113 ms, sys: 18 ms, total: 131 ms          Wall time: 131 msEquipped with the complete data set now stored in the structured array, the creation ofthe table boils down to the following line of code. Note that the row description is notneeded anymore%3B PyTables uses the NumPydtype instead:In%5B111%5D:%25%25timeh5.create_table('/','ints_floats_from_array',sarray,title%3D'Integers and Floats',expectedrows%3Drows,filters%3Dfilters)Out%5B111%5D: CPU times: user 38 ms, sys: 117 ms, total: 155 ms          Wall time: 154 ms          /ints_floats_from_array (Table(2000000,)) 'Integers and Floats'            description :%3D %7B","            %22Date%22: StringCol(itemsize%3D26, shape%3D(), dflt%3D'', pos%3D0),            %22No1%22: Int32Col(shape%3D(), dflt%3D0, pos%3D1),            %22No2%22: Int32Col(shape%3D(), dflt%3D0, pos%3D2),            %22No3%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D3),            %22No4%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D4)%7D            byteorder :%3D 'little'            chunkshape :%3D (2621,)Being an order of magnitude faster than the previous approach, this approach achievesthe same result and also needs less code:In%5B112%5D:h5Out%5B112%5D: File(filename%3D/flash/data/tab.h5, title%3Du'', mode%3D'w', root_uep%3D'/',          filters%3DFilters(complevel%3D0, shuffle%3DFalse, fletcher32%3DFalse,          least_significant_digit%3DNone))          / (RootGroup) u''          /ints_floats (Table(2000000,)) 'Integers and Floats'            description :%3D %7B            %22Date%22: StringCol(itemsize%3D26, shape%3D(), dflt%3D'', pos%3D0),            %22No1%22: Int32Col(shape%3D(), dflt%3D0, pos%3D1),            %22No2%22: Int32Col(shape%3D(), dflt%3D0, pos%3D2),            %22No3%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D3),            %22No4%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D4)%7D            byteorder :%3D 'little'            chunkshape :%3D (2621,)          /ints_floats_from_array (Table(2000000,)) 'Integers and Floats'            description :%3D %7B            %22Date%22: StringCol(itemsize%3D26, shape%3D(), dflt%3D'', pos%3D0),            %22No1%22: Int32Col(shape%3D(), dflt%3D0, pos%3D1),            %22No2%22: Int32Col(shape%3D(), dflt%3D0, pos%3D2),            %22No3%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D3),            %22No4%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D4)%7D            byteorder :%3D 'little'            chunkshape :%3D (2621,)We can now delete the duplicate table, since it is no longer needed:In%5B113%5D:h5.remove_node('/','ints_floats_from_array')The Table object behaves like typical Python and NumPy objects when it comes to slicing,for example:In%5B114%5D:tab%5B:3%5DOut%5B114%5D: array(%5B('2014-09-28 15:17:57.631234', 4342, 1672, -0.9293, 0.06343),                 ('2014-09-28 15:17:57.631368', 3839, 1563, -2.02808, 0.3964),                 ('2014-09-28 15:17:57.631383', 5100, 1326, 0.03401, 0.46742)%5D,                dtype%3D%5B('Date', 'S26'), ('No1', '%3Ci4'), ('No2', '%3Ci4'), ('No3',          '%3Cf8'), ('No4', '%3Cf8')%5D)Similarly, we can select single columns only:","In%5B115%5D:tab%5B:4%5D%5B'No4'%5DOut%5B115%5D: array(%5B 0.06343,  0.3964 ,  0.46742, -0.56959%5D)Even more convenient and important: we can apply NumPy universal functions to tablesor subsets of the table:In%5B116%5D:%25timenp.sum(tab%5B:%5D%5B'No3'%5D)Out%5B116%5D: CPU times: user 31 ms, sys: 58 ms, total: 89 ms          Wall time: 88.3 ms          -115.34513999999896In%5B117%5D:%25timenp.sum(np.sqrt(tab%5B:%5D%5B'No1'%5D))Out%5B117%5D: CPU times: user 53 ms, sys: 48 ms, total: 101 ms          Wall time: 101 ms          133360523.08794475When it comes to plotting, the Tableobject also behaves very similarly to an ndarrayobject (cf. Figure 7-5):In%5B118%5D:%25%25timeplt.hist(tab%5B:%5D%5B'No3'%5D,bins%3D30)plt.grid(True)printlen(tab%5B:%5D%5B'No3'%5D)Out%5B118%5D: 2000000          CPU times: user 396 ms, sys: 89 ms, total: 485 ms          Wall time: 485 msFigure 7-5. Histogram of dataAnd, of course, we have rather flexible tools to query data via typical SQL-like statements,as in the following example (the result of which is neatly illustrated in Figure 7-6%3B com%E2%80%90pare it with Figure 7-2, based on a pandas query):","In%5B119%5D:%25%25timeres%3Dnp.array(%5B(row%5B'No3'%5D,row%5B'No4'%5D)forrowintab.where('((No3 %3C -0.5) %7C (No3 %3E 0.5)) %5C                           %26 ((No4 %3C -1) %7C (No4 %3E 1))')%5D)%5B::100%5DOut%5B119%5D: CPU times: user 530 ms, sys: 52 ms, total: 582 ms          Wall time: 469 msIn%5B120%5D:plt.plot(res.T%5B0%5D,res.T%5B1%5D,'ro')plt.grid(True)Figure 7-6. Scatter plot of query result","BothpandasandPyTablesareabletoprocesscomplex,SQL-likequeriesandselections.Theyarebothoptimizedforspeedwhenitcomes to such operations.As the following examples show, working with data stored in PyTablesas a Tableobjectmakes you feel like you are working with NumPy and in-memory, both from a syntaxand a performance point of view:In%5B121%5D:%25%25timevalues%3Dtab.cols.No3%5B:%5Dprint%22Max %2518.3f%22%25values.max()print%22Ave %2518.3f%22%25values.mean()print%22Min %2518.3f%22%25values.min()print%22Std %2518.3f%22%25values.std()Out%5B121%5D: Max              5.152          Ave             -0.000          Min             -5.537          Std              1.000          CPU times: user 44 ms, sys: 39 ms, total: 83 ms          Wall time: 82.6 ms","In%5B122%5D:%25%25timeresults%3D%5B(row%5B'No1'%5D,row%5B'No2'%5D)forrowintab.where('((No1 %3E 9800) %7C (No1 %3C 200)) %5C                              %26 ((No2 %3E 4500) %26 (No2 %3C 5500))')%5Dforresinresults%5B:4%5D:printresOut%5B122%5D: (9987, 4965)          (9934, 5263)          (9960, 4729)          (130, 5023)          CPU times: user 167 ms, sys: 37 ms, total: 204 ms          Wall time: 118 msIn%5B123%5D:%25%25timeresults%3D%5B(row%5B'No1'%5D,row%5B'No2'%5D)forrowintab.where('(No1 %3D%3D 1234) %26 (No2 %3E 9776)')%5Dforresinresults:printresOut%5B123%5D: (1234, 9805)          (1234, 9785)          (1234, 9821)          CPU times: user 93 ms, sys: 40 ms, total: 133 ms          Wall time: 90.1 ms","A major advantage of working with PyTablesis the approach it takes to compression.It uses compression not only to save space on disk, but also to improve the performanceof I/O operations. How does this work%3F When I/O is the bottleneck and the CPU is ableto (de)compress data fast, the net effect of compression in terms of speed might bepositive. Since the following examples are based on the I/O of a state-of-the-art (at thetime of this writing) SSD, there is no speed advantage of compression to be observed.However, there is also almost no disadvantage of using compression:In%5B124%5D:filename%3Dpath%2B'tab.h5c'h5c%3Dtb.open_file(filename,'w')In%5B125%5D:filters%3Dtb.Filters(complevel%3D4,complib%3D'blosc')In%5B126%5D:tabc%3Dh5c.create_table('/','ints_floats',sarray,title%3D'Integers and Floats',expectedrows%3Drows,filters%3Dfilters)In%5B127%5D:%25%25timeres%3Dnp.array(%5B(row%5B'No3'%5D,row%5B'No4'%5D)forrowintabc.where('((No3 %3C -0.5) %7C (No3 %3E 0.5)) %5C                                 %26 ((No4 %3C -1) %7C (No4 %3E 1))')%5D)%5B::100%5DOut%5B127%5D: CPU times: user 670 ms, sys: 41 ms, total: 711 ms          Wall time: 602 ms","Generating the table with the original data and doing analytics on it is slightly slowercompared to the uncompressed table. What about reading the data into an ndarray%3FLet's check:In%5B128%5D:%25timearr_non%3Dtab.read()Out%5B128%5D: CPU times: user 13 ms, sys: 49 ms, total: 62 ms          Wall time: 61.3 msIn%5B129%5D:%25timearr_com%3Dtabc.read()Out%5B129%5D: CPU times: user 161 ms, sys: 33 ms, total: 194 ms          Wall time: 193 msThis indeed takes much longer than before. However, the compression ratio is about20%25, saving 80%25 of the space on disk. This may be of importance for backup routinesor when shuffling large data sets between servers or even data centers:In%5B130%5D:ll%24path*Out%5B130%5D: -rw-r--r-- 1 root 200313168 28. Sep 15:18 /flash/data/tab.h5          -rw-r--r-- 1 root  41335178 28. Sep 15:18 /flash/data/tab.h5cIn%5B131%5D:h5c.close()","We have already seen that NumPy has built-in fast writing and reading capabilities forndarray objects. PyTablesis also quite fast and efficient when it comes to storing andretrieving ndarray objects:In%5B132%5D:%25%25timearr_int%3Dh5.create_array('/','integers',ran_int)arr_flo%3Dh5.create_array('/','floats',ran_flo)Out%5B132%5D: CPU times: user 2 ms, sys: 33 ms, total: 35 ms          Wall time: 35 msWriting these objects directly to an HDF5 database is of course much faster than loopingover the objects and writing the data row-by-row to a Table object. A final inspectionof the database shows now three objects in it, the table and the two arrays:In%5B133%5D:h5Out%5B133%5D: File(filename%3D/flash/data/tab.h5, title%3Du'', mode%3D'w', root_uep%3D'/', f          ilters%3DFilters(complevel%3D0, shuffle%3DFalse, fletcher32%3DFalse, least_sig          nificant_digit%3DNone))          / (RootGroup) u''          /floats (Array(2000000, 2)) ''            atom :%3D Float64Atom(shape%3D(), dflt%3D0.0)            maindim :%3D 0            flavor :%3D 'numpy'            byteorder :%3D 'little'            chunkshape :%3D None","          /integers (Array(2000000, 2)) ''            atom :%3D Int64Atom(shape%3D(), dflt%3D0)            maindim :%3D 0            flavor :%3D 'numpy'            byteorder :%3D 'little'            chunkshape :%3D None          /ints_floats (Table(2000000,)) 'Integers and Floats'            description :%3D %7B            %22Date%22: StringCol(itemsize%3D26, shape%3D(), dflt%3D'', pos%3D0),            %22No1%22: Int32Col(shape%3D(), dflt%3D0, pos%3D1),            %22No2%22: Int32Col(shape%3D(), dflt%3D0, pos%3D2),            %22No3%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D3),            %22No4%22: Float64Col(shape%3D(), dflt%3D0.0, pos%3D4)%7D            byteorder :%3D 'little'            chunkshape :%3D (2621,)In%5B134%5D:ll%24path*Out%5B134%5D: -rw-r--r-- 1 root 200313168 28. Sep 15:18 /flash/data/tab.h5          -rw-r--r-- 1 root  41335178 28. Sep 15:18 /flash/data/tab.h5cIn%5B135%5D:h5.close()In%5B136%5D:!rm-f%24path*","TheHDF5database(file)formatisapowerfulalternativeto,forex%E2%80%90ample,relationaldatabaseswhenitcomestostructurednumericaland financial data. Both on a standalone basis when using PyTablesdirectlyandwhencombiningitwiththecapabilitiesofpandas,youcanexpecttogetalmostthemaximumI/Operformancethattheavailable hardware allows.","PyTablessupports out-of-memory operations, which makes it possible to implementarray-based computations that do not fit into the memory:In%5B137%5D:filename%3Dpath%2B'array.h5'h5%3Dtb.open_file(filename,'w')We create an EArray object that is extendable in the first dimension and has a fixedwidth of 1,000 in the second dimension:In%5B138%5D:n%3D1000ear%3Dh5.createEArray(h5.root,'ear',atom%3Dtb.Float64Atom(),shape%3D(0,n))Since it is extendable, such an object can be populated chunk-wise:","In%5B139%5D:%25%25timerand%3Dnp.random.standard_normal((n,n))foriinrange(750):ear.append(rand)ear.flush()Out%5B139%5D: CPU times: user 2.42 s, sys: 7.29 s, total: 9.71 s          Wall time: 20.6 sTo check how much data we have generated logically and physically, we can inspect themeta-information provided for the object as well as the disk space consumption:In%5B140%5D:earOut%5B140%5D: /ear (EArray(750000, 1000)) ''            atom :%3D Float64Atom(shape%3D(), dflt%3D0.0)            maindim :%3D 0            flavor :%3D 'numpy'            byteorder :%3D 'little'            chunkshape :%3D (8, 1000)In%5B141%5D:ear.size_on_diskOut%5B141%5D: 6000000000LThe EArrayobject is 6 GB large. For an out-of-memory computation, we need a targetEArray object in the database:In%5B142%5D:out%3Dh5.createEArray(h5.root,'out',atom%3Dtb.Float64Atom(),shape%3D(0,n))PyTables has a special module to cope with numerical expressions efficiently. It is calledExprand is based on the numerical expression library numexpr. This is what we wantto use to calculate the mathematical expression in Equation 7-1 on the whole EArrayobject that we generated before.Equation 7-1. Example mathematical expressiony%3D3sinx%2BxThe following code shows the capabilities for out-of-memory calculations in action:In%5B143%5D:expr%3Dtb.Expr('3 * sin(ear) %2B sqrt(abs(ear))')# the numerical expression as a string objectexpr.setOutput(out,append_mode%3DTrue)# target to store results is disk-based arrayIn%5B144%5D:%25timeexpr.eval()# evaluation of the numerical expression# and storage of results in disk-based array","Out%5B144%5D: CPU times: user 34.4 s, sys: 11.6 s, total: 45.9 s          Wall time: 1min 41s          /out (EArray(750000, 1000)) ''            atom :%3D Float64Atom(shape%3D(), dflt%3D0.0)            maindim :%3D 0            flavor :%3D 'numpy'            byteorder :%3D 'little'            chunkshape :%3D (8, 1000)In%5B145%5D:out%5B0,:10%5DOut%5B145%5D: array(%5B-0.95979563, -1.21530335,  0.02687751,  2.88229293, -0.05596624,                 -1.70266651, -0.58575264,  1.70317385,  3.54571202,  2.81602673          %5D)Given that the whole operation takes place out-of-memory, it can be considered quitefast, in particular as it is executed on standard hardware. Let us briefly compare this tothe in-memory performance of the numexpr module (see also Chapter 8):In%5B146%5D:%25timeimarray%3Dear.read()# read whole array into memoryOut%5B146%5D: CPU times: user 1.26 s, sys: 4.11 s, total: 5.37 s          Wall time: 5.39 sIn%5B147%5D:importnumexprasneexpr%3D'3 * sin(imarray) %2B sqrt(abs(imarray))'In%5B148%5D:ne.set_num_threads(16)%25timene.evaluate(expr)%5B0,:10%5DOut%5B148%5D: CPU times: user 24.2 s, sys: 29.1 s, total: 53.3 s          Wall time: 3.81 s          array(%5B-0.95979563, -1.21530335,  0.02687751,  2.88229293, -0.05596624,                 -1.70266651, -0.58575264,  1.70317385,  3.54571202,  2.81602673          %5D)In%5B149%5D:h5.close()In%5B150%5D:!rm-f%24path*","SQL-based (i.e., relational) databases have advantages when it comes to complex datastructures that exhibit lots of relations between single objects/tables. This might justifyin some circumstances their performance disadvantage over pure NumPyndarray-basedor pandasDataFrame-based approaches.However, many application areas in finance or science in general, can succeed with amainly array-based data modeling approach. In these cases, huge performance im%E2%80%90provements can be realized by making use of native NumPyI/O capabilities, a combina%E2%80%90","tion of NumPy and PyTables capabilities, or of the pandas approach via HDF5-basedstores.While a recent trend has been to use cloud-based solutions-where the cloud is madeup of a large number of computing nodes based on commodity hardware-one shouldcarefully consider, especially in a financial context, which hardware architecture bestserves the analytics requirements. A recent study by Microsoft sheds some light on thistopic:We claim that a single %E2%80%9Cscale-up%E2%80%9D server can process each of these jobs and do as well orbetter than a cluster in terms of performance, cost, power, and server density.- Appuswamy et al. (2013)Companies, research institutions, and others involved in data analytics should thereforeanalyze first what specific tasks have to be accomplished in general and then decide onthe hardware/software architecture, in terms of:Scaling outUsing a cluster with many commodity nodes with standard CPUs and relativelylow memoryScaling upUsing one or a few powerful servers with many-core CPUs, possibly a GPU, andlarge amounts of memoryOur out-of-memory analytics example in this chapter underpins the observation. Theout-of-memory calculation of the numerical expression with PyTables takes roughly1.5 minutes on standard hardware. The same task executed in-memory (using thenumexpr library) takes about 4 seconds, while reading the whole data set from disk takesjust over 5 seconds. This value is from an eight-core server with enough memory (inthis particular case, 64 GB of RAM) and an SSD drive. Therefore, scaling up hardwareand applying different implementation approaches might significantly influence per%E2%80%90formance. More on this in the next chapter.","The paper cited at the beginning of the chapter as well as in the %E2%80%9CConclusions%E2%80%9D sectionis a good read, and a good starting point to think about hardware architecture for fi%E2%80%90nancial analytics:%E2%80%A2Appuswamy, Raja et al. (2013): %E2%80%9CNobody Ever Got Fired for Buying a Cluster.%E2%80%9D Mi%E2%80%90crosoft Research, Cambridge, England, http://research.microsoft.com/apps/pubs/default.aspx%3Fid%3D179615.As usual, the Web provides many valuable resources with regard to the topics coveredin this chapter:","%E2%80%A2For serialization of Pythonobjects with pickle, refer to the documentation: http://docs.python.org/2/library/pickle.html.%E2%80%A2An overview of the I/O capabilities of NumPy is provided on the SciPywebsite:http://docs.scipy.org/doc/numpy/reference/routines.io.html.%E2%80%A2For I/O with pandassee the respective section in the online documentation: http://pandas.pydata.org/pandas-docs/stable/io.html.%E2%80%A2The PyTables home page provides both tutorials and detailed documentation:http://www.pytables.org.","Don't lower your expectations to meet your performance.Raise your level of performance to meet your expectations.- Ralph MarstonWhen it comes to performance-critical applications two things should always bechecked: are we using the right implementation paradigm and are we using the rightperformance libraries%3F A number of performance libraries can be used to speed up theexecution of Python code. Among others, you will find the following libraries useful,all of which are presented in this chapter (although in a different order):%E2%80%A2Cython, for merging Python with  C  paradigms for static compilation%E2%80%A2IPython.parallel, for the parallel execution of code/functions locally or over acluster%E2%80%A2numexpr, for fast numerical operations%E2%80%A2multiprocessing, Python's built-in module for (local) parallel processing%E2%80%A2Numba, for dynamically compiling Python code for the CPU%E2%80%A2NumbaPro, for dynamically compiling Python code for multicore CPUs and GPUsThroughout this chapter, we compare the performance of different implementations ofthe same algorithms. To make the comparison a bit easier, we define a conveniencefunction that allows us to systematically compare the performance of different functionsexecuted on the same or different data sets:In%5B1%5D:defperf_comp_data(func_list,data_list,rep%3D3,number%3D1):''' Function to compare the performance of different functions.            Parameters            %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D            func_list : list","                list with function names as strings            data_list : list                list with data set names as strings            rep : int                number of repetitions of the whole comparison            number : int                number of executions for every function            '''fromtimeitimportrepeatres_list%3D%7B%7Dfornameinenumerate(func_list):stmt%3Dname%5B1%5D%2B'('%2Bdata_list%5Bname%5B0%5D%5D%2B')'setup%3D%22from __main__ import %22%2Bname%5B1%5D%2B', ' %5C%2Bdata_list%5Bname%5B0%5D%5Dresults%3Drepeat(stmt%3Dstmt,setup%3Dsetup,repeat%3Drep,number%3Dnumber)res_list%5Bname%5B1%5D%5D%3Dsum(results)/repres_sort%3Dsorted(res_list.iteritems(),key%3Dlambda(k,v):(v,k))foriteminres_sort:rel%3Ditem%5B1%5D/res_sort%5B0%5D%5B1%5Dprint'function: '%2Bitem%5B0%5D%2B %5C', av. time sec: %259.5f, '%25item%5B1%5D %5C%2B'relative: %256.1f'%25rel","In finance, like in other scientific and data-intensive disciplines, numerical computa%E2%80%90tions on large data sets can be quite time-consuming. As an example, we want to evaluatea somewhat complex mathematical expression on an array with 500,000 numbers. Wechoose the expression in Equation 8-1, which leads to some computational burden percalculation. Apart from that, it does not have any specific meaning.Equation 8-1. Example mathematical expressiony%3Dcosx%2Bsin2%2B3xEquation 8-1 is easily translated into a Python function:In%5B2%5D:frommathimport*deff(x):returnabs(cos(x))**0.5%2Bsin(2%2B3*x)Using the range function we can generate efficiently a listobject with 500,000 numbersthat we can work with:In%5B3%5D:I%3D500000a_py%3Drange(I)","As the first implementation, consider function f1, which loops over the whole data setand appends the single results of the function evaluations to a results list object:In%5B4%5D:deff1(a):res%3D%5B%5Dforxina:res.append(f(x))returnresThis is not the only way to implement this. One can also use different Pythonparadigms,like iterators or the eval function, to get functions of the form f2 and f3:In%5B5%5D:deff2(a):return%5Bf(x)forxina%5DIn%5B6%5D:deff3(a):ex%3D'abs(cos(x)) ** 0.5 %2B sin(2 %2B 3 * x)'return%5Beval(ex)forxina%5DOf course, the same algorithm can be implemented by the use of NumPyvectorizationtechniques. In this case, the array of data is an ndarray object instead of a list object.The function implementation f4 shows no loops whatsoever%3B all looping takes place onthe NumPy level and not on the Python level:In%5B7%5D:importnumpyasnpIn%5B8%5D:a_np%3Dnp.arange(I)In%5B9%5D:deff4(a):return(np.abs(np.cos(a))**0.5%2Bnp.sin(2%2B3*a))Then, we can use a specialized library called numexprto evaluate the numerical expres%E2%80%90sion. This library has built-in support for multithreaded execution. Therefore, to com%E2%80%90pare the performance of the single with the multithreaded approach, we define twodifferent functions, f5 (single thread) and f6 (multiple threads):In%5B10%5D:importnumexprasneIn%5B11%5D:deff5(a):ex%3D'abs(cos(a)) ** 0.5 %2B sin(2 %2B 3 * a)'ne.set_num_threads(1)returnne.evaluate(ex)In%5B12%5D:deff6(a):ex%3D'abs(cos(a)) ** 0.5 %2B sin(2 %2B 3 * a)'ne.set_num_threads(16)returnne.evaluate(ex)In total, the same task-i.e., the evaluation of the numerical expression in Equation 8-1on an array of size 500,000-is implemented in six different ways:","%E2%80%A2Standard Python function with explicit looping%E2%80%A2Iterator approach with implicit looping%E2%80%A2Iterator approach with implicit looping and using eval%E2%80%A2NumPy vectorized implementation%E2%80%A2Single-threaded implementation using numexpr%E2%80%A2Multithreaded implementation using numexprFirst, let us check whether the implementations deliver the same results. We use theIPython cell magic command %25%25time to record the total execution time:In%5B13%5D:%25%25timer1%3Df1(a_py)r2%3Df2(a_py)r3%3Df3(a_py)r4%3Df4(a_np)r5%3Df5(a_np)r6%3Df6(a_np)Out%5B13%5D: CPU times: user 16 s, sys: 125 ms, total: 16.1 s         Wall time: 16 sThe NumPy function allcloseallows for easy checking of whether two ndarray(-like)objects contain the same data:In%5B14%5D:np.allclose(r1,r2)Out%5B14%5D: TrueIn%5B15%5D:np.allclose(r1,r3)Out%5B15%5D: TrueIn%5B16%5D:np.allclose(r1,r4)Out%5B16%5D: TrueIn%5B17%5D:np.allclose(r1,r5)Out%5B17%5D: TrueIn%5B18%5D:np.allclose(r1,r6)Out%5B18%5D: TrueThis obviously is the case. The more interesting question, of course, is how the differentimplementations compare with respect to execution speed. To this end, we use theperf_comp_data function and provide all the function and data set names to it:In%5B19%5D:func_list%3D%5B'f1','f2','f3','f4','f5','f6'%5Ddata_list%3D%5B'a_py','a_py','a_py','a_np','a_np','a_np'%5DWe now have everything together to initiate the competition:","In%5B20%5D:perf_comp_data(func_list,data_list)Out%5B20%5D: function: f6, av. time sec:   0.00583, relative:    1.0         function: f5, av. time sec:   0.02711, relative:    4.6         function: f4, av. time sec:   0.06331, relative:   10.9         function: f2, av. time sec:   0.46864, relative:   80.3         function: f1, av. time sec:   0.59660, relative:  102.3         function: f3, av. time sec:  15.15156, relative: 2597.2There is a clear winner: the multithreaded numexpr implementation f6. Its speed ad%E2%80%90vantage, of course, depends on the number of cores available. The vectorized NumPyversion f4 is slower than f5. The pure Pythonimplementations f1 and f2are more than80 times slower than the winner. f3 is the slowest version, since the use of the evalfunction for such a large number of evaluations generates a huge overhead. In the caseof numexpr, the string-based expression is evaluated once and then compiled for lateruse%3B with the Pythoneval function this evaluation takes place 500,000 times.","NumPy allows the specification of a so-called dtypeper ndarrayobject: for example,np.int32 or f8. NumPy also allows us to choose from two different memory layoutswheninitializing an ndarray object. Depending on the structure of the object, one layout canhave advantages compared to the other. This is illustrated in the following:In%5B21%5D:importnumpyasnpIn%5B22%5D:np.zeros((3,3),dtype%3Dnp.float64,order%3D'C')Out%5B22%5D: array(%5B%5B 0.,  0.,  0.%5D,                %5B 0.,  0.,  0.%5D,                %5B 0.,  0.,  0.%5D%5D)The way you initialize a NumPyndarray object can have a significant influence on theperformance of operations on these arrays (given a certain size of array). In summary,the initialization of an ndarray object (e.g., via np.zeros or np.array) takes as input:shapeEither an int, a sequence of ints, or a reference to another numpy.ndarraydtype (optional)A numpy.dtype-these are NumPy-specific basic data types for numpy.ndarray ob%E2%80%90jectsorder (optional)The order in which to store elements in memory:  C  for  C -like (i.e., row-wise) or Ffor Fortran-like (i.e., column-wise)Consider the  C -like (i.e., row-wise), storage:","In%5B23%5D:c%3Dnp.array(%5B%5B1.,1.,1.%5D,%5B2.,2.,2.%5D,%5B3.,3.,3.%5D%5D,order%3D'C')In this case, the 1s, the 2s, and the 3s are stored next to each other. By contrast, considerthe Fortran-like (i.e., column-wise) storage:In%5B24%5D:f%3Dnp.array(%5B%5B1.,1.,1.%5D,%5B2.,2.,2.%5D,%5B3.,3.,3.%5D%5D,order%3D'F')Now, the data is stored in such a way that 1, 2, and 3 are next to each other in eachcolumn. Let's see whether the memory layout makes a difference in some way when thearray is large:In%5B25%5D:x%3Dnp.random.standard_normal((3,1500000)) C %3Dnp.array(x,order%3D'C')F%3Dnp.array(x,order%3D'F')x%3D0.0Now let's implement some standard operations on the  C -like layout array. First, calcu%E2%80%90lating sums:In%5B26%5D:%25timeit C .sum(axis%3D0)Out%5B26%5D: 100 loops, best of 3: 11.3 ms per loopIn%5B27%5D:%25timeit C .sum(axis%3D1)Out%5B27%5D: 100 loops, best of 3: 5.84 ms per loopCalculating sums over the first axis is roughly two times slower than over the secondaxis. One gets similar results for calculating standard deviations:In%5B28%5D:%25timeit C .std(axis%3D0)Out%5B28%5D: 10 loops, best of 3: 70.6 ms per loopIn%5B29%5D:%25timeit C .std(axis%3D1)Out%5B29%5D: 10 loops, best of 3: 32.6 ms per loopFor comparison, consider the Fortran-like layout. Sums first:In%5B30%5D:%25timeitF.sum(axis%3D0)Out%5B30%5D: 10 loops, best of 3: 29.2 ms per loopIn%5B31%5D:%25timeitF.sum(axis%3D1)Out%5B31%5D: 10 loops, best of 3: 37 ms per loopAlthough absolutely slower compared to the other layout, there is hardly a relativedifference for the two axes. Now, standard deviations:In%5B32%5D:%25timeitF.std(axis%3D0)Out%5B32%5D: 10 loops, best of 3: 107 ms per loop","In%5B33%5D:%25timeitF.std(axis%3D1)Out%5B33%5D: 10 loops, best of 3: 98.8 ms per loopAgain, this layout option leads to worse performance compared to the  C -like layout.There is a small difference between the two axes, but again it is not as pronounced aswith the other layout. The results indicate that in general the  C -like option will performbetter-which is also the reason why NumPyndarray objects default to this memorylayout if not otherwise specified:In%5B34%5D: C %3D0.0%3BF%3D0.0","Nowadays, even the most compact notebooks have mainboards with processors thathave multiple cores. Moreover, modern cloud-based computing offerings, like Amazon'sEC2 or Microsoft's Azure, allow for highly scalable, parallel architectures at rather low,variable costs. This brings large-scale computing to the small business, the researcher,and even the ambitious amateur. However, to harness the power of such offerings, ap%E2%80%90propriate tools are necessary. One such tool is the IPython.parallel library.","A financial algorithm that leads to a high computational burden is the Monte Carlovaluation of options. As a specific example, we pick the Monte Carlo estimator for aEuropean call option value in the Black-Scholes-Merton setup (see also Chapter 3forthe same example). In this setup, the underlying of the option to be valued follows thestochastic differential equation (SDE), as in Equation 8-2. Stis the value of the under%E2%80%90lying at time t%3B r is the constant, riskless short rate%3B %ED%9C%8E is the constant instantaneousvolatility%3B and Zt is a Brownian motion.Equation 8-2. Black-Scholes-Merton SDEdSt%3DrStdt%2B%CF%83StdZtThe Monte Carlo estimator for a European call option is given by Equation 8-3, whereST(i) is the ith simulated value of the underlying at maturity T.Equation 8-3. Monte Carlo estimator for European call option C 0%3De%E2%88%92rT1I%E2%88%91ImaxSTi%E2%88%92K,0","A function implementing the Monte Carlo valuation for the Black-Scholes-Merton set-up could look like the following, if we only allow the strike of the European call optionto vary:In%5B35%5D:defbsm_mcs_valuation(strike):''' Dynamic Black-Scholes-Merton Monte Carlo estimator             for European calls.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             strike : float                 strike price of the option             Results             %3D%3D%3D%3D%3D%3D%3D             value : float                 estimate for present value of call option             '''importnumpyasnpS0%3D100.%3BT%3D1.0%3Br%3D0.05%3Bvola%3D0.2M%3D50%3BI%3D20000dt%3DT/Mrand%3Dnp.random.standard_normal((M%2B1,I))S%3Dnp.zeros((M%2B1,I))%3BS%5B0%5D%3DS0fortinrange(1,M%2B1):S%5Bt%5D%3DS%5Bt-1%5D*np.exp((r-0.5*vola**2)*dt%2Bvola*np.sqrt(dt)*rand%5Bt%5D)value%3D(np.exp(-r*T)*np.sum(np.maximum(S%5B-1%5D-strike,0))/I)returnvalue","As the benchmark case we take the valuation of 100 options with different strike prices.The function seq_value calculates the Monte Carlo estimators and returns listobjectscontaining strikes and valuation results:In%5B36%5D:defseq_value(n):''' Sequential option valuation.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             n : int                 number of option valuations/strikes             '''strikes%3Dnp.linspace(80,120,n)option_values%3D%5B%5Dforstrikeinstrikes:option_values.append(bsm_mcs_valuation(strike))returnstrikes,option_values","In%5B37%5D:n%3D100# number of options to be valued%25timestrikes,option_values_seq%3Dseq_value(n)Out%5B37%5D: CPU times: user 11.7 s, sys: 1e%2B03 %C2%B5s, total: 11.7 sWall time: 11.7 sThe productivity is roughly 8.5 options per second. Figure 8-1shows the valuationresults:In%5B38%5D:importmatplotlib.pyplotasplt%25matplotlibinlineplt.figure(figsize%3D(8,4))plt.plot(strikes,option_values_seq,'b')plt.plot(strikes,option_values_seq,'r.')plt.grid(True)plt.xlabel('strikes')plt.ylabel('European call option values')Figure 8-1. European call option values by Monte Carlo simulation","For the parallel calculation of the 100 option values, we use IPython.parallel and alocal %E2%80%9Ccluster.%E2%80%9D A local cluster is most easily started via the Clusters tab in the IPythonNotebook dashboard. The number of threads to be used of course depends on the ma%E2%80%90chine and the processor you are running your code on. Figure 8-2shows the IPythonpage for starting a cluster.","Figure 8-2. Screenshot of IPython cluster pageIPython.parallelneeds the information on which cluster to use for the parallel exe%E2%80%90cution of code. In this case, the cluster profile is stored in the %E2%80%9Cdefault%E2%80%9D profile. In ad%E2%80%90dition, we need to generate a view on the cluster:In%5B39%5D:fromIPython.parallelimportClientc%3DClient(profile%3D%22default%22)view%3Dc.load_balanced_view()The function implementing the parallel valuation of the options looks rather similar tothe sequential implementation:In%5B40%5D:defpar_value(n):''' Parallel option valuation.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             n : int                 number of option valuations/strikes             '''strikes%3Dnp.linspace(80,120,n)option_values%3D%5B%5Dforstrikeinstrikes:value%3Dview.apply_async(bsm_mcs_valuation,strike)option_values.append(value)","c.wait(option_values)returnstrikes,option_valuesThere are two major differences to note. The first is that the valuation function is appliedasynchronously via view.apply_sync to our cluster view, which in effect initiates theparallel valuation of all options at once. Of course, not all options can be valued inparallel because there are (generally) not enough cores/threads available. Therefore, wehave to wait until the queue is completely finished%3B this is accomplished by the waitmethod of the Client object c. When all results are available, the function returns, asbefore, listobjects containing the strike prices and the valuation results, respectively.Execution of the parallel valuation function yields a productivity that ideally scales lin%E2%80%90early with the number of cores (threads) available. For example, having eightcores(threads) available reduces the execution time to maximally one-eighth of the timeneeded for the sequential calculation:In%5B41%5D:%25timestrikes,option_values_obj%3Dpar_value(n)Out%5B41%5D: CPU times: user 415 ms, sys: 30 ms, total: 445 ms         Wall time: 1.88 sThe parallel execution does not return option values directly%3B it rather returns morecomplex result objects:In%5B42%5D:option_values_obj%5B0%5D.metadataOut%5B42%5D: %7B'after': %5B%5D,          'completed': datetime.datetime(2014, 9, 28, 16, 6, 54, 93979),          'data': %7B%7D,          'engine_id': 5,          'engine_uuid': u'6b64aebb-39d5-49aa-9466-e6ab37d3b2c9',          'follow': %5B%5D,          'msg_id': u'c7a44c22-b4bd-46d7-ba5e-34690f178fa9',          'outputs': %5B%5D,          'outputs_ready': True,          'pyerr': None,          'pyin': None,          'pyout': None,          'received': datetime.datetime(2014, 9, 28, 16, 6, 54, 97195),          'started': datetime.datetime(2014, 9, 28, 16, 6, 53, 921633),          'status': u'ok',          'stderr': '',          'stdout': '',          'submitted': datetime.datetime(2014, 9, 28, 16, 6, 53, 917290)%7DThe valuation result itself is stored in the result attribute of the object:In%5B43%5D:option_values_obj%5B0%5D.resultOut%5B43%5D: 24.436651486350289To arrive at a results list as with the sequential calculation, we need to read the singleresults out from the returned objects:","In%5B44%5D:option_values_par%3D%5B%5Dforresinoption_values_obj:option_values_par.append(res.result)This could have been done, of course, in the parallel valuation loop directly. Figure 8-3compares the valuation results of the sequential calculation with those of the parallelcalculation. Differences are due to numerical issues concerning the Monte Carlovaluation:In%5B45%5D:plt.figure(figsize%3D(8,4))plt.plot(strikes,option_values_seq,'b',label%3D'Sequential')plt.plot(strikes,option_values_par,'r.',label%3D'Parallel')plt.grid(True)%3Bplt.legend(loc%3D0)plt.xlabel('strikes')plt.ylabel('European call option values')Figure 8-3. Comparison of European call option values","With the help of the perf_comp_func function, we can compare the performance a bitmore rigorously:In%5B46%5D:n%3D50# number of option valuationsfunc_list%3D%5B'seq_value','par_value'%5Ddata_list%3D2*%5B'n'%5DIn%5B47%5D:perf_comp_data(func_list,data_list)Out%5B47%5D: function: par_value, av. time sec:   0.90832, relative:    1.0         function: seq_value, av. time sec:   5.75137, relative:    6.3The results clearly demonstrate that using IPython.parallel for parallel execution offunctions can lead to an almost linear scaling of the performance with the number ofcores available.","The advantage of IPython.parallel is that it scales over small- and medium-sizedclusters (e.g., with 256 nodes). Sometimes it is, however, helpful to parallelize codeexecution locally. This is where the %E2%80%9Cstandard%E2%80%9D multiprocessingmodule of Pythonmight prove beneficial:In%5B48%5D:importmultiprocessingasmpConsider the following function to simulate a geometric Brownian motion:In%5B49%5D:importmathdefsimulate_geometric_brownian_motion(p):M,I%3Dp# time steps, pathsS0%3D100%3Br%3D0.05%3Bsigma%3D0.2%3BT%3D1.0# model parametersdt%3DT/Mpaths%3Dnp.zeros((M%2B1,I))paths%5B0%5D%3DS0fortinrange(1,M%2B1):paths%5Bt%5D%3Dpaths%5Bt-1%5D*np.exp((r-0.5*sigma**2)*dt%2Bsigma*math.sqrt(dt)*np.random.standard_normal(I))returnpathsThis function returns simulated paths given the parameterization for M and I:In%5B50%5D:paths%3Dsimulate_geometric_brownian_motion((5,2))pathsOut%5B50%5D: array(%5B%5B 100.        ,  100.        %5D,                %5B  93.65851581,   98.93916652%5D,                %5B  94.70157252,   93.44208625%5D,                %5B  96.73499004,   97.88294562%5D,                %5B 110.64677908,   96.04515015%5D,                %5B 124.09826521,  101.86087283%5D%5D)Let us implement a test series on a server with eight cores and the following parametervalues. In particular, we want to do 100 simulations:In%5B51%5D:I%3D10000# number of pathsM%3D100# number of time stepst%3D100# number of tasks/simulationsIn%5B52%5D:# running on server with 8 cores/16 threadsfromtimeimporttimetimes%3D%5B%5Dforwinrange(1,17):t0%3Dtime()pool%3Dmp.Pool(processes%3Dw)# the pool of workersresult%3Dpool.map(simulate_geometric_brownian_motion,t*%5B(M,I),%5D)","# the mapping of the function to the list of parameter tuplestimes.append(time()-t0)We again come to the conclusion that performance scales with the number of coresavailable. Hyperthreading, however, does not add much (or is even worse) in this case,as Figure 8-4 illustrates:In%5B53%5D:plt.plot(range(1,17),times)plt.plot(range(1,17),times,'ro')plt.grid(True)plt.xlabel('number of processes')plt.ylabel('time in seconds')plt.title('%25d Monte Carlo simulations'%25t)Figure 8-4. Execution speed depending on the number of threads used (eight-coremachine)","Many problems in finance allow for the application of simple paral%E2%80%90lelizationtechniques,forexample,whennodataissharedbetweeninstancesofanalgorithm.ThemultiprocessingmoduleofPythonallows us to efficiently harness the power of modern hardware archi%E2%80%90tectureswithoutingeneralchangingthebasicalgorithmsand/orPython functions to be parallelized.","1.Formerly, LLVMwas meant to be an acronym for Low Level Virtual Machine%3B now %E2%80%9Cit is the full name ofthe project.%E2%80%9D","Numbais an open source, NumPy-aware optimizing compiler for Pythoncode. It uses theLLVM compiler infrastructure1 to compile Python byte code to machine code especiallyfor use in the NumPy runtime and SciPy modules.","Let us start with a problem that typically leads to performance issues in Python: algo%E2%80%90rithms with nested loops. A sandbox variant can illustrate the problem:In%5B54%5D:frommathimportcos,logdeff_py(I,J):res%3D0foriinrange(I):forjinrange(J):res%2B%3Dint(cos(log(1)))returnresIn a somewhat compute-intensive way, this function returns the total number of loopsgiven the input parameters I and J. Setting both equal to 5,000 leads to 25,000,000 loops:In%5B55%5D:I,J%3D5000,5000%25timef_py(I,J)Out%5B55%5D: CPU times: user 17.4 s, sys: 2.3 s, total: 19.7 s         Wall time: 15.2 s         25000000In principle, this can be vectorized with the help of NumPyndarray objects:In%5B56%5D:deff_np(I,J):a%3Dnp.ones((I,J),dtype%3Dnp.float64)returnint(np.sum(np.cos(np.log(a)))),aIn%5B57%5D:%25timeres,a%3Df_np(I,J)Out%5B57%5D: CPU times: user 1.41 s, sys: 285 ms, total: 1.69 s         Wall time: 1.65 sThis is much faster, roughly by a factor of 8%E2%80%9310 times, but not really memory-efficient.The ndarray object consumes 200 MB of memory:In%5B58%5D:a.nbytesOut%5B58%5D: 200000000","I and Jcan easily be chosen to make the NumPyapproach infeasible given a certain sizeof RAM. Numba provides an attractive alternative to tackle the performance issue of suchloop structures while preserving the memory efficiency of the pure Python approach:In%5B59%5D:importnumbaasnbWith Numba you only need to apply the jit function to the pure Pythonfunction togenerate a Python-callable, compiled version of the function:In%5B60%5D:f_nb%3Dnb.jit(f_py)As promised, this new function can be called directly from within the Pythoninterpreter,realizing a significant speedup compared to the NumPy vectorized version:In%5B61%5D:%25timef_nb(I,J)Out%5B61%5D: CPU times: user 143 ms, sys: 12 ms, total: 155 ms         Wall time: 139 ms         25000000LAgain, let us compare the performance of the different alternatives a bit moresystematically:In%5B62%5D:func_list%3D%5B'f_py','f_np','f_nb'%5Ddata_list%3D3*%5B'I, J'%5DIn%5B63%5D:perf_comp_data(func_list,data_list)Out%5B63%5D: function: f_nb, av. time sec:   0.02022, relative:    1.0         function: f_np, av. time sec:   1.67494, relative:   82.8         function: f_py, av. time sec:  15.82375, relative:  782.4The Numbaversion of the nested loop implementation is by far the fastest%3B much fastereven than the NumPy vectorized version. The pure Python version is much slower thanthe other two versions.","Manyapproachesforperformanceimprovements(ofnumericalal%E2%80%90gorithms)involveconsiderableeffort.WithPythonandNumbayouhave an approach available that involves only the smallest effort pos%E2%80%90sible-ingeneral,importingthelibraryandasingleadditionallineofcode.Itdoesnotworkforallkindsofalgorithms,butitisoftenwortha (quick) try and sometimes indeed yields a quick win.","The previous section uses Monte Carlo simulation to value European call options, usinga parallel computing approach. Another popular numerical method to value options isthe binomial option pricing model pioneered by Cox, Ross, and Rubinstein (1979). In","this model, as in the Black-Scholes-Merton setup, there is a risky asset, an index or stock,and a riskless asset, a bond. As with Monte Carlo, the relevant time interval from todayuntil the maturity of the option is divided into generally equidistant subintervals, %ED%9B%A5t.Given an index level at time s of Ss, the index level at t%3D s%2B %ED%9B%A5t is given by St %3D Ss%C2%B7 m,where m is chosen randomly from from %7Bu,d%7D with 0%3Cd%3Cer%CE%94t%3Cu%3De%CF%83%CE%94t as well as u%3D1d.ris the constant, riskless short rate. The risk-neutral probability for an up-movementis given as q%3Der%CE%94t%E2%88%92du%E2%88%92d.Consider that a parameterization for the model is given as follows:In%5B64%5D:# model %26 option parametersS0%3D100.# initial index levelT%3D1.# call option maturityr%3D0.05# constant short ratevola%3D0.20# constant volatility factor of diffusion# time parametersM%3D1000# time stepsdt%3DT/M# length of time intervaldf%3Dexp(-r*dt)# discount factor per time interval# binomial parametersu%3Dexp(vola*sqrt(dt))# up-movementd%3D1/u# down-movementq%3D(exp(r*dt)-d)/(u-d)# martingale probabilityAn implementation of the binomial algorithm for European options consists mainly ofthese parts:Index level simulationSimulate step by step the index levels.Inner value calculationCalculate the inner values at maturity and/or at every time step.Risk-neutral discountingDiscount the (expected) inner values at maturity step by step to arrive at the presentvalue.In Pythonthis might take on the form seen in the function binomial_py. This functionuses NumPyndarrayobjects as the basic data structure and implements three differentnested loops to accomplish the three steps just sketched:In%5B65%5D:importnumpyasnpdefbinomial_py(strike):''' Binomial option pricing via looping.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             strike : float","                 strike price of the European call option             '''# LOOP 1 - Index LevelsS%3Dnp.zeros((M%2B1,M%2B1),dtype%3Dnp.float64)# index level arrayS%5B0,0%5D%3DS0z1%3D0forjinxrange(1,M%2B1,1):z1%3Dz1%2B1foriinxrange(z1%2B1):S%5Bi,j%5D%3DS%5B0,0%5D*(u**j)*(d**(i*2))# LOOP 2 - Inner Valuesiv%3Dnp.zeros((M%2B1,M%2B1),dtype%3Dnp.float64)# inner value arrayz2%3D0forjinxrange(0,M%2B1,1):foriinxrange(z2%2B1):iv%5Bi,j%5D%3Dmax(S%5Bi,j%5D-strike,0)z2%3Dz2%2B1# LOOP 3 - Valuationpv%3Dnp.zeros((M%2B1,M%2B1),dtype%3Dnp.float64)# present value arraypv%5B:,M%5D%3Div%5B:,M%5D# initialize last time pointz3%3DM%2B1forjinxrange(M-1,-1,-1):z3%3Dz3-1foriinxrange(z3):pv%5Bi,j%5D%3D(q*pv%5Bi,j%2B1%5D%2B(1-q)*pv%5Bi%2B1,j%2B1%5D)*dfreturnpv%5B0,0%5DThis function returns the present value of a European call option with parameters asspecified before:In%5B66%5D:%25timeround(binomial_py(100),3)Out%5B66%5D: CPU times: user 4.18 s, sys: 312 ms, total: 4.49 s         Wall time: 3.64 s         10.449We can compare this result with the estimated value the Monte Carlo functionbsm_mcs_valuation returns:In%5B67%5D:%25timeround(bsm_mcs_valuation(100),3)Out%5B67%5D: CPU times: user 133 ms, sys: 0 ns, total: 133 ms         Wall time: 126 ms         10.318","The values are similar. They are only %E2%80%9Csimilar%E2%80%9D and not the same since the Monte Carlovaluation as implemented with bsm_mcs_valuationis not too precise, in that differentsets of random numbers will lead to (slightly) different estimates. 20,000 paths per sim%E2%80%90ulation can also be considered a bit too low for robust Monte Carlo estimates (leading,however, to high valuation speeds). By contrast, the binomial option pricing model with1,000 time steps is rather precise but also takes much longer in this case.Again, we can try NumPyvectorization techniques to come up with equally precise butfaster results from the binomial approach. The binomial_npfunction might seem a bitcryptic at first sight%3B however, when you step through the individual construction stepsand inspect the results, it becomes clear what happens behind the (NumPy) scenes:In%5B68%5D:defbinomial_np(strike):''' Binomial option pricing with NumPy.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             strike : float                 strike price of the European call option             '''# Index Levels with NumPymu%3Dnp.arange(M%2B1)mu%3Dnp.resize(mu,(M%2B1,M%2B1))md%3Dnp.transpose(mu)mu%3Du**(mu-md)md%3Dd**mdS%3DS0*mu*md# Valuation Looppv%3Dnp.maximum(S-strike,0)z%3D0fortinrange(M-1,-1,-1):# backward iterationpv%5B0:M-z,t%5D%3D(q*pv%5B0:M-z,t%2B1%5D%2B(1%E2%80%93q)*pv%5B1:M-z%2B1,t%2B1%5D)*dfz%2B%3D1returnpv%5B0,0%5DLet us briefly take a look behind the scenes. For simplicity and readability, consider onlyM%3D4 time steps. The first step:In%5B69%5D:M%3D4# four time steps onlymu%3Dnp.arange(M%2B1)muOut%5B69%5D: array(%5B0, 1, 2, 3, 4%5D)The second step of the construction:In%5B70%5D:mu%3Dnp.resize(mu,(M%2B1,M%2B1))mu","Out%5B70%5D: array(%5B%5B0, 1, 2, 3, 4%5D,                %5B0, 1, 2, 3, 4%5D,                %5B0, 1, 2, 3, 4%5D,                %5B0, 1, 2, 3, 4%5D,                %5B0, 1, 2, 3, 4%5D%5D)The third one:In%5B71%5D:md%3Dnp.transpose(mu)mdOut%5B71%5D: array(%5B%5B0, 0, 0, 0, 0%5D,                %5B1, 1, 1, 1, 1%5D,                %5B2, 2, 2, 2, 2%5D,                %5B3, 3, 3, 3, 3%5D,                %5B4, 4, 4, 4, 4%5D%5D)The fourth and fifth steps:In%5B72%5D:mu%3Du**(mu-md)mu.round(3)Out%5B72%5D: array(%5B%5B 1.   ,  1.006,  1.013,  1.019,  1.026%5D,                %5B 0.994,  1.   ,  1.006,  1.013,  1.019%5D,                %5B 0.987,  0.994,  1.   ,  1.006,  1.013%5D,                %5B 0.981,  0.987,  0.994,  1.   ,  1.006%5D,                %5B 0.975,  0.981,  0.987,  0.994,  1.   %5D%5D)In%5B73%5D:md%3Dd**mdmd.round(3)Out%5B73%5D: array(%5B%5B 1.   ,  1.   ,  1.   ,  1.   ,  1.   %5D,                %5B 0.994,  0.994,  0.994,  0.994,  0.994%5D,                %5B 0.987,  0.987,  0.987,  0.987,  0.987%5D,                %5B 0.981,  0.981,  0.981,  0.981,  0.981%5D,                %5B 0.975,  0.975,  0.975,  0.975,  0.975%5D%5D)Finally, bringing everything together:In%5B74%5D:S%3DS0*mu*mdS.round(3)Out%5B74%5D: array(%5B%5B 100.   ,  100.634,  101.273,  101.915,  102.562%5D,                %5B  98.743,   99.37 ,  100.   ,  100.634,  101.273%5D,                %5B  97.502,   98.121,   98.743,   99.37 ,  100.   %5D,                %5B  96.276,   96.887,   97.502,   98.121,   98.743%5D,                %5B  95.066,   95.669,   96.276,   96.887,   97.502%5D%5D)From the ndarray object S, only the upper triangular matrix is of importance. Althoughwe do more calculations with this approach than are needed in principle, the approachis, as expected, much faster than the first version, which relies heavily on nested loopson the Python level:In%5B75%5D:M%3D1000# reset number of time steps%25timeround(binomial_np(100),3)","Out%5B75%5D: CPU times: user 308 ms, sys: 6 ms, total: 314 ms         Wall time: 304 ms         10.449Numbahas proven a valuable performance enhancement tool for our sandbox example.Here, it can prove its worth in the context of a very important financial algorithm:In%5B76%5D:binomial_nb%3Dnb.jit(binomial_py)In%5B77%5D:%25timeround(binomial_nb(100),3)Out%5B77%5D: CPU times: user 1.71 s, sys: 137 ms, total: 1.84 s         Wall time: 1.59 s         10.449We do not yet see a significant speedup over the NumPy vectorized version since the firstcall of the compiled function involves some overhead. Therefore, using theperf_comp_func function shall shed a more realistic light on how the three differentimplementations compare with regard to performance. Obviously, the Numba compiledversion is indeed significantly faster than the NumPy version:In%5B78%5D:func_list%3D%5B'binomial_py','binomial_np','binomial_nb'%5DK%3D100.data_list%3D3*%5B'K'%5DIn%5B79%5D:perf_comp_data(func_list,data_list)Out%5B79%5D: function: binomial_nb, av. time sec:   0.14800, relative:    1.0         function: binomial_np, av. time sec:   0.31770, relative:    2.1         function: binomial_py, av. time sec:   3.36707, relative:   22.8In summary, we can state the following:%E2%80%A2",": using Numba involves only a little additional effort. The original functionis often not changed at all%3B all you need to do is call the jit function.%E2%80%A2",": Numbaoften leads to significant improvements in execution speed, notonly compared to pure Python but also to vectorized NumPy implementations.%E2%80%A2",": with Numbathere is no need to initialize large array objects%3B the compilerspecializes the machine code to the problem at hand (as compared to the %E2%80%9Cuniversal%E2%80%9Dfunctions of NumPy) and maintains memory efficiency, as with pure Python.","The strength of Numba is the effortless application of the approach to arbitrary functions.However, Numba will only %E2%80%9Ceffortlessly%E2%80%9D generate significant performance improvementsfor certain types of problems. Another approach, which is more flexible but also more","involved, is to go the route of static compiling with Cython. In effect, Cythonis a hybridlanguage of Python and  C . Coming from Python, the major differences to be noticed arethe static type declarations (as in  C ) and a separate compiling step (as with any compiledlanguage).As a simple example function, consider the following nested loop that again returnssimply the number of loops. Compared to the previous nested loop example, this timethe number of inner loop iterations is scaled by the outer loop iterations. In such a case,you will pretty quickly run into memory troubles when you try to apply NumPyfor aspeedup:In%5B80%5D:deff_py(I,J):res%3D0.# we work on a float objectforiinrange(I):forjinrange(J*I):res%2B%3D1returnresLet us check Python performance for I %3D 500 and J %3D 500. A NumPyndarrayobjectallowing us to vectorize the function f_pyin such a case would already have to have ashape of (500, 250000):In%5B81%5D:I,J%3D500,500%25timef_py(I,J)Out%5B81%5D: CPU times: user 17 s, sys: 2.72 s, total: 19.7 s         Wall time: 14.2 s         125000000.0Consider next the code shown in Example 8-1. It takes the very same function andintroduces static type declarations for use with Cython. Note that the suffix of this Cythonfile is .pyx.Example 8-1. Nested loop example with Cython static type declarations## Nested loop example with Cython# nested_loop.pyx#deff_cy(intI,intJ):cdefdoubleres%3D0# double float much slower than int or longforiinrange(I):forjinrange(J*I):res%2B%3D1returnresIn such a simple case, when no special  C  modules are needed, there is an easy way toimport such a module-namely, via pyximport:","In%5B82%5D:importpyximportpyximport.install()Out%5B82%5D: (None, %3Cpyximport.pyximport.PyxImporter at 0x92cfc10%3E)This allows us now to directly import from the Cython module:In%5B83%5D:importsyssys.path.append('data/')# path to the Cython script# not needed if in same directoryIn%5B84%5D:fromnested_loopimportf_cyNow, we can check the performance of the Cython function:In%5B85%5D:%25timeres%3Df_cy(I,J)Out%5B85%5D: CPU times: user 154 ms, sys: 0 ns, total: 154 ms         Wall time: 153 msIn%5B86%5D:resOut%5B86%5D: 125000000.0When working in IPythonNotebookthere is a more convenient way to use Cython-cythonmagic:In%5B87%5D:%25load_extcythonmagicLoading this extension from within the IPython Notebookallows us to compile codewith Cython from within the tool:In%5B88%5D:%25%25cython## Nested loop example with Cython#deff_cy(intI,intJ):cdefdoubleres%3D0# double float much slower than int or longforiinrange(I):forjinrange(J*I):res%2B%3D1returnresThe performance results should, of course, be (almost) the same:In%5B89%5D:%25timeres%3Df_cy(I,J)Out%5B89%5D: CPU times: user 156 ms, sys: 0 ns, total: 156 ms         Wall time: 154 msIn%5B90%5D:resOut%5B90%5D: 125000000.0Let us see what Numba can do in this case. The application is as straightforward as before:","2.See also Chapter 10 on these topics.In%5B91%5D:importnumbaasnbIn%5B92%5D:f_nb%3Dnb.jit(f_py)The performance is-when invoking the function for the first time-worse than that ofthe Cythonversion (recall that with the first call of the Numbacompiled function thereis always some overhead involved):In%5B93%5D:%25timeres%3Df_nb(I,J)Out%5B93%5D: CPU times: user 285 ms, sys: 9 ms, total: 294 ms         Wall time: 273 msIn%5B94%5D:resOut%5B94%5D: 125000000.0Finally, the more rigorous comparison-showing that the Numba version indeed keepsup with the Cython version(s):In%5B95%5D:func_list%3D%5B'f_py','f_cy','f_nb'%5DI,J%3D500,500data_list%3D3*%5B'I, J'%5DIn%5B96%5D:perf_comp_data(func_list,data_list)Out%5B96%5D: function: f_nb, av. time sec:   0.15162, relative:    1.0         function: f_cy, av. time sec:   0.15275, relative:    1.0         function: f_py, av. time sec:  14.08304, relative:   92.9","The last topic in this chapter is the use of devices for massively parallel operations-i.e.,General Purpose Graphical Processing Units (GPGPUs, or simply GPUs). To use anNvidia GPU, we need to have CUDA(Compute Unified Device Architecture, cf.https://developer.nvidia.com) installed. An easy way to harness the power of NvidiaGPUs is to use NumbaPro, a performance library by Continuum Analytics that dynam%E2%80%90ically compiles Python code for the GPU (or a multicore CPU).This chapter does not allow us to go into the details of GPU usage for Pythonprogram%E2%80%90ming. However, there is one financial field that can benefit strongly from the use of aGPU: Monte Carlo simulation and (pseudo)random number generation in particular.2In what follows, we use the native CUDA library curandto generate random numbers onthe GPU:In%5B97%5D:fromnumbapro.cudalibimportcurandAs the benchmark case, we define a function, using NumPy, that delivers a two-dimensional array of standard normally distributed pseudorandom numbers:","In%5B98%5D:defget_randoms(x,y):rand%3Dnp.random.standard_normal((x,y))returnrandFirst, let's check if it works:In%5B99%5D:get_randoms(2,2)Out%5B99%5D: array(%5B%5B-0.30561007,  1.33124048%5D,                %5B-0.04382143,  2.31276888%5D%5D)Now the function for the Nvidia GPU:In%5B100%5D:defget_cuda_randoms(x,y):rand%3Dnp.empty((x*y),np.float64)# rand serves as a container for the randoms# CUDA only fills 1-dimensional arraysprng%3Dcurand.PRNG(rndtype%3Dcurand.PRNG.XORWOW)# the argument sets the random number algorithmprng.normal(rand,0,1)# filling the containerrand%3Drand.reshape((x,y))# to be %22fair%22, we reshape rand to 2 dimensionsreturnrandAgain, a brief check of the functionality:In%5B101%5D:get_cuda_randoms(2,2)Out%5B101%5D: array(%5B%5B 1.07102161,  0.70846868%5D,                 %5B 0.89437398, -0.86693007%5D%5D)And a first comparison of the performance:In%5B102%5D:%25timeita%3Dget_randoms(1000,1000)Out%5B102%5D: 10 loops, best of 3: 72 ms per loopIn%5B103%5D:%25timeita%3Dget_cuda_randoms(1000,1000)Out%5B103%5D: 100 loops, best of 3: 14.8 ms per loopNow, a more systematic routine to compare the performance:In%5B104%5D:importtimeaststep%3D1000deftime_comparsion(factor):cuda_times%3Dlist()cpu_times%3Dlist()forjinrange(1,10002,step):i%3Dj*factort0%3Dt.time()a%3Dget_randoms(i,1)t1%3Dt.time()cpu_times.append(t1-t0)t2%3Dt.time()a%3Dget_cuda_randoms(i,1)t3%3Dt.time()","cuda_times.append(t3-t2)print%22Bytes of largest array %25i%22%25a.nbytesreturncuda_times,cpu_timesAnd a helper function to visualize performance results:In%5B105%5D:defplot_results(cpu_times,cuda_times,factor):plt.plot(x*factor,cpu_times,'b',label%3D'NUMPY')plt.plot(x*factor,cuda_times,'r',label%3D'CUDA')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('size of random number array')plt.ylabel('time')plt.axis('tight')Let's take a look at the first test series with a medium workload:In%5B106%5D:factor%3D100cuda_times,cpu_times%3Dtime_comparsion(factor)Out%5B106%5D: Bytes of largest array 8000800Calculation time for the random numbers on the GPU is almost independentof thenumbers to be generated. By constrast, time on the CPU rises sharply with increasingsize of the random number array to be generated. Both statements can be verified inFigure 8-5:In%5B107%5D:x%3Dnp.arange(1,10002,step)In%5B108%5D:plot_results(cpu_times,cuda_times,factor)Figure 8-5. Random number generation on GPU and CPU (factor %3D 100)Now let's look at the second test series, with a pretty low workload:In%5B109%5D:factor%3D10cuda_times,cpu_times%3Dtime_comparsion(factor)Out%5B109%5D: Bytes of largest array 800080","The overhead of using the GPU is too large for low workloads-something quite obviousfrom inspecting Figure 8-6:In%5B110%5D:plot_results(cpu_times,cuda_times,factor)Figure 8-6. Random number generation on GPU and CPU (factor %3D 10)Now let's consider a test series with a comparatively heavy workload. The largest randomnumber array is 400 MB in size:In%5B111%5D:%25%25timefactor%3D5000cuda_times,cpu_times%3Dtime_comparsion(factor)Out%5B111%5D: Bytes of largest array 400040000          CPU times: user 22 s, sys: 3.52 s, total: 25.5 s          Wall time: 25.4 sFor heavy workloads the GPU clearly shows its advantages, as Figure 8-7 impressivelyillustrates:In%5B112%5D:plot_results(cpu_times,cuda_times,factor)","Figure 8-7. Random number generation on GPU and CPU (factor %3D 5,000)","Nowadays, the Python ecosystem provides a number of ways to improve the perfor%E2%80%90mance of code:ParadigmsSome Python paradigms might be more performant than others, given a specificproblem.LibrariesThere is a wealth of libraries available for different types of problems, which oftenlead to much higher performance given a problem that fits into the scope of thelibrary (e.g., numexpr).CompilingA number of powerful compiling solutions are available, including static (e.g.,Cython) and dynamic ones (e.g., Numba).ParallelizationSome Python libraries have built-in parallelization capabilities (e.g., numexpr),while others allow us to harness the full power of multiple-core CPUs, whole clusters(e.g., IPython.parallel), or GPUs (e.g., NumbaPro).A major benefit of the Python ecosystem is that all these approaches generally are easilyimplementable, meaning that the additional effort included is generally quite low (evenfor nonexperts). In other words, performance improvements often are low-hangingfruits given the performance libraries available as of today.","For all performance libraries introduced in this chapter, there are valuable web resourcesavailable:%E2%80%A2For details on numexpr see http://github.com/pydata/numexpr.%E2%80%A2IPython.parallel is explained here: http://ipython.org/ipython-doc/stable/parallel.%E2%80%A2Find the documentation for the multiprocessing module here: https://docs.python.org/2/library/multiprocessing.html.%E2%80%A2Information on Numba can be found at http://github.com/numba/numba.%E2%80%A2http://cython.org is the home of the Cython compiler project.%E2%80%A2For the documentation of NumbaPro, refer to http://docs.continuum.io/numbapro.For a reference in book form, see the following:%E2%80%A2Gorelick, Misha and Ian Ozsvald (2014): High Performance Python. O'Reilly, Se%E2%80%90bastopol, CA.","The mathematicians are the priests of the modern world.- Bill GaedeSince the arrival of the so-called Rocket Scientists on Wall Street in the '80s and '90s,finance has evolved into a discipline of applied mathematics. While early research papersin finance came with few mathematical expressions and equations, current ones aremainly comprised of mathematical expressions and equations, with some explanatorytext around.This chapter introduces a number of useful mathematical tools for finance, withoutproviding a detailed background for each of them. There are many useful books on thistopic available. Therefore, this chapter focuses on how to use the tools and techniqueswith Python. Among other topics, it covers:ApproximationRegression and interpolation are among the most often used numerical techniquesin finance.Convex optimizationA number of financial disciplines need tools for convex optimization (e.g., optionpricing when it comes to model calibration).IntegrationIn particular, the valuation of financial (derivative) assets often boils down to theevaluation of integrals.Symbolic mathematicsPython provides with SymPy a powerful tool for symbolic mathematics, e.g., to solve(systems of) equations.","To begin with, let us import the libraries that we need for the moment-NumPyandmatplotlib.pyplot:In%5B1%5D:importnumpyasnpimportmatplotlib.pyplotasplt%25matplotlibinlineThroughout this discussion, the main example function we will use is the following,which is comprised of a trigonometric term and a linear term:In%5B2%5D:deff(x):returnnp.sin(x)%2B0.5*xThe main focus is the approximation of this function over a given interval by regressionand interpolation. First, let us generate a plot of the function to get a better view of whatexactly the approximation shall achieve. The interval of interest shall be %5B%E2%80%932%ED%9C%8B,2%ED%9C%8B%5D.Figure 9-1 displays the function over the fixed interval defined via the linspace func%E2%80%90tion. np.linspace(start,stop,num) returns num points beginning with startandending with stop, with the subintervals between two consecutive points being evenlyspaced:In%5B3%5D:x%3Dnp.linspace(-2*np.pi,2*np.pi,50)In%5B4%5D:plt.plot(x,f(x),'b')plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')Figure 9-1. Example function plot","Regression is a rather efficient tool when it comes to function approximation. It is notonly suited to approximate one-dimensional functions but also works well in higher","dimensions. The numerical techniques needed to come up with regression results areeasily implemented and quickly executed. Basically, the task of regression, given a setof so-called basis functions bd, d%E2%88%88 %7B1,%E2%80%A6,D%7D, is to find optimal parameters %CE%B11*,...,%CE%B1D*according to Equation 9-1, where yi%E2%89%A1f(xi) for i%E2%88%88 %7B1,%E2%8B%AF, I%7D observation points. The xiare considered independent observations and the yi dependent observations (in a func%E2%80%90tional or statistical sense).Equation 9-1. Minimization problem of regressionmin%CE%B11,...,%CE%B1D1I%E2%88%91i%3D1Iyi%E2%88%92%E2%88%91d%3D1D%CE%B1d%C2%B7bdxi2","One of the simplest cases is to take monomials as basis functions-i.e., b1%3D 1, b2 %3D x, b3%3D x2, b4 %3D x3,%E2%80%A6. In such a case, NumPyhas built-in functions for both the determinationof the optimal parameters (namely, polyfit) and the evaluation of the approximationgiven a set of input values (namely, polyval).Table 9-1lists the parameters the polyfitfunction takes. Given the returned optimalregression coefficients p from polyfit, np.polyval(p, x) then returns the regressionvalues for the x coordinates.Table 9-1. Parameters of polyfit function","xx coordinates (independent variable values)yy coordinates (dependent variable values)degDegree of the fitting polynomialfullIf True, returns diagnostic information in additionwWeights to apply to the y coordinatescovIf True, covariance matrix is also returnedIn typical vectorized fashion, the application of polyfit and polyval takes on the fol%E2%80%90lowing form for a linear regression (i.e., for deg%3D1):In%5B5%5D:reg%3Dnp.polyfit(x,f(x),deg%3D1)ry%3Dnp.polyval(reg,x)Given the regression estimates stored in the ry array, we can compare the regressionresult with the original function as presented in Figure 9-2. Of course, a linear regressioncannot account for the sin part of the example function:","In%5B6%5D:plt.plot(x,f(x),'b',label%3D'f(x)')plt.plot(x,ry,'r.',label%3D'regression')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')Figure 9-2. Example function and linear regressionTo account for the sinpart of the example function, higher-order monomials are nec%E2%80%90essary. The next regression attempt takes monomials up to the order of 5 as basis func%E2%80%90tions. It should not be too surprising that the regression result, as seen in Figure 9-3,now looks much closer to the original function. However, it is still far away from beingperfect:In%5B7%5D:reg%3Dnp.polyfit(x,f(x),deg%3D5)ry%3Dnp.polyval(reg,x)In%5B8%5D:plt.plot(x,f(x),'b',label%3D'f(x)')plt.plot(x,ry,'r.',label%3D'regression')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')The last attempt takes monomials up to order 7 to approximate the example function.In this case the result, as presented in Figure 9-4, is quite convincing:In%5B9%5D:reg%3Dnp.polyfit(x,f(x),7)ry%3Dnp.polyval(reg,x)","Figure 9-3. Regression with monomials up to order 5In%5B10%5D:plt.plot(x,f(x),'b',label%3D'f(x)')plt.plot(x,ry,'r.',label%3D'regression')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')Figure 9-4. Regression with monomials up to order 7A brief check reveals that the result is not perfect:In%5B11%5D:np.allclose(f(x),ry)Out%5B11%5D: FalseHowever, the mean squared error (MSE) is not too large-at least, over this narrowrange of x values:In%5B12%5D:np.sum((f(x)-ry)**2)/len(x)Out%5B12%5D: 0.0017769134759517413","In general, you can reach better regression results when you can choose better sets ofbasis functions, e.g., by exploiting knowledge about the function to approximate. In thiscase, the individual basis functions have to be defined via a matrix approach (i.e., usinga NumPyndarray object). First, the case with monomials up to order 3:In%5B13%5D:matrix%3Dnp.zeros((3%2B1,len(x)))matrix%5B3,:%5D%3Dx**3matrix%5B2,:%5D%3Dx**2matrix%5B1,:%5D%3Dxmatrix%5B0,:%5D%3D1The sublibrary numpy.linalgprovides the function lstsqto solve least-squares opti%E2%80%90mization problems like the one in Equation 9-1:In%5B14%5D:reg%3Dnp.linalg.lstsq(matrix.T,f(x))%5B0%5DApplying lstsqto our problem in this way yields the optimal parameters for the singlebasis functions:In%5B15%5D:regOut%5B15%5D: array(%5B  1.13968447e-14,   5.62777448e-01,  -8.88178420e-16,                 -5.43553615e-03%5D)To get the regression estimates we apply the dotfunction to the reg and matrix arrays.Figure 9-5 shows the result. np.dot(a, b) simply gives the dot product for the two arraysa and b:In%5B16%5D:ry%3Dnp.dot(reg,matrix)In%5B17%5D:plt.plot(x,f(x),'b',label%3D'f(x)')plt.plot(x,ry,'r.',label%3D'regression')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')The result in Figure 9-5is not really as good as expected based on our previous expe%E2%80%90rience with monomials. Using the more general approach allows us to exploit ourknowledge about the example function. We know that there is a sin part in the function.Therefore, it makes sense to include a sine function in the set of basis functions. Forsimplicity, we replace the highest-order monomial:In%5B18%5D:matrix%5B3,:%5D%3Dnp.sin(x)reg%3Dnp.linalg.lstsq(matrix.T,f(x))%5B0%5Dry%3Dnp.dot(reg,matrix)","Figure 9-5. Regression via least-squares functionFigure 9-6 illustrates that the regression is now pretty close to the original function:In%5B19%5D:plt.plot(x,f(x),'b',label%3D'f(x)')plt.plot(x,ry,'r.',label%3D'regression')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')Figure 9-6. Regression using individual functionsIndeed, the regression now is %E2%80%9Cperfect%E2%80%9D in a numerical sense:In%5B20%5D:np.allclose(f(x),ry)Out%5B20%5D: TrueIn%5B21%5D:np.sum((f(x)-ry)**2)/len(x)Out%5B21%5D: 2.2749084503102031e-31","In fact, the minimization routine recovers the correct parameters of 1 for the sin partand 0.5 for the linear part:In%5B22%5D:regOut%5B22%5D: array(%5B  1.55428020e-16,   5.00000000e-01,   0.00000000e%2B00,                  1.00000000e%2B00%5D)","Regression can cope equally well with noisy data, be it data from simulation or from(non-perfect) measurements. To illustrate this point, let us generate both independentobservations with noise and also dependent observations with noise:In%5B23%5D:xn%3Dnp.linspace(-2*np.pi,2*np.pi,50)xn%3Dxn%2B0.15*np.random.standard_normal(len(xn))yn%3Df(xn)%2B0.25*np.random.standard_normal(len(xn))The very regression is the same:In%5B24%5D:reg%3Dnp.polyfit(xn,yn,7)ry%3Dnp.polyval(reg,xn)Figure 9-7reveals that the regression results are closer to the original function than thenoisy data points. In a sense, the regression averages out the noise to some extent:In%5B25%5D:plt.plot(xn,yn,'b%5E',label%3D'f(x)')plt.plot(xn,ry,'ro',label%3D'regression')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')Figure 9-7. Regression with noisy data","Another important aspect of regression is that the approach also works seamlessly withunsorted data. The previous examples all rely on sorted x data. This does not have to bethe case. To make the point, let us randomize the independent data points as follows:In%5B26%5D:xu%3Dnp.random.rand(50)*4*np.pi-2*np.piyu%3Df(xu)In this case, you can hardly identify any structure by just visually inspecting theraw data:In%5B27%5D:printxu%5B:10%5D.round(2)printyu%5B:10%5D.round(2)Out%5B27%5D: %5B 4.09  0.5   1.48 -1.85  1.65  4.51 -5.7   1.83  4.42 -4.2 %5D         %5B 1.23  0.72  1.74 -1.89  1.82  1.28 -2.3   1.88  1.25 -1.23%5DAs with the noisy data, the regression approach does not care for the order of the ob%E2%80%90servation points. This becomes obvious upon inspecting the structure of the minimi%E2%80%90zation problem in Equation 9-1. It is also obvious by the results, as presented inFigure 9-8:In%5B28%5D:reg%3Dnp.polyfit(xu,yu,5)ry%3Dnp.polyval(reg,xu)In%5B29%5D:plt.plot(xu,yu,'b%5E',label%3D'f(x)')plt.plot(xu,ry,'ro',label%3D'regression')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')Figure 9-8. Regression with unsorted data","Another convenient characteristic of the least-squares regression approach is that itcarries over to multiple dimensions without too many modifications. As an examplefunction we take fm, as presented next:In%5B30%5D:deffm((x,y)):returnnp.sin(x)%2B0.25*x%2Bnp.sqrt(y)%2B0.05*y**2To visualize this function, we need a grid of (independent) data points:In%5B31%5D:x%3Dnp.linspace(0,10,20)y%3Dnp.linspace(0,10,20)X,Y%3Dnp.meshgrid(x,y)# generates 2-d grids out of the 1-d arraysZ%3Dfm((X,Y))x%3DX.flatten()y%3DY.flatten()# yields 1-d arrays from the 2-d gridsBased on the grid of independent and dependent data points as embodied now by X,Y, Z, Figure 9-9 presents the shape of the function fm:In%5B32%5D:frommpl_toolkits.mplot3dimportAxes3Dimportmatplotlibasmplfig%3Dplt.figure(figsize%3D(9,6))ax%3Dfig.gca(projection%3D'3d')surf%3Dax.plot_surface(X,Y,Z,rstride%3D2,cstride%3D2,cmap%3Dmpl.cm.coolwarm,linewidth%3D0.5,antialiased%3DTrue)ax.set_xlabel('x')ax.set_ylabel('y')ax.set_zlabel('f(x, y)')fig.colorbar(surf,shrink%3D0.5,aspect%3D5)","1.For details on the use of OLS, refer to the documentation.Figure 9-9. Function with two parametersTo get good regression results we compile a set of basis functions, including both a sinand a sqrt function, which leverages our knowledge of the example function:In%5B33%5D:matrix%3Dnp.zeros((len(x),6%2B1))matrix%5B:,6%5D%3Dnp.sqrt(y)matrix%5B:,5%5D%3Dnp.sin(x)matrix%5B:,4%5D%3Dy**2matrix%5B:,3%5D%3Dx**2matrix%5B:,2%5D%3Dymatrix%5B:,1%5D%3Dxmatrix%5B:,0%5D%3D1The statsmodels library offers the quite general and helpful function OLSfor least-squares regression both in one dimension and multiple dimensions:1In%5B34%5D:importstatsmodels.apiassmIn%5B35%5D:model%3Dsm.OLS(fm((x,y)),matrix).fit()One advantage of using the OLS function is that it provides a wealth of additional in%E2%80%90formation about the regression and its quality. A summary of the results is accessed bycalling model.summary. Single statistics, like the coefficient of determination, can ingeneral also be accessed directly:In%5B36%5D:model.rsquaredOut%5B36%5D: 1.0","For our purposes, we of course need the optimal regression parameters, which are storedin the params attribute of our model object:In%5B37%5D:a%3Dmodel.paramsaOut%5B37%5D: array(%5B  7.14706072e-15,   2.50000000e-01,  -2.22044605e-16,                 -1.02348685e-16,   5.00000000e-02,   1.00000000e%2B00,                  1.00000000e%2B00%5D)The function reg_func gives back, for the given optimal regression parameters and theindpendent data points, the function values for the regression function:In%5B38%5D:defreg_func(a,(x,y)):f6%3Da%5B6%5D*np.sqrt(y)f5%3Da%5B5%5D*np.sin(x)f4%3Da%5B4%5D*y**2f3%3Da%5B3%5D*x**2f2%3Da%5B2%5D*yf1%3Da%5B1%5D*xf0%3Da%5B0%5D*1return(f6%2Bf5%2Bf4%2Bf3%2Bf2%2Bf1%2Bf0)These values can then be compared with the original shape of the example function, asshown in Figure 9-10:In%5B39%5D:RZ%3Dreg_func(a,(X,Y))In%5B40%5D:fig%3Dplt.figure(figsize%3D(9,6))ax%3Dfig.gca(projection%3D'3d')surf1%3Dax.plot_surface(X,Y,Z,rstride%3D2,cstride%3D2,cmap%3Dmpl.cm.coolwarm,linewidth%3D0.5,antialiased%3DTrue)surf2%3Dax.plot_wireframe(X,Y,RZ,rstride%3D2,cstride%3D2,label%3D'regression')ax.set_xlabel('x')ax.set_ylabel('y')ax.set_zlabel('f(x, y)')ax.legend()fig.colorbar(surf,shrink%3D0.5,aspect%3D5)","Figure 9-10. Higher-dimension regression","Least-squaresregressionapproacheshavemultipleareasofapplica%E2%80%90tion, including simple function approximation and function approx%E2%80%90imationbasedonnoisyorunsorteddata.Theseapproachescanbeappliedtosingleaswellasmultidimensionalproblems.Duetotheunderlying mathematics, the application is always %E2%80%9Calmost the same.%E2%80%9D","Compared to regression, interpolation (e.g., with cubic splines), is much more involvedmathematically. It is also limited to low-dimensional problems. Given an ordered set ofobservation points (ordered in the x dimension), the basic idea is to do a regressionbetween two neighboring data points in such a way that not only are the data pointsperfectly matched by the resulting, piecewise-defined interpolation function, but alsothat the function is continuously differentiable at the data points. Continuous differ%E2%80%90entiability requires at least interpolation of degree 3-i.e., with cubic splines. However,the approach also works in general with quadratic and even linear splines. First, theimporting of the respective sublibrary:In%5B41%5D:importscipy.interpolateasspiIn%5B42%5D:x%3Dnp.linspace(-2*np.pi,2*np.pi,25)We take again the original example function for illustration purposes:In%5B43%5D:deff(x):returnnp.sin(x)%2B0.5*x","The application itself, given an x-ordered set of data points, is as simple as the applicationof polyfit and polyval. Here, the respective functions are splrep and splev. Table 9-2lists the major parameters that the splrep function takes.Table 9-2. Parameters of splrep function","x(Ordered) x coordinates (independent variable values)y(x-ordered) y coordinates (dependent variable values)wWeights to apply to the y coordinatesxb, xeInterval to fit, if None%5Bx%5B0%5D, x%5B-1%5D%5DkOrder of the spline fit (1 %3C%3D k %3C%3D 5)sSmoothing factor (the larger, the more smoothing)full_outputIf True additional output is returnedquietIf True suppress messagesTable 9-3 lists the parameters that the splev function takes.Table 9-3. Parameters of splev function","x(Ordered) x coordinates (independent variable values)tckSequence of length 3 returned by splrep (knots, coefficients, degree)derOrder of derivative (0 for function, 1 for first derivative)extBehavior if x not in knot sequence (0 extrapolate, 1 return 0, 2 raise ValueError)Applied to the current example, this translates into the following:In%5B44%5D:ipo%3Dspi.splrep(x,f(x),k%3D1)In%5B45%5D:iy%3Dspi.splev(x,ipo)As Figure 9-11 shows, the interpolation already seems really good with linear splines(i.e., k%3D1):In%5B46%5D:plt.plot(x,f(x),'b',label%3D'f(x)')plt.plot(x,iy,'r.',label%3D'interpolation')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')","Figure 9-11. Example plot with linear interpolationThis can be confirmed numerically:In%5B47%5D:np.allclose(f(x),iy)Out%5B47%5D: TrueSpline interpolation is often used in finance to generate estimates for dependent valuesof independent data points not included in the original observations. To this end, let uspick a much smaller interval and have a closer look at the interpolated values with thelinear splines:In%5B48%5D:xd%3Dnp.linspace(1.0,3.0,50)iyd%3Dspi.splev(xd,ipo)Figure 9-12 reveals that the interpolation function indeed interpolates linearlybetweentwo observation points. For certain applications this might not be precise enough. Inaddition, it is evident that the function is not continuously differentiable at the originaldata points-another drawback:In%5B49%5D:plt.plot(xd,f(xd),'b',label%3D'f(x)')plt.plot(xd,iyd,'r.',label%3D'interpolation')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')","Figure 9-12. Example plot (detail) with linear interpolationTherefore, let us repeat the complete exercise, this time using cubic splines:In%5B50%5D:ipo%3Dspi.splrep(x,f(x),k%3D3)iyd%3Dspi.splev(xd,ipo)Now, the detailed subinterval in Figure 9-13 shows a graphically perfect interpolation:In%5B51%5D:plt.plot(xd,f(xd),'b',label%3D'f(x)')plt.plot(xd,iyd,'r.',label%3D'interpolation')plt.legend(loc%3D0)plt.grid(True)plt.xlabel('x')plt.ylabel('f(x)')Figure 9-13. Example plot (detail) with cubic spline interpolationNumerically, the interpolation is not perfect, but the MSE is really small:In%5B52%5D:np.allclose(f(xd),iyd)Out%5B52%5D: False","In%5B53%5D:np.sum((f(xd)-iyd)**2)/len(xd)Out%5B53%5D: 1.1349319851436252e-08","In those cases where spline interpolation can be applied you can ex%E2%80%90pectbetterapproximationresultscomparedtoaleast-squaresregres%E2%80%90sionapproach.However,rememberthatyouneedtohavesorted(and%E2%80%9Cnonnoisy%E2%80%9D)dataandthattheapproachislimitedtolow-dimensionalproblems.Itisalsocomputationallymoredemandingandmighttherefore take (much) longer than regression in certain use cases.","In finance and economics, convex optimizationplays an important role. Examples arethe calibration of option pricing models to market data or the optimization of an agent'sutility. As an example function that we want to minimize, we take fm, as defined in thefollowing:In%5B54%5D:deffm((x,y)):return(np.sin(x)%2B0.05*x**2%2Bnp.sin(y)%2B0.05*y**2)In%5B55%5D:x%3Dnp.linspace(-10,10,50)y%3Dnp.linspace(-10,10,50)X,Y%3Dnp.meshgrid(x,y)Z%3Dfm((X,Y))Figure 9-14shows the function graphically for the defined intervals for x and y. Visualinspection already reveals that this function has multiple local minima. The existenceof a global minimum cannot really be confirmed by this particular graphicalrepresentation:In%5B56%5D:fig%3Dplt.figure(figsize%3D(9,6))ax%3Dfig.gca(projection%3D'3d')surf%3Dax.plot_surface(X,Y,Z,rstride%3D2,cstride%3D2,cmap%3Dmpl.cm.coolwarm,linewidth%3D0.5,antialiased%3DTrue)ax.set_xlabel('x')ax.set_ylabel('y')ax.set_zlabel('f(x, y)')fig.colorbar(surf,shrink%3D0.5,aspect%3D5)","Figure 9-14. Function to minimize with two parametersIn what follows, we want to implement both a global minimization approach and a localone. The functions brute and fminthat we want to use can be found in the sublibraryscipy.optimize:In%5B57%5D:importscipy.optimizeasspo","To have a closer look behind the scenes when we initiate the minimization procedures,we amend the original function by an option to output current parameter values as wellas the function value:In%5B58%5D:deffo((x,y)):z%3Dnp.sin(x)%2B0.05*x**2%2Bnp.sin(y)%2B0.05*y**2ifoutput%3D%3DTrue:print'%258.4f%258.4f%258.4f'%25(x,y,z)returnzThis allows us to keep track of all relevant information for the procedure, as the followingcode with its respective output illustrates. brutetakes the parameter ranges as input.For example, providing parameter range (-10, 10.1, 5) for the x value will lead to%E2%80%9Ctested%E2%80%9D values of -10, -5, 0, 5, 10:In%5B59%5D:output%3DTruespo.brute(fo,((-10,10.1,5),(-10,10.1,5)),finish%3DNone)Out%5B59%5D: -10.0000 -10.0000  11.0880         -10.0000 -10.0000  11.0880         -10.0000  -5.0000   7.7529         -10.0000   0.0000   5.5440         -10.0000   5.0000   5.8351","         -10.0000  10.0000  10.0000          -5.0000 -10.0000   7.7529          -5.0000  -5.0000   4.4178          -5.0000   0.0000   2.2089          -5.0000   5.0000   2.5000          -5.0000  10.0000   6.6649           0.0000 -10.0000   5.5440           0.0000  -5.0000   2.2089           0.0000   0.0000   0.0000           0.0000   5.0000   0.2911           0.0000  10.0000   4.4560           5.0000 -10.0000   5.8351           5.0000  -5.0000   2.5000           5.0000   0.0000   0.2911           5.0000   5.0000   0.5822           5.0000  10.0000   4.7471          10.0000 -10.0000  10.0000          10.0000  -5.0000   6.6649          10.0000   0.0000   4.4560          10.0000   5.0000   4.7471          10.0000  10.0000   8.9120         array(%5B 0.,  0.%5D)The optimal parameter values, given the initial parameterization of the function, are x%3D y%3D 0. The resulting function value is also 0, as a quick review of the preceding outputreveals. The first parameterization here is quite rough, in that we used steps of width 5for both input parameters. This can of course be refined considerably, leading to betterresults in this case:In%5B60%5D:output%3DFalseopt1%3Dspo.brute(fo,((-10,10.1,0.1),(-10,10.1,0.1)),finish%3DNone)opt1Out%5B60%5D: array(%5B-1.4, -1.4%5D)In%5B61%5D:fm(opt1)Out%5B61%5D: -1.7748994599769203The optimal parameter values are now x %3D y%3D %E2%80%931.4 and the minimal function value forthe global minimization is about %E2%80%931.7749.","For the local convex optimization we want to draw on the results from the global opti%E2%80%90mization. The function fmin takes as input the function to minimize and the startingparameter values. In addition, you can define levels for the input parameter toleranceand the function value tolerance, as well as for the maximum number of iterations andfunction calls:","In%5B62%5D:output%3DTrueopt2%3Dspo.fmin(fo,opt1,xtol%3D0.001,ftol%3D0.001,maxiter%3D15,maxfun%3D20)opt2Out%5B62%5D:  -1.4000  -1.4000  -1.7749          -1.4700  -1.4000  -1.7743          -1.4000  -1.4700  -1.7743          -1.3300  -1.4700  -1.7696          -1.4350  -1.4175  -1.7756          -1.4350  -1.3475  -1.7722          -1.4088  -1.4394  -1.7755          -1.4438  -1.4569  -1.7751          -1.4328  -1.4427  -1.7756          -1.4591  -1.4208  -1.7752          -1.4213  -1.4347  -1.7757          -1.4235  -1.4096  -1.7755          -1.4305  -1.4344  -1.7757          -1.4168  -1.4516  -1.7753          -1.4305  -1.4260  -1.7757          -1.4396  -1.4257  -1.7756          -1.4259  -1.4325  -1.7757          -1.4259  -1.4241  -1.7757          -1.4304  -1.4177  -1.7757          -1.4270  -1.4288  -1.7757         Warning: Maximum number of function evaluations has been exceeded.         array(%5B-1.42702972, -1.42876755%5D)Again, we can observe a refinement of the solution and a somewhat lower functionvalue:In%5B63%5D:fm(opt2)Out%5B63%5D: -1.7757246992239009For many convex optimization problems it is advisable to have a global minimizationbefore the local one. The major reason for this is that local convex optimization algo%E2%80%90rithms can easily be trapped in a local minimum (or do %E2%80%9Cbasin hopping%E2%80%9D), ignoringcompletely %E2%80%9Cbetter%E2%80%9D local minima and/or a global minimum. The following shows thatsetting the starting parameterization to x%3D y %3D 2 gives a %E2%80%9Cminimum%E2%80%9D value of above zero:In%5B64%5D:output%3DFalsespo.fmin(fo,(2.0,2.0),maxiter%3D250)Out%5B64%5D: Optimization terminated successfully.                  Current function value: 0.015826                  Iterations: 46                  Function evaluations: 86         array(%5B 4.2710728 ,  4.27106945%5D)","So far, we have only considered unconstrained optimization problems. However, largeclasses of economic or financial optimization problems are constrained by one or mul%E2%80%90tiple constraints. Such constraints can formally take on the form of equations orinequalities.As a simple example, consider the utility maximization problem of an (expected utilitymaximizing) investor who can invest in two risky securities. Both securities cost qa %3D qb%3D 10 today. After one year, they have a payoff of 15 USD and 5 USD, respectively, in stateu, and of 5 USD and 12 USD, respectively, in state d. Both states are equally likely. Denotethe vector payoffs for the two securities by ra and rb, respectively.The investor has a budget of w0 %3D 100 USD to invest and derives utility from futurewealth according to the utility function uw%3Dw, where w is the wealth (USD amount)available. Equation 9-2is a formulation of the maximization problem where a,b are thenumbers of securities bought by the investor.Equation 9-2. Expected utility maximizing problemmaxa,b%ED%90%80uw1%3Dpw1u%2B1%E2%88%92pw1dw1%3Dara%2Bbrbw0%E2%89%A5aqa%2Bbqba,b%E2%89%A50Putting in all numerical assumptions, we get the problem in Equation 9-3. Note that wealso change to the minimization of the negative expected utility.Equation 9-3. Expected utility maximizing problemmina,b%E2%88%92%ED%90%80uw1%3D%E2%88%920.5%C2%B7w1u%2B0.5%C2%B7w1dw1u%3Da%C2%B715%2Bb%C2%B75w1d%3Da%C2%B75%2Bb%C2%B712100%E2%89%A5a%C2%B710%2Bb%C2%B710a,b%E2%89%A50To solve this problem, we use the scipy.optimize.minimize function. This functiontakes as input-in addition to the function to be minimized-equations and inequalities","2.For details and examples of how to use the minimize function, refer to the documentation.(as a list of dict objects) as well as boundaries for the parameters (as a tupleof tupleobjects).2 We can translate the problem from Equation 9-3 into the following code:In%5B65%5D:# function to be minimizedfrommathimportsqrtdefEu((s,b)):return-(0.5*sqrt(s*15%2Bb*5)%2B0.5*sqrt(s*5%2Bb*12))# constraintscons%3D(%7B'type':'ineq','fun':lambda(s,b):100-s*10-b*10%7D)# budget constraintbnds%3D((0,1000),(0,1000))# uppper bounds large enoughWe have everything we need to use the minimize function-we just have to add an initialguess for the optimal parameters:In%5B66%5D:result%3Dspo.minimize(Eu,%5B5,5%5D,method%3D'SLSQP',bounds%3Dbnds,constraints%3Dcons)In%5B67%5D:resultOut%5B67%5D:   status: 0          success: True             njev: 5             nfev: 21              fun: -9.700883611487832                x: array(%5B 8.02547122,  1.97452878%5D)          message: 'Optimization terminated successfully.'              jac: array(%5B-0.48508096, -0.48489535,  0.        %5D)              nit: 5The function returns a dictobject. The optimal parameters can be read out as follows:In%5B68%5D:result%5B'x'%5DOut%5B68%5D: array(%5B 8.02547122,  1.97452878%5D)The optimal function value is (changing the sign again):In%5B69%5D:-result%5B'fun'%5DOut%5B69%5D: 9.700883611487832Given the parameterization for the simple model, it is optimal for the investor to buyabout eight units of security aand about two units of security b. The budget constraintis binding%3B i.e., the investor invests his/her total wealth of 100 USD into the securities.This is easily verified through taking the dot product of the optimal parameter vectorand the price vector:In%5B70%5D:np.dot(result%5B'x'%5D,%5B10,10%5D)Out%5B70%5D: 99.999999999999986","Especially when it comes to valuation and option pricing, integration is an importantmathematical tool. This stems from the fact that risk-neutral values of derivatives canbe expressed in general as the discounted expectation of their payoff under the risk-neutral (martingale) measure. The expectation in turn is a sum in the discrete case andan integral in the continuous case. The sublibrary scipy.integrate provides differentfunctions for numerical integration:In%5B71%5D:importscipy.integrateassciAgain, we stick to the example function comprised of a sincomponent and alinear one:In%5B72%5D:deff(x):returnnp.sin(x)%2B0.5*xWe are interested in the integral over the interval %5B0.5, 9.5%5D%3B i.e., the integral as inEquation 9-4.Equation 9-4. Integral of example function0.59.5sinx%2B0.5xdxFigure 9-15 provides a graphical representation of the integral with a plot of the functionf(x) %E2%89%A1 sin(x) %2B 0.5x:In%5B73%5D:a%3D0.5# left integral limitb%3D9.5# right integral limitx%3Dnp.linspace(0,10)y%3Df(x)In%5B74%5D:frommatplotlib.patchesimportPolygonfig,ax%3Dplt.subplots(figsize%3D(7,5))plt.plot(x,y,'b',linewidth%3D2)plt.ylim(ymin%3D0)# area under the function# between lower and upper limitIx%3Dnp.linspace(a,b)Iy%3Df(Ix)verts%3D%5B(a,0)%5D%2Blist(zip(Ix,Iy))%2B%5B(b,0)%5Dpoly%3DPolygon(verts,facecolor%3D'0.7',edgecolor%3D'0.5')ax.add_patch(poly)# labelsplt.text(0.75*(a%2Bb),1.5,r%22%24%5Cint_a%5Eb f(x)dx%24%22,","horizontalalignment%3D'center',fontsize%3D20)plt.figtext(0.9,0.075,'%24x%24')plt.figtext(0.075,0.9,'%24f(x)%24')ax.set_xticks((a,b))ax.set_xticklabels(('%24a%24','%24b%24'))ax.set_yticks(%5Bf(a),f(b)%5D)Figure 9-15. Example function with integral area","The integratesublibrary contains a selection of functions to numerically integrate agiven mathematical function given upper and lower integration limits. Examples arefixed_quad for fixed Gaussian quadrature, quad for adaptive quadrature, and rombergfor Romberg integration:In%5B75%5D:sci.fixed_quad(f,a,b)%5B0%5DOut%5B75%5D: 24.366995967084588In%5B76%5D:sci.quad(f,a,b)%5B0%5DOut%5B76%5D: 24.374754718086752In%5B77%5D:sci.romberg(f,a,b)Out%5B77%5D: 24.374754718086713There are also a number of integration functions that take as input listor ndarrayobjects with function values and input values. Examples in this regard are trapz, usingthe trapezoidal rule, and simps, implementing Simpson's rule:In%5B78%5D:xi%3Dnp.linspace(0.5,9.5,25)","In%5B79%5D:sci.trapz(f(xi),xi)Out%5B79%5D: 24.352733271544516In%5B80%5D:sci.simps(f(xi),xi)Out%5B80%5D: 24.374964184550748","The valuation of options and derivatives by Monte Carlo simulation (cf. Chapter 10)rests on the insight that you can evaluate an integral by simulation. To this end, draw Irandom values of xbetween the integral limits and evaluate the integration function atevery random value of x. Sum up all the function values and take the average to arriveat an average function value over the integration interval. Multiply this value by thelength of the integration interval to derive an estimate for the integral value.The following code shows how the Monte Carlo estimated integral value converges tothe real one when one increases the number of random draws. The estimator is alreadyquite close for really small numbers of random draws:In%5B81%5D:foriinrange(1,20):np.random.seed(1000)x%3Dnp.random.random(i*10)*(b-a)%2Baprintnp.sum(f(x))/len(x)*(b-a)Out%5B81%5D: 24.8047622793         26.5229188983         26.2655475192         26.0277033994         24.9995418144         23.8818101416         23.5279122748         23.507857659         23.6723674607         23.6794104161         24.4244017079         24.2390053468         24.115396925         24.4241919876         23.9249330805         24.1948421203         24.1173483782         24.1006909297         23.7690510985","The previous sections are mainly concerned with numerical computation. This sectionnow introduces symbolic computation, which can be applied beneficially in many areas","of finance. To this end, let us import SymPy, the library specifically dedicated to symboliccomputation:In%5B82%5D:importsympyassy","SymPy introduces new classes of objects. A fundamental class is the Symbol class:In%5B83%5D:x%3Dsy.Symbol('x')y%3Dsy.Symbol('y')In%5B84%5D:type(x)Out%5B84%5D: sympy.core.symbol.SymbolLike NumPy, SymPy has a number of (mathematical) function definitions. For example:In%5B85%5D:sy.sqrt(x)Out%5B85%5D: sqrt(x)This already illustrates a major difference. Although x has no numerical value, the squareroot of x is nevertheless defined with SymPysince x is a Symbol object. In that sense,sy.sqrt(x) can be part of arbitrary mathematical expressions. Notice that SymPyingeneral automatically simplifies a given mathematical expression:In%5B86%5D:3%2Bsy.sqrt(x)-4**2Out%5B86%5D: sqrt(x) - 13Similarly, you can define arbitrary functions using Symbolobjects. They are not to beconfused with Python functions:In%5B87%5D:f%3Dx**2%2B3%2B0.5*x**2%2B3/2In%5B88%5D:sy.simplify(f)Out%5B88%5D: 1.5*x**2 %2B 4SymPy provides three basic renderers for mathematical expressions:%E2%80%A2LaTeX-based%E2%80%A2Unicode-based%E2%80%A2ASCII-basedWhen working, for example, solely in the IPythonNotebook, LaTeXrendering is gen%E2%80%90erally a good (i.e., visually appealing) choice. In what follows, we stick to the simplestoption, ASCII, to illustrate that there is no handmade type setting involved:In%5B89%5D:sy.init_printing(pretty_print%3DFalse,use_unicode%3DFalse)In%5B90%5D:printsy.pretty(f)","Out%5B90%5D:      2         1.5*x  %2B 4As you can see from the output, multiple lines are used whenever needed. Also, forexample, see the following for the visual representation of the square-root function:In%5B91%5D:printsy.pretty(sy.sqrt(x)%2B0.5)Out%5B91%5D:   ___         %5C/ x  %2B 0.5We can not go into details here, but SymPy also provides many other useful mathematicalfunctions-for example, when it comes to numerically evaluating %ED%9C%8B. The followingshows the first 40 characters of the string representation of %ED%9C%8B up to the 400,000th digit:In%5B92%5D:pi_str%3Dstr(sy.N(sy.pi,400000))pi_str%5B:40%5DOut%5B92%5D: '3.14159265358979323846264338327950288419'And here are the last 40 digits of the first 400,000:In%5B93%5D:pi_str%5B-40:%5DOut%5B93%5D: '8245672736856312185020980470362464176198'You can also look up your birthday if you wish%3B however, there is no guarantee of a hit:In%5B94%5D:pi_str.find('111272')Out%5B94%5D: 366713","A strength of SymPy is solving equations, e.g., of the form x2 %E2%80%93 1 %3D 0:In%5B95%5D:sy.solve(x**2-1)Out%5B95%5D: %5B-1, 1%5DIn general, SymPy presumes that you are looking for a solution to the equation obtainedby equating the given expression to zero. Therefore, equations like x2 %E2%80%93 1 %3D 3 might haveto be reformulated to get the desired result:In%5B96%5D:sy.solve(x**2-1-3)Out%5B96%5D: %5B-2, 2%5DOf course, SymPy can cope with more complex expressions, like x3 %2B 0.5x2 %E2%80%93 1 %3D 0:In%5B97%5D:sy.solve(x**3%2B0.5*x**2-1)Out%5B97%5D: %5B0.858094329496553, -0.679047164748276 - 0.839206763026694*I,         -0.679047164748276 %2B 0.839206763026694*I%5D","However, there is obviously no guarantee of a solution, either from a mathematical pointof view (i.e., the existence of a solution) or from an algorithmic point of view (i.e., animplementation).SymPy works similarly with functions exhibiting more than one input parameter, andto this end also with complex numbers. As a simple example take the equation x2 %2B y2%3D 0:In%5B98%5D:sy.solve(x**2%2By**2)Out%5B98%5D: %5B%7Bx: -I*y%7D, %7Bx: I*y%7D%5D","Another strength of SymPyis integration and differentiation. In what follows, we revisitthe example function used for numerical- and simulation-based integration and derivenow both a symbolic and a numerically exact solution. We need symbols for the inte%E2%80%90gration limits:In%5B99%5D:a,b%3Dsy.symbols('a b')Having defined the new symbols, we can %E2%80%9Cpretty print%E2%80%9D the symbolic integral:In%5B100%5D:printsy.pretty(sy.Integral(sy.sin(x)%2B0.5*x,(x,a,b)))Out%5B100%5D:   b            /           %7C           %7C  (0.5*x %2B sin(x)) dx           %7C          /          aUsing integrate, we can then derive the antiderivative of the integration function:In%5B101%5D:int_func%3Dsy.integrate(sy.sin(x)%2B0.5*x,x)In%5B102%5D:printsy.pretty(int_func)Out%5B102%5D:       2          0.25*x  - cos(x)Equipped with the antiderivative, the numerical evaluation of the integral is only threesteps away. To numerically evaluate a SymPyexpression, replace the respective symbolwith the numerical value using the method subsand call the method evalfon the newexpression:In%5B103%5D:Fb%3Dint_func.subs(x,9.5).evalf()Fa%3Dint_func.subs(x,0.5).evalf()The difference between Fb and Fa then yields the exact integral value:In%5B104%5D:Fb-Fa# exact value of integral","Out%5B104%5D: 24.3747547180867The integral can also be solved symbolically with the symbolic integration limits:In%5B105%5D:int_func_limits%3Dsy.integrate(sy.sin(x)%2B0.5*x,(x,a,b))printsy.pretty(int_func_limits)Out%5B105%5D:         2         2          - 0.25*a  %2B 0.25*b  %2B cos(a) - cos(b)As before, numerical substitution-this time using a dictobject for multiple substitu%E2%80%90tions-and evaluation then yields the integral value:In%5B106%5D:int_func_limits.subs(%7Ba:0.5,b:9.5%7D).evalf()Out%5B106%5D: 24.3747547180868Finally, providing quantified integration limits yields the exact value in a single step:In%5B107%5D:sy.integrate(sy.sin(x)%2B0.5*x,(x,0.5,9.5))Out%5B107%5D: 24.3747547180867","The derivative of the antiderivative shall yield in general the original function. Let uscheck this by applying the diff function to the symbolic antiderivative from before:In%5B108%5D:int_func.diff()Out%5B108%5D: 0.5*x %2B sin(x)As with the integration example, we want to use differentiation now to derive the exactsolution of the convex minimization problem we looked at earlier. To this end, we definethe respective function symbolically as follows:In%5B109%5D:f%3D(sy.sin(x)%2B0.05*x**2%2Bsy.sin(y)%2B0.05*y**2)For the minimization, we need the two partial derivatives with respect to both variables,x and y:In%5B110%5D:del_x%3Dsy.diff(f,x)del_xOut%5B110%5D: 0.1*x %2B cos(x)In%5B111%5D:del_y%3Dsy.diff(f,y)del_yOut%5B111%5D: 0.1*y %2B cos(y)A necessary but not sufficient condition for a global minimum is that both partial de%E2%80%90rivatives are zero. As stated before, there is no guarantee of a symbolic solution. Bothalgorithmic and (multiple) existence issues come into play here. However, we can solve","the two equations numerically, providing %E2%80%9Ceducated%E2%80%9D guesses based on the global andlocal minimization efforts from before:In%5B112%5D:xo%3Dsy.nsolve(del_x,-1.5)xoOut%5B112%5D: mpf('-1.4275517787645941')In%5B113%5D:yo%3Dsy.nsolve(del_y,-1.5)yoOut%5B113%5D: mpf('-1.4275517787645941')In%5B114%5D:f.subs(%7Bx:xo,y:yo%7D).evalf()# global minimumOut%5B114%5D: -1.77572565314742Again, providing uneducated/arbitrary guesses might trap the algorithm in a local min%E2%80%90imum instead of the global one:In%5B115%5D:xo%3Dsy.nsolve(del_x,1.5)xoOut%5B115%5D: mpf('1.7463292822528528')In%5B116%5D:yo%3Dsy.nsolve(del_y,1.5)yoOut%5B116%5D: mpf('1.7463292822528528')In%5B117%5D:f.subs(%7Bx:xo,y:yo%7D).evalf()# local minimumOut%5B117%5D: 2.27423381055640This numerically illustrates that zero partial derivatives are necessary but not sufficient.","WhendoingmathematicswithPython,youshouldalwaysthinkofSymPyandsymboliccomputations.Especiallyforinteractivefinancialanalytics,thiscanbeamoreefficientapproachcomparedtonon-symbolic approaches.","This chapter covers some mathematical topics and tools important to finance. For ex%E2%80%90ample, the approximation of functions is important in many financial areas, like yieldcurve interpolation and regression-based Monte Carlo valuation approaches for Amer%E2%80%90ican options. Convex optimizationtechniques are also regularly needed in finance%3B forexample, when calibrating parametric option pricing models to market quotes or im%E2%80%90plied volatilities of options.","Numerical integrationis, for example, central to the pricing of options and derivatives.Having derived the risk-neutral probability measure for a (set of) stochastic process(es),option pricing boils down to taking the expectation of the option's payoff under the risk-neutral measure and discounting this value back to the present date. Chapter 10 coversthe simulation of several types of stochastic processes under the risk-neutral measure.Finally, this chapter introduces symbolic computation with SymPy. For a number ofmathematical operations, like integration, differentiation, or the solving of equations,symbolic computation can prove a really useful and efficient tool.","For further information on the Python libraries used in this chapter, you should consultthe following web resources:%E2%80%A2See http://docs.scipy.org/doc/numpy/reference/ for all functions used from NumPy.%E2%80%A2The statsmodels library is documented here: http://statsmodels.sourceforge.net.%E2%80%A2Visit http://docs.scipy.org/doc/scipy/reference/optimize.html for details on scipy.optimize.%E2%80%A2Integration with scipy.integrate is explained here: http://docs.scipy.org/doc/scipy/reference/integrate.html.%E2%80%A2The home of SymPy is http://sympy.org.For a good reference to the mathematical topics covered, see:%E2%80%A2Brandimarte, Paolo (2006): Numerical Methods in Finance and Economics, 2nd ed.John Wiley %26 Sons, Hoboken, NJ.","Predictability is not how things will go, but how they can go.- Raheel FarooqNowadays, stochastics is one of the most important mathematical and numerical dis%E2%80%90ciplines in finance. In the beginning of the modern era of finance, mainly in the 1970sand 1980s, the major goal of financial research was to come up with closed-form solu%E2%80%90tions for, e.g., option prices given a specific financial model. The requirements havedrastically changed in recent years in that not only is the correct valuation of singlefinancial instruments important to participants in the financial markets, but also theconsistent valuation of whole derivatives books, for example. Similary, to come up withconsistent risk measures across a whole financial institution, like value-at-risk and creditvalue adjustments, one needs to take into account the whole book of the institution andall its counterparties. Such daunting tasks can only be tackled by flexible and efficientnumerical methods. Therefore, stochastics in general and Monte Carlo simulation inparticular have risen to prominence.This chapter introduces the following topics from a Python perspective:Random number generationIt all starts with (pseudo)random numbers, which build the basis for all simulationefforts%3B although quasirandom numbers, e.g., based on Sobol sequences, havegained some popularity in finance, pseudorandom numbers still seem to be thebenchmark.SimulationIn finance, two simulation tasks are of particular importance: simulation of randomvariables and of stochastic processes.ValuationThe two main disciplines when it comes to valuation are the valuation of derivativeswith European exercise (at a specific date) and American exercise(over a specific","1.For simplicity, we will speak of random numbers knowing that all numbers used will be pseudorandom.time interval)%3B there are also instruments with Bermudan exercise, or exercise at afinite set of specific dates.Risk measuresSimulation lends itself pretty well to the calculation of risk measures like value-at-risk, credit value-at-risk, and credit value adjustments.","Throughout this chapter, to generate random numbers1 we will work with the functionsprovided by the numpy.random sublibrary:In%5B1%5D:importnumpyasnpimportnumpy.randomasnprimportmatplotlib.pyplotasplt%25matplotlibinlineFor example, the randfunction returns random numbers from the open interval %5B0,1)in the shape provided as a parameter to the function. The return object is an ndarrayobject:In%5B2%5D:npr.rand(10)Out%5B2%5D: array(%5B 0.40628966,  0.43098644,  0.9435419 ,  0.26760198,  0.2729951 ,                0.67519064,  0.41349754,  0.3585647 ,  0.07450132,  0.95130158%5D)In%5B3%5D:npr.rand(5,5)Out%5B3%5D: array(%5B%5B 0.87263851,  0.8143348 ,  0.34154499,  0.56695052,  0.60645041%5D,               %5B 0.39398181,  0.71671577,  0.63568321,  0.61652708,  0.93526172%5D,               %5B 0.12632038,  0.35793789,  0.04241014,  0.88085228,  0.54260211%5D,               %5B 0.14503456,  0.32939077,  0.28834351,  0.4050322 ,  0.21120017%5D,               %5B 0.45345805,  0.29771411,  0.67157606,  0.73563706,  0.48003387%5D        %5D)Such numbers can be easily transformed to cover other intervals of the real line. Forinstance, if you want to generate random numbers from the interval %5Ba,b)%3D%5B5,10), youcan transform the returned numbers from rand as follows:In%5B4%5D:a%3D5.b%3D10.npr.rand(10)*(b-a)%2BaOut%5B4%5D: array(%5B 7.27123881,  6.51309437,  7.51380629,  7.84258434,  7.62199611,                8.86229349,  6.78202851,  6.33248656,  8.10776244,  9.48668419%5D)This also works for multidimensional shapes due to NumPy broadcasting:In%5B5%5D:npr.rand(5,5)*(b-a)%2Ba","2.Cf. http://docs.scipy.org/doc/numpy/reference/routines.random.html.Out%5B5%5D: array(%5B%5B 6.65649828,  6.51657569,  9.7912274 ,  8.93721206,  6.66937996%5D,               %5B 8.97919481,  8.27547365,  5.00975386,  8.99797249,  6.05374605%5D,               %5B 7.50268777,  8.43810167,  9.33608096,  8.5513646 ,  5.53651748%5D,               %5B 7.04179874,  6.98111966,  8.42677435,  6.22325043,  6.39226557%5D,               %5B 9.88334499,  7.59597546,  5.93724861,  5.39285822,  5.28435207%5D        %5D)Table 10-1 lists functions for generating simple random numbers.2Table 10-1. Functions for simple random number generation","randd0, d1, %E2%80%A6, dnRandoms in the given shaperandnd0, d1, %E2%80%A6, dnA sample (or samples) from the standard normal distributionrandintlow%5B, high, size%5DRandom integers from low (inclusive) to high (exclusive)random_integerslow%5B, high, size%5DRandom integers between low and high, inclusiverandom_sample%5Bsize%5DRandom floats in the half-open interval %5B0.0, 1.0)random%5Bsize%5DRandom floats in the half-open interval %5B0.0, 1.0)ranf%5Bsize%5DRandom floats in the half-open interval %5B0.0, 1.0)sample%5Bsize%5DRandom floats in the half-open interval %5B0.0, 1.0)choicea%5B, size, replace, p%5DRandom sample from a given 1D arraybyteslengthRandom bytesLet us visualize some random draws generated by selected functions from Table 10-1:In%5B6%5D:sample_size%3D500rn1%3Dnpr.rand(sample_size,3)rn2%3Dnpr.randint(0,10,sample_size)rn3%3Dnpr.sample(size%3Dsample_size)a%3D%5B0,25,50,75,100%5Drn4%3Dnpr.choice(a,size%3Dsample_size)Figure 10-1 shows the results graphically for two continuous distributions and twodiscrete ones:In%5B7%5D:fig,((ax1,ax2),(ax3,ax4))%3Dplt.subplots(nrows%3D2,ncols%3D2,figsize%3D(7,7))ax1.hist(rn1,bins%3D25,stacked%3DTrue)ax1.set_title('rand')ax1.set_ylabel('frequency')ax1.grid(True)ax2.hist(rn2,bins%3D25)ax2.set_title('randint')ax2.grid(True)ax3.hist(rn3,bins%3D25)ax3.set_title('sample')","3.Cf. http://docs.scipy.org/doc/numpy/reference/routines.random.html.ax3.set_ylabel('frequency')ax3.grid(True)ax4.hist(rn4,bins%3D25)ax4.set_title('choice')ax4.grid(True)Figure 10-1. Simple pseudorandom numbersTable 10-2 lists functions for generating random numbers according to different dis%E2%80%90tributions.3Table 10-2. Functions to generate random numbers according to different distributionlaws","betaa, b%5B, size%5DSamples for beta distribution over %5B0, 1%5Dbinomialn, p%5B, size%5DSamples from a binomial distributionchisquaredf%5B, size%5DSamples from a chi-square distributiondirichletalpha%5B, size%5DSamples from the Dirichlet distributionexponential%5Bscale, size%5DSamples from the exponential distributionfdfnum, dfden%5B, size%5DSamples from an F distributiongammashape%5B, scale, size%5DSamples from a gamma distribution","geometricp%5B, size%5DSamples from the geometric distributiongumbel%5Bloc, scale, size%5DSamples from a Gumbel distributionhypergeometricngood, nbad, nsample%5B, size%5DSamples from a hypergeometric distributionlaplace%5Bloc, scale, size%5DSamples from the Laplace or double exponentialdistributionlogistic%5Bloc, scale, size%5DSamples from a logistic distributionlognormalv%5Bmean, sigma, size%5DSamples from a log-normal distributionlogseriesp%5B, size%5DSamples from a logarithmic series distributionmultinomialn, pvals%5B, size%5DSamples from a multinomial distributionmultivariate_normalmean, cov%5B, size%5DSamples from a multivariate normal distributionnegative_binomialn, p%5B, size%5DSamples from a negative binomial distributionnoncentral_chisquaredf, nonc%5B, size%5DSamples from a noncentral chi-square distributionnoncentral_fdfnum, dfden, nonc%5B, size%5Dsamples from the noncentral F distributionnormal%5Bloc, scale, size%5DSamples from a normal (Gaussian) distributionparetoa%5B, size%5DSamples from a Pareto II or Lomax distribution withspecified shapepoisson%5Blam, size%5DSamples from a Poisson distributionpowera%5B, size%5DSamplesin%5B0,1%5Dfromapowerdistributionwithpositiveexponent a%E2%80%931rayleigh%5Bscale, size%5DSamples from a Rayleigh distributionstandard_cauchy%5Bsize%5DSamples from standard Cauchy distribution with mode%3D 0standard_exponential%5Bsize%5DSamples from the standard exponential distributionstandard_gammashape%5B, size%5DSamples from a standard gamma distributionstandard_normal%5Bsize%5DSamples from a standard normal distribution (mean%3D0,stdev%3D1)standard_tdf%5B, size%5DSamplesfromaStudent'stdistributionwithdfdegreesof freedomtriangularleft, mode, right%5B, size%5DSamples from the triangular distributionuniform%5Blow, high, size%5DSamples from a uniform distributionvonmisesmu, kappa%5B, size%5DSamples from a von Mises distributionwaldmean, scale%5B, size%5DSamples from a Wald, or inverse Gaussian, distributionweibulla%5B, size%5DSamples from a Weibull distributionzipfa%5B, size%5DSamples from a Zipf distributionAlthough there is much criticism around the use of (standard) normal distributions infinance, they are an indispensible tool and still the most widely used type of distribution,in analytical as well as numerical applications. One reason is that many financial models","directly rest in one way or another on a normal distribution or a log-normal distribution.Another reason is that many financial models that do not rest directly on a (log-)normalassumption can be discretized, and therewith approximated for simulation purposes,by the use of the normal distribution.As an illustration, we want to visualize random draws from the following distributions:%E2%80%A2"," with mean of 0 and standard deviation of 1%E2%80%A2"," with mean of 100 and standard deviation of 20%E2%80%A2"," with 0.5 degrees of freedom%E2%80%A2"," with lambda of 1We do this as follows:In%5B8%5D:sample_size%3D500rn1%3Dnpr.standard_normal(sample_size)rn2%3Dnpr.normal(100,20,sample_size)rn3%3Dnpr.chisquare(df%3D0.5,size%3Dsample_size)rn4%3Dnpr.poisson(lam%3D1.0,size%3Dsample_size)Figure 10-2 shows the results for the three continuous distributions and the discreteone (Poisson). The Poisson distribution is used, for example, to simulate the arrival of(rare) external events, like a jump in the price of an instrument or an exogenic shock.Here is the code that generates it:In%5B9%5D:fig,((ax1,ax2),(ax3,ax4))%3Dplt.subplots(nrows%3D2,ncols%3D2,figsize%3D(7,7))ax1.hist(rn1,bins%3D25)ax1.set_title('standard normal')ax1.set_ylabel('frequency')ax1.grid(True)ax2.hist(rn2,bins%3D25)ax2.set_title('normal(100, 20)')ax2.grid(True)ax3.hist(rn3,bins%3D25)ax3.set_title('chi square')ax3.set_ylabel('frequency')ax3.grid(True)ax4.hist(rn4,bins%3D25)ax4.set_title('Poisson')ax4.grid(True)","Figure 10-2. Pseudorandom numbers from different distributions","Monte Carlo simulation (MCS) is among the most important numerical techniques infinance, if not the most important and widely used. This mainly stems from the fact thatit is the most flexible numerical method when it comes to the evaluation of mathematicalexpressions (e.g., integrals), and specifically the valuation of financial derivatives. Theflexibility comes at the cost of a relatively high computational burden, though, sinceoften hundreds of thousands or even millions of complex computations have to becarried out to come up with a single value estimate.","Consider, for example, the Black-Scholes-Merton setup for option pricing (cf. alsoChapter 3). In their setup, the level of a stock index STat a future date Tgiven a level S0as of today is given according to Equation 10-1.Equation 10-1. Simulating future index level in Black-Scholes-Merton setupST%3DS0expr%E2%88%9212%CF%832T%2B%CF%83TzThe variables and parameters have the following meaning:","STIndex level at date TrConstant riskless short rate%CF%83Constant volatility (%3D standard deviation of returns) of SzStandard normally distributed random variableThis simple financial model is easily parameterized and simulated as follows:In%5B10%5D:S0%3D100# initial valuer%3D0.05# constant short ratesigma%3D0.25# constant volatilityT%3D2.0# in yearsI%3D10000# number of random drawsST1%3DS0*np.exp((r-0.5*sigma**2)*T%2Bsigma*np.sqrt(T)*npr.standard_normal(I))The output of this simulation code is shown in Figure 10-3:In%5B11%5D:plt.hist(ST1,bins%3D50)plt.xlabel('index level')plt.ylabel('frequency')plt.grid(True)Figure 10-3. Simulated geometric Brownian motion (via standard_normal)Figure 10-3 suggests that the distribution of the random variable as defined inEquation 10-1 is log-normal. We could therefore also try to use the lognormal functionto directly derive the values for the random variable. In that case, we have to providethe mean and the standard deviation to the function:","In%5B12%5D:ST2%3DS0*npr.lognormal((r-0.5*sigma**2)*T,sigma*np.sqrt(T),size%3DI)Figure 10-4 shows the output of the following simulation code:In%5B13%5D:plt.hist(ST2,bins%3D50)plt.xlabel('index level')plt.ylabel('frequency')plt.grid(True)Figure 10-4. Simulated geometric Brownian motion (via lognormal)By visual inspection, Figure 10-4 and Figure 10-3indeed look pretty similar. But let usverify this more rigorously by comparing statistical moments of the resultingdistributions.To compare the distributional characteristics of simulation results we use the scipy.stats sublibrary and the helper function print_statistics, as defined here:In%5B14%5D:importscipy.statsasscsIn%5B15%5D:defprint_statistics(a1,a2):''' Prints selected statistics.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             a1, a2 : ndarray objects                 results object from simulation             '''sta1%3Dscs.describe(a1)sta2%3Dscs.describe(a2)print%22%2514s%2514s%2514s%22%25 %5C('statistic','data set 1','data set 2')print45*%22-%22print%22%2514s%2514.3f%2514.3f%22%25('size',sta1%5B0%5D,sta2%5B0%5D)print%22%2514s%2514.3f%2514.3f%22%25('min',sta1%5B1%5D%5B0%5D,sta2%5B1%5D%5B0%5D)print%22%2514s%2514.3f%2514.3f%22%25('max',sta1%5B1%5D%5B1%5D,sta2%5B1%5D%5B1%5D)print%22%2514s%2514.3f%2514.3f%22%25('mean',sta1%5B2%5D,sta2%5B2%5D)","print%22%2514s%2514.3f%2514.3f%22%25('std',np.sqrt(sta1%5B3%5D),np.sqrt(sta2%5B3%5D))print%22%2514s%2514.3f%2514.3f%22%25('skew',sta1%5B4%5D,sta2%5B4%5D)print%22%2514s%2514.3f%2514.3f%22%25('kurtosis',sta1%5B5%5D,sta2%5B5%5D)In%5B16%5D:print_statistics(ST1,ST2)Out%5B16%5D:      statistic     data set 1     data set 2         ---------------------------------------------                   size      10000.000      10000.000                    min         27.936         27.266                    max        410.795        358.997                   mean        110.442        110.528                    std         39.932         40.894                   skew          1.082          1.150               kurtosis          1.927          2.273Obviously, the statistics of both simulation results are quite similar. The differences aremainly due to what is called the sampling errorin simulation. Error can also be intro%E2%80%90duced when discretely simulating continuousstochastic processes-namely the discre%E2%80%90tization error, which plays no role here due to the static nature of the simulationapproach.","Roughly speaking, a stochastic processis a sequence of random variables. In that sense,we should expect something similar to a sequence of repeated simulations of a randomvariable when simulating a process. This is mainly true, apart from the fact that thedraws are in general not independent but rather depend on the result(s) of the previousdraw(s). In general, however, stochastic processes used in finance exhibit the Markovproperty, which mainly says that tomorrow's value of the process only depends on today'sstate of the process, and not any other more %E2%80%9Chistoric%E2%80%9D state or even the whole pathhistory. The process then is also called memoryless.","Consider now the Black-Scholes-Merton model in its dynamic form, as described bythe stochastic differential equation (SDE) in Equation 10-2. Here, Zt is a standard Brow%E2%80%90nian motion. The SDE is called a geometric Brownian motion. The values of St are log-normally distributed and the (marginal) returns dStSt normally.Equation 10-2. Stochastic differential equation in Black-Scholes-Merton setupdSt %3D rStdt %2B %ED%9C%8EStdZt","The SDE in Equation 10-2 can be discretized exactly by an Euler scheme. Such a schemeis presented in Equation 10-3, with %ED%9B%A5t being the fixed discretization interval and ztbeinga standard normally distributed random variable.Equation 10-3. Simulating index levels dynamically in Black-Scholes-Merton setupSt%3DSt%E2%88%92%CE%94texpr%E2%88%9212%CF%832%CE%94t%2B%CF%83%CE%94tztAs before, translation into Python and NumPy code is straightforward:In%5B17%5D:I%3D10000M%3D50dt%3DT/MS%3Dnp.zeros((M%2B1,I))S%5B0%5D%3DS0fortinrange(1,M%2B1):S%5Bt%5D%3DS%5Bt-1%5D*np.exp((r-0.5*sigma**2)*dt%2Bsigma*np.sqrt(dt)*npr.standard_normal(I))The resulting end values for the index level are log-normally distributed again, asFigure 10-5 illustrates:In%5B18%5D:plt.hist(S%5B-1%5D,bins%3D50)plt.xlabel('index level')plt.ylabel('frequency')plt.grid(True)Figure 10-5. Simulated geometric Brownian motion at maturityThe first four moments are also quite close to those resulting from the static simulationapproach:In%5B19%5D:print_statistics(S%5B-1%5D,ST2)","Out%5B19%5D:      statistic     data set 1     data set 2         ---------------------------------------------                   size      10000.000      10000.000                    min         25.531         27.266                    max        425.051        358.997                   mean        110.900        110.528                    std         40.135         40.894                   skew          1.086          1.150               kurtosis          2.224          2.273Figure 10-6 shows the first 10 simulated paths:In%5B20%5D:plt.plot(S%5B:,:10%5D,lw%3D1.5)plt.xlabel('time')plt.ylabel('index level')plt.grid(True)Figure 10-6. Simulated geometric Brownian motion pathsUsing the dynamic simulation approach not only allows us to visualize paths as displayedin Figure 10-6, but also to value options with American/Bermudan exercise or optionswhose payoff is path-dependent. You get the full dynamic picture, so to say:","Another important class of financial processes is mean-reverting processes, which areused to model short rates or volatility processes, for example. A popular and widely usedmodel is the square-root diffusion, as proposed by Cox, Ingersoll, and Ross (1985).Equation 10-4 provides the respective SDE.Equation 10-4. Stochastic differential equation for square-root diffusiondxt%3D%CE%BA%CE%B8%E2%88%92xtdt%2B%CF%83xtdZt","The variables and parameters have the following meaning:xtProcess level at date t%CE%BAMean-reversion factor%CE%B8Long-term mean of the process%CF%83Constant volatility parameterZStandard Brownian motionIt is well known that the values of xt are chi-squared distributed. However, as statedbefore, many financial models can be discretized and approximated by using the normaldistribution (i.e., a so-called Euler discretization scheme). While the Euler scheme isexact for the geometric Brownian motion, it is biased for the majority of other stochasticprocesses. Even if there is an exact scheme available-one for the square-root diffusionwill be presented shortly-the use of an Euler scheme might be desirable due to nu%E2%80%90merical and/or computational reasons. Defining s%E2%89%A1t %E2%80%93 %CE%94t and x%2B%E2%89%A1 max(x,0),Equation 10-5 presents such an Euler scheme. This particular one is generally calledfull truncation in the literature (cf. Hilpisch (2015)).Equation 10-5. Euler discretization for square-root diffusionx%CB%9Ct%3Dx%CB%9Cs%2B%CE%BA%CE%B8%E2%88%92x%CB%9Cs%2B%CE%94t%2B%CF%83x%CB%9Cs%2B%CE%94tztxt%3Dx%CB%9Ct%2BWe parameterize the model for the simulations to follow with values that could representthose of a short rate model:In%5B21%5D:x0%3D0.05kappa%3D3.0theta%3D0.02sigma%3D0.1The square-root diffusion has the convenient and realistic characteristic that the valuesof xtremain strictly positive. When discretizing it by an Euler scheme, negative valuescannot be excluded. That is the reason why one works always with the positive versionof the originally simulated process. In the simulation code, one therefore needs twondarray objects instead of only one:","In%5B22%5D:I%3D10000M%3D50dt%3DT/Mdefsrd_euler():xh%3Dnp.zeros((M%2B1,I))x1%3Dnp.zeros_like(xh)xh%5B0%5D%3Dx0x1%5B0%5D%3Dx0fortinrange(1,M%2B1):xh%5Bt%5D%3D(xh%5Bt-1%5D%2Bkappa*(theta-np.maximum(xh%5Bt-1%5D,0))*dt%2Bsigma*np.sqrt(np.maximum(xh%5Bt-1%5D,0))*np.sqrt(dt)*npr.standard_normal(I))x1%3Dnp.maximum(xh,0)returnx1x1%3Dsrd_euler()Figure 10-7 shows the result of the simulation graphically as a histogram:In%5B23%5D:plt.hist(x1%5B-1%5D,bins%3D50)plt.xlabel('value')plt.ylabel('frequency')plt.grid(True)Figure 10-7. Simulated square-root diffusion at maturity (Euler scheme)Figure 10-8then shows the first 10 simulated paths, illustrating the resulting negative,averagef drift (due to x0 %3E %ED%9C%83) and the convergence to %ED%9C%83 %3D 0.02:In%5B24%5D:plt.plot(x1%5B:,:10%5D,lw%3D1.5)plt.xlabel('time')plt.ylabel('index level')plt.grid(True)","Figure 10-8. Simulated square-root diffusion paths (Euler scheme)Let us now get more exact. Equation 10-6presents the exact discretization scheme forthe square-root diffusion based on the noncentral chi-square distribution %CF%87d'2withdf%3D4%CE%B8%CE%BA%CF%832 degrees of freedom and noncentrality parameter nc%3D4%CE%BAe%E2%88%92%CE%BA%CE%94t%CF%8321%E2%88%92e%E2%88%92%CE%BA%CE%94txs.Equation 10-6. Exact discretization for square-root diffusionxt%3D%CF%8321%E2%88%92e%E2%88%92%CE%BA%CE%94t4%CE%BA%CF%87d'24%CE%BAe%E2%88%92%CE%BA%CE%94t%CF%8321%E2%88%92e%E2%88%92%CE%BA%CE%94txsThe Python implementation of this discretization scheme is a bit more involved but stillquite concise:In%5B25%5D:defsrd_exact():x2%3Dnp.zeros((M%2B1,I))x2%5B0%5D%3Dx0fortinrange(1,M%2B1):df%3D4*theta*kappa/sigma**2c%3D(sigma**2*(1-np.exp(-kappa*dt)))/(4*kappa)nc%3Dnp.exp(-kappa*dt)/c*x2%5Bt-1%5Dx2%5Bt%5D%3Dc*npr.noncentral_chisquare(df,nc,size%3DI)returnx2x2%3Dsrd_exact()Figure 10-9 shows the output of the simulation with the exact scheme as a histogram:In%5B26%5D:plt.hist(x2%5B-1%5D,bins%3D50)plt.xlabel('value')plt.ylabel('frequency')plt.grid(True)","Figure 10-9. Simulated square-root diffusion at maturity (exact scheme)Figure 10-10 presents as before the first 10 simulated paths, again displaying the negativeaverage drift and the convergence to %ED%9C%83:In%5B27%5D:plt.plot(x2%5B:,:10%5D,lw%3D1.5)plt.xlabel('time')plt.ylabel('index level')plt.grid(True)Figure 10-10. Simulated square-root diffusion paths (exact scheme)Comparing the main statistics from the different approaches reveals that the biasedEuler scheme indeed performs quite well when it comes to the desired statistical prop%E2%80%90erties:In%5B28%5D:print_statistics(x1%5B-1%5D,x2%5B-1%5D)Out%5B28%5D:      statistic     data set 1     data set 2         ---------------------------------------------                   size      10000.000      10000.000                    min          0.004          0.005","                    max          0.049          0.050                   mean          0.020          0.020                    std          0.006          0.006                   skew          0.529          0.572               kurtosis          0.418          0.503However, a major difference can be observed in terms of execution speed, since samplingfrom the noncentral chi-square distribution is more computationally demanding thanfrom the standard normal distribution. To illustrate this point, consider a larger numberof paths to be simulated:In%5B29%5D:I%3D250000%25timex1%3Dsrd_euler()Out%5B29%5D: CPU times: user 1.02 s, sys: 84 ms, total: 1.11 s         Wall time: 1.11 sIn%5B30%5D:%25timex2%3Dsrd_exact()Out%5B30%5D: CPU times: user 2.26 s, sys: 32 ms, total: 2.3 s         Wall time: 2.3 sThe exact scheme takes roughly twice as much time for virtually the same results as withthe Euler scheme:In%5B31%5D:print_statistics(x1%5B-1%5D,x2%5B-1%5D)x1%3D0.0%3Bx2%3D0.0Out%5B31%5D:      statistic     data set 1     data set 2         ---------------------------------------------                   size     250000.000     250000.000                    min          0.003          0.004                    max          0.069          0.060                   mean          0.020          0.020                    std          0.006          0.006                   skew          0.554          0.578               kurtosis          0.488          0.502","One of the major simplifying assumptions of the Black-Scholes-Merton model is theconstantvolatility. However, volatility in general is neither constant nor deterministic%3Bit is stochastic. Therefore, a major advancement with regard to financial modeling wasachieved in the early 1990s with the introduction of so-called stochastic volatility mod%E2%80%90els. One of the most popular models that fall into that category is that of Heston (1993),which is presented in Equation 10-7.","Equation 10-7. Stochastic differential equations for Heston stochastic volatility modeldSt%3DrStdt%2BvtStdZt1dvt%3D%CE%BAv%CE%B8v%E2%88%92vtdt%2B%CF%83vvtdZt2dZt1dZt2%3D%CF%81The meaning of the single variables and parameters can now be inferred easily from thediscussion of the geometric Brownian motion and the square-root diffusion. The pa%E2%80%90rameter %ED%9C%8C represents the instantaneous correlation between the two standard Brownianmotions Zt1,Zt2. This allows us to account for a stylized fact called the leverage effect,which in essence states that volatility goes up in times of stress (declining markets) andgoes down in times of a bull market (rising markets).Consider the following parameterization of the model:In%5B32%5D:S0%3D100.r%3D0.05v0%3D0.1kappa%3D3.0theta%3D0.25sigma%3D0.1rho%3D0.6T%3D1.0To account for the correlation between the two stochastic processes, we need to deter%E2%80%90mine the Cholesky decomposition of the correlation matrix:In%5B33%5D:corr_mat%3Dnp.zeros((2,2))corr_mat%5B0,:%5D%3D%5B1.0,rho%5Dcorr_mat%5B1,:%5D%3D%5Brho,1.0%5Dcho_mat%3Dnp.linalg.cholesky(corr_mat)In%5B34%5D:cho_matOut%5B34%5D: array(%5B%5B 1. ,  0. %5D,                %5B 0.6,  0.8%5D%5D)Before we start simulating the stochastic processes, we generate the whole set of randomnumbers for both processes, looking to use set 0 for the index process and set 1 for thevolatility process:In%5B35%5D:M%3D50I%3D10000ran_num%3Dnpr.standard_normal((2,M%2B1,I))For the volatility process modeled by the square-root diffusion process type, we use theEuler scheme, taking into account the correlation parameter:","In%5B36%5D:dt%3DT/Mv%3Dnp.zeros_like(ran_num%5B0%5D)vh%3Dnp.zeros_like(v)v%5B0%5D%3Dv0vh%5B0%5D%3Dv0fortinrange(1,M%2B1):ran%3Dnp.dot(cho_mat,ran_num%5B:,t,:%5D)vh%5Bt%5D%3D(vh%5Bt-1%5D%2Bkappa*(theta-np.maximum(vh%5Bt-1%5D,0))*dt%2Bsigma*np.sqrt(np.maximum(vh%5Bt-1%5D,0))*np.sqrt(dt)*ran%5B1%5D)v%3Dnp.maximum(vh,0)For the index level process, we also take into account the correlation and use the exactEuler scheme for the geometric Brownian motion:In%5B37%5D:S%3Dnp.zeros_like(ran_num%5B0%5D)S%5B0%5D%3DS0fortinrange(1,M%2B1):ran%3Dnp.dot(cho_mat,ran_num%5B:,t,:%5D)S%5Bt%5D%3DS%5Bt-1%5D*np.exp((r-0.5*v%5Bt%5D)*dt%2Bnp.sqrt(v%5Bt%5D)*ran%5B0%5D*np.sqrt(dt))This illustrates another advantage of working with the Euler scheme for the square-rootdiffusion: correlation is easily and consistently accounted for since we only draw standardnormally distributed random numbers. There is no simple way of achieving the samewith a mixed approach, using Euler for the index and the noncentral chi square-basedexact approach for the volatility process.Figure 10-11 shows the simulation results as a histogram for both the index level processand the volatility process:In%5B38%5D:fig,(ax1,ax2)%3Dplt.subplots(1,2,figsize%3D(9,5))ax1.hist(S%5B-1%5D,bins%3D50)ax1.set_xlabel('index level')ax1.set_ylabel('frequency')ax1.grid(True)ax2.hist(v%5B-1%5D,bins%3D50)ax2.set_xlabel('volatility')ax2.grid(True)An inspection of the first 10 simulated paths of each process (cf. Figure 10-12) showsthat the volatility process is drifting positively on average and that it, as expected, con%E2%80%90verges to %ED%9C%83v %3D 0.25:In%5B39%5D:fig,(ax1,ax2)%3Dplt.subplots(2,1,sharex%3DTrue,figsize%3D(7,6))ax1.plot(S%5B:,:10%5D,lw%3D1.5)ax1.set_ylabel('index level')ax1.grid(True)ax2.plot(v%5B:,:10%5D,lw%3D1.5)ax2.set_xlabel('time')ax2.set_ylabel('volatility')ax2.grid(True)","Figure 10-11. Simulated stochastic volatility model at maturityFigure 10-12. Simulated stochastic volatility model pathsFinally, let us take a brief look at the statistics for the last point in time for both data sets,showing a pretty high maximum value for the index level process. In fact, this is muchhigher than a geometric Brownian motion with constant volatility could ever climb,ceteris paribus:In%5B40%5D:print_statistics(S%5B-1%5D,v%5B-1%5D)Out%5B40%5D:      statistic     data set 1     data set 2         ---------------------------------------------","                   size      10000.000      10000.000                    min         19.814          0.174                    max        600.080          0.322                   mean        108.818          0.243                    std         52.535          0.020                   skew          1.702          0.151               kurtosis          5.407          0.071","Stochastic volatility and the leverage effect are stylized (empirical) facts found in anumber of markets. Another important stylized empirical fact is the existence of jumpsin asset prices and, for example, volatility. In 1976, Merton published his jump diffusionmodel, enhancing the Black-Scholes-Merton setup by a model component generatingjumps with log-normal distribution. The risk-neutral SDE is presented inEquation 10-8.Equation 10-8. Stochastic differential equation for Merton jump diffusion modeldSt %3D (r %E2%80%93 rJ)Stdt %2B %ED%9C%8EStdZt %2B JtStdNtFor completeness, here is an overview of the variables' and parameters' meaning:StIndex level at date trConstant riskless short raterJ%E2%89%A1%CE%BB%C2%B7e%CE%BCJ%2B%CE%B42/2%E2%88%921Drift correction for jump to maintain risk neutrality%CF%83Constant volatility of SZtStandard Brownian motionJtJump at date t with distribution %E2%80%A6%E2%80%A2%E2%80%A6 log1%2BJt%E2%89%88%ED%90%80log1%2B%CE%BCJ%E2%88%92%CE%B422,%CE%B42 with %E2%80%A6%E2%80%A2%E2%80%A6 "," as the cumulative distribution function of a standard normal randomvariable","NtPoisson process with intensity %ED%9C%86Equation 10-9presents an Euler discretization for the jump diffusion where the ztn arestandard normally distributed and the yt are Poisson distributed with intensity %ED%9C%86.Equation 10-9. Euler discretization for Merton jump diffusion modelSt%3DSt%E2%88%92%CE%94ter%E2%88%92rJ%E2%88%92%CF%832/2%CE%94t%2B%CF%83%CE%94tzt1%2Be%CE%BCJ%2B%CE%B4zt2%E2%88%921ytGiven the discretization scheme, consider the following numerical parameterization:In%5B41%5D:S0%3D100.r%3D0.05sigma%3D0.2lamb%3D0.75mu%3D-0.6delta%3D0.25T%3D1.0To simulate the jump diffusion, we need to generate three sets of (independent) randomnumbers:In%5B42%5D:M%3D50I%3D10000dt%3DT/Mrj%3Dlamb*(np.exp(mu%2B0.5*delta**2)-1)S%3Dnp.zeros((M%2B1,I))S%5B0%5D%3DS0sn1%3Dnpr.standard_normal((M%2B1,I))sn2%3Dnpr.standard_normal((M%2B1,I))poi%3Dnpr.poisson(lamb*dt,(M%2B1,I))fortinrange(1,M%2B1,1):S%5Bt%5D%3DS%5Bt-1%5D*(np.exp((r-rj-0.5*sigma**2)*dt%2Bsigma*np.sqrt(dt)*sn1%5Bt%5D)%2B(np.exp(mu%2Bdelta*sn2%5Bt%5D)-1)*poi%5Bt%5D)S%5Bt%5D%3Dnp.maximum(S%5Bt%5D,0)Since we have assumed a highly negative mean for the jump, it should not come as asurprise that the final values of the simulated index level are more right-skewedinFigure 10-13 compared to a typical log-normal distribution:In%5B43%5D:plt.hist(S%5B-1%5D,bins%3D50)plt.xlabel('value')plt.ylabel('frequency')plt.grid(True)","Figure 10-13. Simulated jump diffusion at maturityThe highly negative jumps can also be found in the first 10 simulated index level paths,as presented in Figure 10-14:In%5B44%5D:plt.plot(S%5B:,:10%5D,lw%3D1.5)plt.xlabel('time')plt.ylabel('index level')plt.grid(True)Figure 10-14. Simulated jump diffusion paths","Not only because of the fact that the Python functions we have used so far generatepseudorandom numbers, but also due to the varying sizes of the samples drawn, resultingsets of numbers might not exhibit statistics really close enough to the expected/desiredones. For example, you would expect a set of standard normally distributed randomnumbers to show a mean of 0 and a standard deviation of 1. Let us check what statistics","4.The described method works for symmetric median 0 random variables only, like standard normally dis%E2%80%90tributed random variables, which we almost exclusively use throughout.different sets of random numbers exhibit. To achieve a realistic comparison, we fix theseed value for the random number generator:In%5B45%5D:print%22%2515s%2515s%22%25('Mean','Std. Deviation')print31*%22-%22foriinrange(1,31,2):npr.seed(1000)sn%3Dnpr.standard_normal(i**2*10000)print%22%2515.12f%2515.12f%22%25(sn.mean(),sn.std())Out%5B45%5D:            Mean  Std. Deviation         -------------------------------         -0.011870394558  1.008752430725         -0.002815667298  1.002729536352         -0.003847776704  1.000594044165         -0.003058113374  1.001086345326         -0.001685126538  1.001630849589         -0.001175212007  1.001347684642         -0.000803969036  1.000159081432         -0.000601970954  0.999506522127         -0.000147787693  0.999571756099         -0.000313035581  0.999646153704         -0.000178447061  0.999677277878          0.000096501709  0.999684346792         -0.000135677013  0.999823841902         -0.000015726986  0.999906493379         -0.000039368519  1.000063091949In%5B46%5D:i**2*10000Out%5B46%5D: 8410000The results show that the statistics %E2%80%9Csomehow%E2%80%9D get better the larger the number of drawsbecomes. But they still do not match the desired ones, even in our largest sample withmore than 8,000,000 random numbers.Fortunately, there are easy-to-implement, generic variance reduction techniques avail%E2%80%90able to improve the matching of the first two moments of the (standard) normal dis%E2%80%90tribution. The first technique is to use antithetic variates. This approach simply drawsonly half the desired number of random draws, and adds the same set of random num%E2%80%90bers with the opposite sign afterward.4For example, if the random number generator(i.e., the respective Python function) draws 0.5, then another number with value %E2%80%930.5is added to the set.With NumPy this is concisely implemented by using the function concatenate:","In%5B47%5D:sn%3Dnpr.standard_normal(10000/2)sn%3Dnp.concatenate((sn,-sn))np.shape(sn)Out%5B47%5D: (10000,)The following repeats the exercise from before, this time using antithetic variates:In%5B48%5D:print%22%2515s%2515s%22%25('Mean','Std. Deviation')print31*%22-%22foriinrange(1,31,2):npr.seed(1000)sn%3Dnpr.standard_normal(i**2*10000/2)sn%3Dnp.concatenate((sn,-sn))print%22%2515.12f%2515.12f%22%25(sn.mean(),sn.std())Out%5B48%5D:            Mean  Std. Deviation         -------------------------------          0.000000000000  1.009653753942         -0.000000000000  1.000413716783          0.000000000000  1.002925061201         -0.000000000000  1.000755212673          0.000000000000  1.001636910076         -0.000000000000  1.000726758438         -0.000000000000  1.001621265149          0.000000000000  1.001203722778         -0.000000000000  1.000556669784          0.000000000000  1.000113464185         -0.000000000000  0.999435175324          0.000000000000  0.999356961431         -0.000000000000  0.999641436845         -0.000000000000  0.999642768905         -0.000000000000  0.999638303451As you immediately notice, this approach corrects the first moment perfectly-whichshould not come as a surprise. This follows from the fact that whenever a number n isdrawn, %E2%80%93n is also added. Since we only have such pairs, the mean is equal to 0 over thewhole set of random numbers. However, this approach does not have any influence onthe second moment, the standard deviation.Using another variance reduction technique, called moment matching, helps correct inone step both the first and second moments:In%5B49%5D:sn%3Dnpr.standard_normal(10000)In%5B50%5D:sn.mean()Out%5B50%5D: -0.001165998295162494In%5B51%5D:sn.std()Out%5B51%5D: 0.99125592020460496","By subtracting the mean from every single random number and dividing every singlenumber by the standard deviation, we get a set of random numbers matching the desiredfirst and second moments of the standard normal distribution (almost) perfectly:In%5B52%5D:sn_new%3D(sn-sn.mean())/sn.std()In%5B53%5D:sn_new.mean()Out%5B53%5D: -2.3803181647963357e-17In%5B54%5D:sn_new.std()Out%5B54%5D: 0.99999999999999989The following function utilizes the insight with regard to variance reduction techniquesand generates standard normal random numbers for process simulation using eithertwo, one, or no variance reduction technique(s):In%5B55%5D:defgen_sn(M,I,anti_paths%3DTrue,mo_match%3DTrue):''' Function to generate random numbers for simulation.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             M : int                 number of time intervals for discretization             I : int                 number of paths to be simulated             anti_paths: Boolean                 use of antithetic variates             mo_math : Boolean                 use of moment matching             '''ifanti_pathsisTrue:sn%3Dnpr.standard_normal((M%2B1,I/2))sn%3Dnp.concatenate((sn,-sn),axis%3D1)else:sn%3Dnpr.standard_normal((M%2B1,I))ifmo_matchisTrue:sn%3D(sn-sn.mean())/sn.std()returnsn","One of the most important applications of Monte Carlo simulation is the valuation ofcontingent claims (options, derivatives, hybrid instruments, etc.). Simply stated, in arisk-neutral world, the value of a contingent claim is the discounted expected payoffunder the risk-neutral (martingale) measure. This is the probability measure that makesall risk factors (stocks, indices, etc.) drift at the riskless short rate. According to theFundamental Theorem of Asset Pricing, the existence of such a probability measure isequivalent to the absence of arbitrage.","A financial option embodies the right to buy (call option) or sell (put option) a specifiedfinancial instrument at a given (maturity) date (European option), or over a specifiedperiod of time (American option), at a given price (the so-called strike price). Let us firstconsider the much simpler case of European options in terms of valuation.","The payoff of a European call option on an index at maturity is given by h(ST) %E2%89%A1max(ST %E2%80%93 K,0), where STis the index level at maturity date T and Kis the strike price.Given a, or in complete markets the, risk-neutral measure for the relevant stochasticprocess (e.g., geometric Brownian motion), the price of such an option is given by theformula in Equation 10-10.Equation 10-10. Pricing by risk-neutral expectation C 0%3De%E2%88%92rT%ED%90%800QhST%3De%E2%88%92rT0%E2%88%9EhsqsdsChapter 9 briefly sketches how to numerically evaluate an integral by Monte Carlosimulation. This approach is used in the following and applied to Equation 10-10.Equation 10-11 provides the respective Monte Carlo estimator for the European option,where S%CB%9CTi is the ith simulated index level at maturity.Equation 10-11. Risk-neutral Monte Carlo estimator C 0%CB%9C%3De%E2%88%92rT1I%E2%88%91i%3D1IhS%CB%9CTiConsider now the following parameterization for the geometric Brownian motion andthe valuation function gbm_mcs_stat, taking as a parameter only the strike price. Here,only the index level at maturity is simulated:In%5B56%5D:S0%3D100.r%3D0.05sigma%3D0.25T%3D1.0I%3D50000defgbm_mcs_stat(K):''' Valuation of European call option in Black-Scholes-Merton             by Monte Carlo simulation (of index level at maturity)             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D","             K : float                 (positive) strike price of the option             Returns             %3D%3D%3D%3D%3D%3D%3D             C0 : float                 estimated present value of European call option             '''sn%3Dgen_sn(1,I)# simulate index level at maturityST%3DS0*np.exp((r-0.5*sigma**2)*T%2Bsigma*np.sqrt(T)*sn%5B1%5D)# calculate payoff at maturityhT%3Dnp.maximum(ST-K,0)# calculate MCS estimatorC0%3Dnp.exp(-r*T)*1/I*np.sum(hT)returnC0As a reference, consider the case with a strike price of K %3D 105:In%5B57%5D:gbm_mcs_stat(K%3D105.)Out%5B57%5D: 10.044221852841922Next, we consider the dynamic simulation approach and allow for European put optionsin addition to the call option. The function gbm_mcs_dyna implements the algorithm:In%5B58%5D:M%3D50defgbm_mcs_dyna(K,option%3D'call'):''' Valuation of European options in Black-Scholes-Merton             by Monte Carlo simulation (of index level paths)             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             K : float                 (positive) strike price of the option             option : string                 type of the option to be valued ('call', 'put')             Returns             %3D%3D%3D%3D%3D%3D%3D             C0 : float                 estimated present value of European call option             '''dt%3DT/M# simulation of index level pathsS%3Dnp.zeros((M%2B1,I))S%5B0%5D%3DS0sn%3Dgen_sn(M,I)fortinrange(1,M%2B1):S%5Bt%5D%3DS%5Bt-1%5D*np.exp((r-0.5*sigma**2)*dt%2Bsigma*np.sqrt(dt)*sn%5Bt%5D)# case-based calculation of payoffifoption%3D%3D'call':","hT%3Dnp.maximum(S%5B-1%5D-K,0)else:hT%3Dnp.maximum(K-S%5B-1%5D,0)# calculation of MCS estimatorC0%3Dnp.exp(-r*T)*1/I*np.sum(hT)returnC0Now, we can compare option price estimates for a call and a put stroke at the same level:In%5B59%5D:gbm_mcs_dyna(K%3D110.,option%3D'call')Out%5B59%5D: 7.9500085250284336In%5B60%5D:gbm_mcs_dyna(K%3D110.,option%3D'put')Out%5B60%5D: 12.629934942682004The question is how well these simulation-based valuation approaches perform relativeto the benchmark value from the Black-Scholes-Merton valuation formula. To find out,let us generate respective option values/estimates for a range of strike prices, using theanalytical option pricing formula for European calls in Black-Scholes-Merton found inthe module BSM_Functions.py:In%5B61%5D:frombsm_functionsimportbsm_call_valuestat_res%3D%5B%5Ddyna_res%3D%5B%5Danal_res%3D%5B%5Dk_list%3Dnp.arange(80.,120.1,5.)np.random.seed(200000)forKink_list:stat_res.append(gbm_mcs_stat(K))dyna_res.append(gbm_mcs_dyna(K))anal_res.append(bsm_call_value(S0,K,T,r,sigma))stat_res%3Dnp.array(stat_res)dyna_res%3Dnp.array(dyna_res)anal_res%3Dnp.array(anal_res)First, we compare the results from the static simulation approach with precise analyticalvalues:In%5B62%5D:fig,(ax1,ax2)%3Dplt.subplots(2,1,sharex%3DTrue,figsize%3D(8,6))ax1.plot(k_list,anal_res,'b',label%3D'analytical')ax1.plot(k_list,stat_res,'ro',label%3D'static')ax1.set_ylabel('European call option value')ax1.grid(True)ax1.legend(loc%3D0)ax1.set_ylim(ymin%3D0)wi%3D1.0ax2.bar(k_list-wi/2,(anal_res-stat_res)/anal_res*100,wi)ax2.set_xlabel('strike')ax2.set_ylabel('difference in %25')ax2.set_xlim(left%3D75,right%3D125)ax2.grid(True)","Figure 10-15 shows the results. All valuation differences are smaller than 1%25 absolutely.There are both negative and positive value differences.Figure 10-15. Comparison of static and dynamic Monte Carlo estimator valuesA similar picture emerges for the dynamic simulation and valuation approach, whoseresults are reported in Figure 10-16. Again, all valuation differences are smaller than1%25, absolutely with both positive and negative deviations. As a general rule, the qualityof the Monte Carlo estimator can be controlled for by adjusting the number of timeintervals M used and/or the number of paths I simulated:In%5B63%5D:fig,(ax1,ax2)%3Dplt.subplots(2,1,sharex%3DTrue,figsize%3D(8,6))ax1.plot(k_list,anal_res,'b',label%3D'analytical')ax1.plot(k_list,dyna_res,'ro',label%3D'dynamic')ax1.set_ylabel('European call option value')ax1.grid(True)ax1.legend(loc%3D0)ax1.set_ylim(ymin%3D0)wi%3D1.0ax2.bar(k_list-wi/2,(anal_res-dyna_res)/anal_res*100,wi)ax2.set_xlabel('strike')ax2.set_ylabel('difference in %25')ax2.set_xlim(left%3D75,right%3D125)ax2.grid(True)","Figure 10-16. Comparison of static and dynamic Monte Carlo estimator values","The valuation of American options is more involved compared to European options.In this case, an optimal stopping problem has to be solved to come up with a fair valueof the option. Equation 10-12formulates the valuation of an American option as sucha problem. The problem formulation is already based on a discrete time grid for usewith numerical simulation. In a sense, it is therefore more correct to speak of an optionvalue given Bermudan exercise. For the time interval converging to zero length, thevalue of the Bermudan option converges to the one of the American option.Equation 10-12. American option prices as optimal stopping problemV0%3Dsup%CF%84%E2%88%880,%CE%94t,2%CE%94t,...,Te%E2%88%92rT%ED%90%800Qh%CF%84S%CF%84The algorithm we describe in the following is called Least-Squares Monte Carlo (LSM)and is from the paper by Longstaff and Schwartz (2001). It can be shown that the valueof an American (Bermudan) option at any given date t is given as Vt(s) %3Dmax(ht(s), C t(s)), where  C ts%3D%ED%90%80tQe%E2%88%92r%CE%94tVt%2B%CE%94tSt%2B%CE%94tSt%3Ds is the so-called continuationvalue of the option given an index level of St %3D s.Consider now that we have simulated Ipaths of the index level over Mtime intervals ofequal size %CE%94t. Define Yt,i%E2%89%A1e%E2%80%93r%ED%9B%A5tVt%2B%ED%9B%A5t,ito be the simulated continuation value for path i attime t. We cannot use this number directly because it would imply perfect foresight.","5.For algorithmic details, refer to Hilpisch (2015).However, we can use the cross section of all such simulated continuation values toestimate the (expected) continuation value by least-squares regression.Given a set of basis functions bd, d %3D 1,%E2%80%A6,D, the continuation value is then given by theregression estimate  C t,i%3D%E2%88%91d%3D1D%CE%B1d,t*%C2%B7bdSt,i, where the optimal regression parameters %ED%9B%BC*are the solution of the least-squares problem stated in Equation 10-13.Equation 10-13. Least-squares regression for American option valuationmin%CE%B11,t,...,%CE%B1D,t1I%E2%88%91i%3D1IYt,i%E2%88%92%E2%88%91d%3D1D%CE%B1d,t%C2%B7bdSt,i2The function gbm_mcs_amer implements the LSM algorithm for both American call andput options:5In%5B64%5D:defgbm_mcs_amer(K,option%3D'call'):''' Valuation of American option in Black-Scholes-Merton             by Monte Carlo simulation by LSM algorithm             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             K : float                 (positive) strike price of the option             option : string                 type of the option to be valued ('call', 'put')             Returns             %3D%3D%3D%3D%3D%3D%3D             C0 : float                 estimated present value of European call option             '''dt%3DT/Mdf%3Dnp.exp(-r*dt)# simulation of index levelsS%3Dnp.zeros((M%2B1,I))S%5B0%5D%3DS0sn%3Dgen_sn(M,I)fortinrange(1,M%2B1):S%5Bt%5D%3DS%5Bt-1%5D*np.exp((r-0.5*sigma**2)*dt%2Bsigma*np.sqrt(dt)*sn%5Bt%5D)# case-based calculation of payoffifoption%3D%3D'call':h%3Dnp.maximum(S-K,0)else:h%3Dnp.maximum(K-S,0)","6.Since we do not assume any dividend payments (having an index in mind), there generally is no early exercisepremium for call options (i.e., no incentive to exercise the option early).# LSM algorithmV%3Dnp.copy(h)fortinrange(M-1,0,-1):reg%3Dnp.polyfit(S%5Bt%5D,V%5Bt%2B1%5D*df,7) C %3Dnp.polyval(reg,S%5Bt%5D)V%5Bt%5D%3Dnp.where( C %3Eh%5Bt%5D,V%5Bt%2B1%5D*df,h%5Bt%5D)# MCS estimatorC0%3Ddf*1/I*np.sum(V%5B1%5D)returnC0In%5B65%5D:gbm_mcs_amer(110.,option%3D'call')Out%5B65%5D: 7.7789332794493156In%5B66%5D:gbm_mcs_amer(110.,option%3D'put')Out%5B66%5D: 13.614023206242445The European value of an option represents a lower bound to the American option'svalue. The difference is generally called the early exercise premium. In what follows, wecompare European and American option values for the same range of strikes as beforeto estimate the option premium. This time we take puts:6In%5B67%5D:euro_res%3D%5B%5Damer_res%3D%5B%5Dk_list%3Dnp.arange(80.,120.1,5.)forKink_list:euro_res.append(gbm_mcs_dyna(K,'put'))amer_res.append(gbm_mcs_amer(K,'put'))euro_res%3Dnp.array(euro_res)amer_res%3Dnp.array(amer_res)Figure 10-17 shows that for the range of strikes chosen the premium can rise to upto 10%25:In%5B68%5D:fig,(ax1,ax2)%3Dplt.subplots(2,1,sharex%3DTrue,figsize%3D(8,6))ax1.plot(k_list,euro_res,'b',label%3D'European put')ax1.plot(k_list,amer_res,'ro',label%3D'American put')ax1.set_ylabel('call option value')ax1.grid(True)ax1.legend(loc%3D0)wi%3D1.0ax2.bar(k_list-wi/2,(amer_res-euro_res)/euro_res*100,wi)ax2.set_xlabel('strike')ax2.set_ylabel('early exercise premium in %25')ax2.set_xlim(left%3D75,right%3D125)ax2.grid(True)","Figure 10-17. Comparison of European and LSM Monte Carlo estimator values","In addition to valuation, risk managementis another important application area of sto%E2%80%90chastic methods and simulation. This section illustrates the calculation/estimation oftwo of the most common risk measures applied today in the finance industry.","Value-at-risk (VaR) is one of the most widely used risk measures, and a much debatedone. Loved by practitioners for its intuitive appeal, it is widely discussed and criticizedby many-mainly on theoretical grounds, with regard to its limited ability to capturewhat is called tail risk (more on this shortly). In words, VaR is a number denoted incurrency units (e.g., USD, EUR, JPY) indicating a loss (of a portfolio, a single position,etc.) that is not exceeded with some confidence level (probability) over a given periodof time.Consider a stock position, worth 1 million USD today, that has a VaR of 50,000 USD ata confidence level of 99%25 over a time period of 30 days (one month). This VaR figuresays that with a probability of 99%25 (i.e., in 99 out of 100 cases), the loss to be expectedover a period of 30 days will not exceed50,000 USD. However, it does not say anythingabout the size of the loss once a loss beyond 50,000 USD occurs-i.e., if the maximumloss is 100,000 or 500,000 USD what the probability of such a specific %E2%80%9Chigher than VaRloss%E2%80%9D is. All it says is that there is a 1%25 probability that a loss of a minimum of 50,000USD or higher will occur.","Assume again that we are in a Black-Scholes-Merton setup and consider the followingparameterization and simulation of index levels at a future date T %3D 30/365 (i.e., weassume a period of 30 days):In%5B69%5D:S0%3D100r%3D0.05sigma%3D0.25T%3D30/365.I%3D10000ST%3DS0*np.exp((r-0.5*sigma**2)*T%2Bsigma*np.sqrt(T)*npr.standard_normal(I))To estimate VaR figures, we need the simulated absolute profits and losses relative tothe value of the position today in a sorted manner, i.e., from the severest loss to thelargest profit:In%5B70%5D:R_gbm%3Dnp.sort(ST-S0)Figure 10-18 shows the histogram of the simulated absolute performance values:In%5B71%5D:plt.hist(R_gbm,bins%3D50)plt.xlabel('absolute return')plt.ylabel('frequency')plt.grid(True)Figure 10-18. Absolute returns of geometric Brownian motion (30d)Having the ndarray object with the sorted results, the function scoreatpercentilealready does the trick. All we have to do is to define the percentiles (in percent values)in which we are interested. In the list object percs, 0.1 translates into a confidencelevel of 100%25 %E2%80%93 0.1%25 %3D 99.9%25. The 30-day VaR given a confidence level of 99.9%25 in thiscase is 20.2 currency units, while it is 8.9 at the 90%25 confidence level:In%5B72%5D:percs%3D%5B0.01,0.1,1.,2.5,5.0,10.0%5Dvar%3Dscs.scoreatpercentile(R_gbm,percs)print%22%2516s%2516s%22%25('Confidence Level','Value-at-Risk')print33*%22-%22","forpairinzip(percs,var):print%22%2516.2f%2516.3f%22%25(100-pair%5B0%5D,-pair%5B1%5D)Out%5B72%5D: Confidence Level    Value-at-Risk         ---------------------------------                    99.99           26.072                    99.90           20.175                    99.00           15.753                    97.50           13.265                    95.00           11.298                    90.00            8.942As a second example, recall the jump diffusion setup from Merton, which we want tosimulate dynamically:In%5B73%5D:dt%3D30./365/Mrj%3Dlamb*(np.exp(mu%2B0.5*delta**2)-1)S%3Dnp.zeros((M%2B1,I))S%5B0%5D%3DS0sn1%3Dnpr.standard_normal((M%2B1,I))sn2%3Dnpr.standard_normal((M%2B1,I))poi%3Dnpr.poisson(lamb*dt,(M%2B1,I))fortinrange(1,M%2B1,1):S%5Bt%5D%3DS%5Bt-1%5D*(np.exp((r-rj-0.5*sigma**2)*dt%2Bsigma*np.sqrt(dt)*sn1%5Bt%5D)%2B(np.exp(mu%2Bdelta*sn2%5Bt%5D)-1)*poi%5Bt%5D)S%5Bt%5D%3Dnp.maximum(S%5Bt%5D,0)In%5B74%5D:R_jd%3Dnp.sort(S%5B-1%5D-S0)In this case, with the jump component having a negative mean, we see something likea bimodal distribution for the simulated profits/losses in Figure 10-19. From a normaldistribution point of view, we have a strongly pronounced left fat tail:In%5B75%5D:plt.hist(R_jd,bins%3D50)plt.xlabel('absolute return')plt.ylabel('frequency')plt.grid(True)","Figure 10-19. Absolute returns of jump diffusion (30d)For this process and parameterization, the VaR over 30 days at the 90%25 level is almostidentical, while it is more than three times as high at the 99.9%25 level as with the geometricBrownian motion (71.8 vs. 20.2 currency units):In%5B76%5D:percs%3D%5B0.01,0.1,1.,2.5,5.0,10.0%5Dvar%3Dscs.scoreatpercentile(R_jd,percs)print%22%2516s%2516s%22%25('Confidence Level','Value-at-Risk')print33*%22-%22forpairinzip(percs,var):print%22%2516.2f%2516.3f%22%25(100-pair%5B0%5D,-pair%5B1%5D)Out%5B76%5D: Confidence Level    Value-at-Risk         ---------------------------------                    99.99           75.029                    99.90           71.833                    99.00           55.901                    97.50           45.697                    95.00           25.993                    90.00            8.773This illustrates the problem of capturing the tail risk so often encountered in financialmarkets by the standard VaR measure.To further illustrate the point, we lastly show the VaR measures for both cases in directcomparison graphically. As Figure 10-20 reveals, the VaR measures behave completelydifferently given a range of typical confidence levels:In%5B77%5D:percs%3Dlist(np.arange(0.0,10.1,0.1))gbm_var%3Dscs.scoreatpercentile(R_gbm,percs)jd_var%3Dscs.scoreatpercentile(R_jd,percs)In%5B78%5D:plt.plot(percs,gbm_var,'b',lw%3D1.5,label%3D'GBM')plt.plot(percs,jd_var,'r',lw%3D1.5,label%3D'JD')plt.legend(loc%3D4)plt.xlabel('100 - confidence level %5B%25%5D')plt.ylabel('value-at-risk')","plt.grid(True)plt.ylim(ymax%3D0.0)Figure 10-20. Value-at-risk for geometric Brownian motion and jump diffusion","Other important risk measures are the credit value-at-risk (CVaR) and the credit valueadjustment (CVA), which is derived from the CVaR. Roughly speaking, CVaR is ameasure for the risk resulting from the possibility that a counterparty might not be ableto honor its obligations-for example, if the counterparty goes bankrupt. In such a casethere are two main assumptions to be made: probability of default and the (average) losslevel.To make it specific, consider again the benchmark setup of Black-Scholes-Merton withthe following parameterization:In%5B79%5D:S0%3D100.r%3D0.05sigma%3D0.2T%3D1.I%3D100000ST%3DS0*np.exp((r-0.5*sigma**2)*T%2Bsigma*np.sqrt(T)*npr.standard_normal(I))In the simplest case, one considers a fixed (average) loss level L and a fixed probabilityp for default (per year) of a counterparty:In%5B80%5D:L%3D0.5In%5B81%5D:p%3D0.01Using the Poisson distribution, default scenarios are generated as follows, taking intoaccount that a default can only occur once:In%5B82%5D:D%3Dnpr.poisson(p*T,I)D%3Dnp.where(D%3E1,1,D)","Without default, the risk-neutral value of the future index level should be equal to thecurrent value of the asset today (up to differences resulting from numerical errors):In%5B83%5D:np.exp(-r*T)*1/I*np.sum(ST)Out%5B83%5D: 99.981825216842921The CVaR under our assumptions is calculated as follows:In%5B84%5D:CVaR%3Dnp.exp(-r*T)*1/I*np.sum(L*D*ST)CVaROut%5B84%5D: 0.5152011134161355Analogously, the present value of the asset, adjusted for the credit risk, is given as follows:In%5B85%5D:S0_CVA%3Dnp.exp(-r*T)*1/I*np.sum((1-L*D)*ST)S0_CVAOut%5B85%5D: 99.466624103426781This should be (roughly) the same as subtracting the CVaR value from the current assetvalue:In%5B86%5D:S0_adj%3DS0-CVaRS0_adjOut%5B86%5D: 99.48479888658386In this particular simulation example, we observe roughly 1,000 losses due to credit risk,which is to be expected given the assumed default probability of 1%25 and 100,000 simu%E2%80%90lated paths:In%5B87%5D:np.count_nonzero(L*D*ST)Out%5B87%5D: 1031Figure 10-21shows the complete frequency distribution of the losses due to a default.Of course, in the large majority of cases (i.e., in about 99,000 of the 100,000 cases) thereis no loss to observe:In%5B88%5D:plt.hist(L*D*ST,bins%3D50)plt.xlabel('loss')plt.ylabel('frequency')plt.grid(True)plt.ylim(ymax%3D175)","Figure 10-21. Losses due to risk-neutrally expected default (stock)Consider now the case of a European call option. Its value is about 10.4 currency unitsat a strike of 100:In%5B89%5D:K%3D100.hT%3Dnp.maximum(ST-K,0)C0%3Dnp.exp(-r*T)*1/I*np.sum(hT)C0Out%5B89%5D: 10.427336109660052The CVaR is about 5 cents given the same assumptions with regard to probability ofdefault and loss level:In%5B90%5D:CVaR%3Dnp.exp(-r*T)*1/I*np.sum(L*D*hT)CVaROut%5B90%5D: 0.053822578452208093Accordingly, the adjusted option value is roughly 5 cents lower:In%5B91%5D:C0_CVA%3Dnp.exp(-r*T)*1/I*np.sum((1-L*D)*hT)C0_CVAOut%5B91%5D: 10.373513531207843Compared to the case of a regular asset, the option case has somewhat different char%E2%80%90acteristics. We only see a little more than 500 losses due to a default, although we againhave about 1,000 defaults. This results from the fact that the payoff of the option atmaturity has a high probability of being zero:In%5B92%5D:np.count_nonzero(L*D*hT)# number of lossesOut%5B92%5D: 582In%5B93%5D:np.count_nonzero(D)# number of defaultsOut%5B93%5D: 1031In%5B94%5D:I-np.count_nonzero(hT)# zero payoff","Out%5B94%5D: 43995Figure 10-22shows that the CVaR for the option has a completely different frequencydistribution compared to the regular asset case:In%5B95%5D:plt.hist(L*D*hT,bins%3D50)plt.xlabel('loss')plt.ylabel('frequency')plt.grid(True)plt.ylim(ymax%3D350)Figure 10-22. Losses due to risk-neutrally expected default (call option)","This chapter deals with methods and techniques important to the application of MonteCarlo simulation in finance. In particular, it shows how to generate (pseudo)randomnumbers based on different distribution laws. It proceeds with the simulation of randomvariables and stochastic processes, which is important in many financial areas. Two ap%E2%80%90plication areas are discussed in some depth in this chapter: valuation of optionswithEuropean and American exercise and the estimation of risk measures like value-at-riskand credit value adjustments.The chapter illustrates that Pythonin combination with NumPyis well suited to imple%E2%80%90menting even such computationally demanding tasks as the valuation of American op%E2%80%90tions by Monte Carlo simulation. This is mainly due to the fact that the majority offunctions and classes of NumPy are implemented in  C , which leads to considerable speedadvantages in general over pure Pythoncode. A further benefit is the compactness andreadability of the resulting code due to vectorized operations.","The original article introducing Monte Carlo simulation to finance is:","%E2%80%A2Boyle, Phelim (1977): %E2%80%9COptions: A Monte Carlo Approach.%E2%80%9D Journal of FinancialEconomics, Vol. 4, No. 4, pp. 322%E2%80%93338.Other original papers cited in this chapter are (see also Chapter 16):%E2%80%A2Black, Fischer and Myron Scholes (1973): %E2%80%9CThe Pricing of Options and CorporateLiabilities.%E2%80%9D Journal of Political Economy, Vol. 81, No. 3, pp. 638%E2%80%93659.%E2%80%A2Cox, John, Jonathan Ingersoll and Stephen Ross (1985): %E2%80%9CA Theory of the TermStructure of Interest Rates.%E2%80%9D Econometrica, Vol. 53, No. 2, pp. 385%E2%80%93407.%E2%80%A2Heston, Steven (1993): %E2%80%9CA Closed-From Solution for Options with Stochastic Vol%E2%80%90atility with Applications to Bond and Currency Options.%E2%80%9D The Review of FinancialStudies, Vol. 6, No. 2, 327%E2%80%93343.%E2%80%A2Merton, Robert (1973): %E2%80%9CTheory of Rational Option Pricing.%E2%80%9D Bell Journal of Eco%E2%80%90nomics and Management Science, Vol. 4, pp. 141%E2%80%93183.%E2%80%A2Merton, Robert (1976): %E2%80%9COption Pricing When the Underlying Stock Returns AreDiscontinuous.%E2%80%9D Journal of Financial Economics, Vol. 3, No. 3, pp. 125%E2%80%93144.The books by Glassermann (2004) and Hilpisch (2015) cover all topics of this chapterin depth (however, the first one does not cover any technical implementation details):%E2%80%A2Glasserman, Paul (2004): Monte Carlo Methods in Financial Engineering. Springer,New York.%E2%80%A2Hilpisch, Yves (2015): Derivatives Analytics with Python. Wiley Finance, Chiches%E2%80%90ter, England. http://www.derivatives-analytics-with-python.com.It took until the turn of the century for an efficient method to value American optionsby Monte Carlo simulation to finally be published:%E2%80%A2Longstaff, Francis and Eduardo Schwartz (2001): %E2%80%9CValuing American Options bySimulation: A Simple Least Squares Approach.%E2%80%9D Review of Financial Studies, Vol.14, No. 1, pp. 113%E2%80%93147.A broad and in-depth treatment of credit risk is provided in:%E2%80%A2Duffie, Darrell and Kenneth Singleton (2003): Credit Risk-Pricing, Measurement,and Management. Princeton University Press, Princeton, NJ.","I can prove anything by statistics except the truth.- George CanningStatistics is a vast field. The tools and results the field provides have become indispen%E2%80%90sible for finance. This also explains the popularity of domain-specific languages like Rin the finance industry. The more elaborate and complex statistical models become, themore important it is to have available easy-to-use and high-performing computationalsolutions.A single chapter in a book like this one cannot do justice to the richness and the broad%E2%80%90ness of the field of statistics. Therefore, the approach-as in many other chapters-is tofocus on selected topics that seem of paramount importance or that provide a goodstarting point when it comes to the use of Pythonfor the particular tasks at hand. Thechapter has four focal points:Normality testsA large number of important financial models, like the mean-variance portfoliotheory and the capital asset pricing model (CAPM), rest on the assumption thatreturns of securities are normally distributed%3B therefore, this chapter presents someapproaches to test a given time series for normality of returns.Portfolio theoryModern portfolio theory (MPT) can be considered one of the biggest successes ofstatistics in finance%3B starting in the early 1950s with the work of pioneer HarryMarkowitz, this theory began to replace people's reliance on judgment and expe%E2%80%90rience with rigorous mathematical and statistical methods when it comes to theinvestment of money in financial markets. In that sense, it is maybe the first realquantitative approach in finance.","Principal component analysisPrincipal component analysis (PCA) is quite a popular tool in finance, for example,when it comes to implementing equity investment strategies or analyzing the prin%E2%80%90cipal components that explain the movement in interest rates. Its major benefit is%E2%80%9Ccomplexity reduction,%E2%80%9D achieved by deriving a small set of linearly independent(noncorrelated, orthogonal) components from a potentially large set of maybehighly correlated time series components%3B we illustrate the application based on theGerman DAX index and the 30 stocks contained in that index.Bayesian regressionOn a fundamental level, Bayesian statistics introduces the notion of beliefsof agentsand the updating of beliefsto statistics%3B when it comes to linear regression, for ex%E2%80%90ample, this might take on the form of having a statistical distribution for regressionparameters instead of single point estimates (e.g., for the intercept and slope of theregression line). Nowadays, Bayesian methods are rather popular and important infinance, which is why we illustrate some (advanced) applications in this chapter.Many aspects in this chapter relate to date and/or time information. Refer to Appendix Cfor an overview of handling such data with Python, NumPy, and pandas.","The normal distributioncan be considered the most important distribution in financeand one of the major statistical building blocks of financial theory. Among others, thefollowing cornerstones of financial theory rest to a large extent on the normal distri%E2%80%90bution of stock market returns:Portfolio theoryWhen stock returns are normally distributed, optimal portfolio choice can be castinto a setting where only the mean return and the variance of the returns(or thevolatility) as well as the covariances between different stocks are relevant for aninvestment decision (i.e., an optimal portfolio composition).Capital asset pricing modelAgain, when stock returns are normally distributed, prices of single stocks can beelegantly expressed in relationship to a broad market index%3B the relationship isgenerally expressed by a measure for the comovement of a single stock with themarket index called beta (%ED%9B%BD).Efficient markets hypothesisAn efficient market is a market where prices reflect all available information, where%E2%80%9Call%E2%80%9D can be defined more narrowly or more widely (e.g., as in %E2%80%9Call publicly available%E2%80%9Dinformation vs. including also %E2%80%9Conly privately available%E2%80%9D information)%3B if this hy%E2%80%90pothesis holds true, then stock prices fluctuate randomly and returns are normallydistributed.","Option pricing theoryBrownian motion is thestandard and benchmark model for the modeling of ran%E2%80%90dom stock (and other security) price movements%3B the famous Black-Scholes-Merton option pricing formula uses a geometric Brownian motion as the modelfor a stock's random fluctuations over time, leading to normally distributed returns.This by far nonexhaustive list underpins the importance of the normality assumptionin finance.","To set the stage for further analyses, we start with the geometric Brownian motion asone of the canonical stochastic processes used in financial modeling. The following canbe said about the characteristics of paths from a geometric Brownian motion S:Normal log returnsLog returns logStSs%3DlogSt%E2%88%92logSs between two times 0 %3C s%3C tare normallydistributed.Log-normal valuesAt any time t %3E 0, the values St are log-normally distributed.For what follows we need a number of Python libraries, including scipy.statsandstatsmodels.api:In%5B1%5D:importnumpyasnpnp.random.seed(1000)importscipy.statsasscsimportstatsmodels.apiassmimportmatplotlibasmplimportmatplotlib.pyplotasplt%25matplotlibinlineLet us define a function to generate Monte Carlo paths for the geometric Brownianmotion (see also Chapter 10):In%5B2%5D:defgen_paths(S0,r,sigma,T,M,I):''' Generates Monte Carlo paths for geometric Brownian motion.            Parameters            %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D            S0 : float                initial stock/index value            r : float                constant short rate            sigma : float                constant volatility            T : float                final time horizon            M : int","                number of time steps/intervals            I : int                number of paths to be simulated            Returns            %3D%3D%3D%3D%3D%3D%3D            paths : ndarray, shape (M %2B 1, I)                simulated paths given the parameters            '''dt%3Dfloat(T)/Mpaths%3Dnp.zeros((M%2B1,I),np.float64)paths%5B0%5D%3DS0fortinrange(1,M%2B1):rand%3Dnp.random.standard_normal(I)rand%3D(rand-rand.mean())/rand.std()paths%5Bt%5D%3Dpaths%5Bt-1%5D*np.exp((r-0.5*sigma**2)*dt%2Bsigma*np.sqrt(dt)*rand)returnpathsThe following is a possible parameterization for the Monte Carlo simulation, generating,in combination with the function gen_paths, 250,000 paths with 50 time steps each:In%5B3%5D:S0%3D100.r%3D0.05sigma%3D0.2T%3D1.0M%3D50I%3D250000In%5B4%5D:paths%3Dgen_paths(S0,r,sigma,T,M,I)Figure 11-1 shows the first 10 simulated paths from the simulation:In%5B5%5D:plt.plot(paths%5B:,:10%5D)plt.grid(True)plt.xlabel('time steps')plt.ylabel('index level')Our main interest is in the distribution of the log returns. The following code generatesan ndarray object with all log returns:In%5B6%5D:log_returns%3Dnp.log(paths%5B1:%5D/paths%5B0:-1%5D)","Figure 11-1. Ten simulated paths of geometric Brownian motionConsider the very first simulated path over the 50 time steps:In%5B7%5D:paths%5B:,0%5D.round(4)Out%5B7%5D: array(%5B 100.    ,   97.821 ,   98.5573,  106.1546,  105.899 ,   99.8363,                100.0145,  102.6589,  105.6643,  107.1107,  108.7943,  108.2449,                106.4105,  101.0575,  102.0197,  102.6052,  109.6419,  109.5725,                112.9766,  113.0225,  112.5476,  114.5585,  109.942 ,  112.6271,                112.7502,  116.3453,  115.0443,  113.9586,  115.8831,  117.3705,                117.9185,  110.5539,  109.9687,  104.9957,  108.0679,  105.7822,                105.1585,  104.3304,  108.4387,  105.5963,  108.866 ,  108.3284,                107.0077,  106.0034,  104.3964,  101.0637,   98.3776,   97.135 ,                 95.4254,   96.4271,   96.3386%5D)A log-return series for a simulated path might then take on the form:In%5B8%5D:log_returns%5B:,0%5D.round(4)Out%5B8%5D: array(%5B-0.022 ,  0.0075,  0.0743, -0.0024, -0.059 ,  0.0018,  0.0261,                0.0289,  0.0136,  0.0156, -0.0051, -0.0171, -0.0516,  0.0095,                0.0057,  0.0663, -0.0006,  0.0306,  0.0004, -0.0042,  0.0177,               -0.0411,  0.0241,  0.0011,  0.0314, -0.0112, -0.0095,  0.0167,                0.0128,  0.0047, -0.0645, -0.0053, -0.0463,  0.0288, -0.0214,               -0.0059, -0.0079,  0.0386, -0.0266,  0.0305, -0.0049, -0.0123,               -0.0094, -0.0153, -0.0324, -0.0269, -0.0127, -0.0178,  0.0104,               -0.0009%5D)This is something one might experience in financial markets as well: days when youmake a positive returnon your investment and other days when you are losing moneyrelative to your most recent wealth position.The function print_statisticsis a wrapper function for the describe function fromthe scipy.stats sublibrary. It mainly generates a more (human-)readable output forsuch statistics as the mean, the skewness, or the kurtosis of a given (historical or simu%E2%80%90lated) data set:","In%5B9%5D:defprint_statistics(array):''' Prints selected statistics.            Parameters            %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D            array: ndarray                object to generate statistics on            '''sta%3Dscs.describe(array)print%22%2514s%2515s%22%25('statistic','value')print30*%22-%22print%22%2514s%2515.5f%22%25('size',sta%5B0%5D)print%22%2514s%2515.5f%22%25('min',sta%5B1%5D%5B0%5D)print%22%2514s%2515.5f%22%25('max',sta%5B1%5D%5B1%5D)print%22%2514s%2515.5f%22%25('mean',sta%5B2%5D)print%22%2514s%2515.5f%22%25('std',np.sqrt(sta%5B3%5D))print%22%2514s%2515.5f%22%25('skew',sta%5B4%5D)print%22%2514s%2515.5f%22%25('kurtosis',sta%5B5%5D)For example, the following shows the function in action, using a flattened version of thendarray object containing the log returns. The method flatten returns a 1D array withall the data given in a multidimensional array:In%5B10%5D:print_statistics(log_returns.flatten())Out%5B10%5D:      statistic           value         ------------------------------                   size  12500000.00000                    min        -0.15664                    max         0.15371                   mean         0.00060                    std         0.02828                   skew         0.00055               kurtosis         0.00085The data set in this case consists of 12,500,000 data points with the values mainly lyingbetween %2B/%E2%80%93 0.15. We would expect annualized values of 0.05 for the mean return and0.2 for the standard deviation (volatility). The annualized values of the data set comeclose to these values, if not matching them perfectly (multiply the mean value by 50 andthe standard deviation by 50).Figure 11-2compares the frequency distribution of the simulated log returns with theprobability density function (pdf) of the normal distribution given the parameteriza%E2%80%90tions for r and sigma. The function used is norm.pdffrom the scipy.statssublibrary.There is obviously quite a good fit:In%5B11%5D:plt.hist(log_returns.flatten(),bins%3D70,normed%3DTrue,label%3D'frequency')plt.grid(True)plt.xlabel('log-return')plt.ylabel('frequency')x%3Dnp.linspace(plt.axis()%5B0%5D,plt.axis()%5B1%5D)plt.plot(x,scs.norm.pdf(x,loc%3Dr/M,scale%3Dsigma/np.sqrt(M)),","'r',lw%3D2.0,label%3D'pdf')plt.legend()Figure 11-2. Histogram of log returns and normal density functionComparing a frequency distribution (histogram) with a theoretical pdf is not the onlyway to graphically %E2%80%9Ctest%E2%80%9D for normality. So-called quantile-quantile plots(qq plots) arealso well suited for this task. Here, sample quantile values are compared to theoreticalquantile values. For normally distributed sample data sets, such a plot might look likeFigure 11-3, with the absolute majority of the quantile values (dots) lying on astraight line:In%5B12%5D:sm.qqplot(log_returns.flatten()%5B::500%5D,line%3D's')plt.grid(True)plt.xlabel('theoretical quantiles')plt.ylabel('sample quantiles')Figure 11-3. Quantile-quantile plot for log returns","However appealing the graphical approaches might be, they generally cannot replacemore rigorous testing procedures. The function normality_testscombines three dif%E2%80%90ferent statistical tests:Skewness test (skewtest)This tests whether the skew of the sample data is %E2%80%9Cnormal%E2%80%9D (i.e., has a value closeenough to zero).Kurtosis test (kurtosistest)Similarly, this tests whether the kurtosis of the sample data is %E2%80%9Cnormal%E2%80%9D (again, closeenough to zero).Normality test (normaltest)This combines the other two test approaches to test for normality.We define this function as follows:In%5B13%5D:defnormality_tests(arr):''' Tests for normality distribution of given data set.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             array: ndarray                 object to generate statistics on             '''print%22Skew of data set  %2514.3f%22%25scs.skew(arr)print%22Skew test p-value %2514.3f%22%25scs.skewtest(arr)%5B1%5Dprint%22Kurt of data set  %2514.3f%22%25scs.kurtosis(arr)print%22Kurt test p-value %2514.3f%22%25scs.kurtosistest(arr)%5B1%5Dprint%22Norm test p-value %2514.3f%22%25scs.normaltest(arr)%5B1%5DThe test values indicate that the log returns are indeed normally distributed-i.e., theyshow p-values of 0.05 or above:In%5B14%5D:normality_tests(log_returns.flatten())Out%5B14%5D: Skew of data set           0.001         Skew test p-value          0.430         Kurt of data set           0.001         Kurt test p-value          0.541         Norm test p-value          0.607Finally, let us check whether the end-of-period values are indeed log-normally dis%E2%80%90tributed. This boils down to a normality test as well, since we only have to transformthe data by applying the log function to it (to then arrive at normally distributed data-or maybe not). Figure 11-4 plots both the log-normally distributed end-of-period valuesand the transformed ones (%E2%80%9Clog index level%E2%80%9D):In%5B15%5D:f,(ax1,ax2)%3Dplt.subplots(1,2,figsize%3D(9,4))ax1.hist(paths%5B-1%5D,bins%3D30)ax1.grid(True)ax1.set_xlabel('index level')","ax1.set_ylabel('frequency')ax1.set_title('regular data')ax2.hist(np.log(paths%5B-1%5D),bins%3D30)ax2.grid(True)ax2.set_xlabel('log index level')ax2.set_title('log data')Figure 11-4. Histogram of simulated end-of-period index levelsThe statistics for the data set show expected behavior-for example, a mean value closeto 105 and a standard deviation (volatility) close to 20%25:In%5B16%5D:print_statistics(paths%5B-1%5D)Out%5B16%5D:      statistic           value         ------------------------------                   size    250000.00000                    min        42.74870                    max       233.58435                   mean       105.12645                    std        21.23174                   skew         0.61116               kurtosis         0.65182The log index level values also have skew and kurtosis values close to zero:In%5B17%5D:print_statistics(np.log(paths%5B-1%5D))Out%5B17%5D:      statistic           value         ------------------------------                   size    250000.00000                    min         3.75534                    max         5.45354                   mean         4.63517                    std         0.19998                   skew        -0.00092               kurtosis        -0.00327","This data set also shows high p-values, providing strong support for the normal distri%E2%80%90bution hypothesis:In%5B18%5D:normality_tests(np.log(paths%5B-1%5D))Out%5B18%5D: Skew of data set          -0.001         Skew test p-value          0.851         Kurt of data set          -0.003         Kurt test p-value          0.744         Norm test p-value          0.931Figure 11-5 compares again the frequency distribution with the pdf of the normal dis%E2%80%90tribution, showing a pretty good fit (as now is, of course, to be expected):In%5B19%5D:log_data%3Dnp.log(paths%5B-1%5D)plt.hist(log_data,bins%3D70,normed%3DTrue,label%3D'observed')plt.grid(True)plt.xlabel('index levels')plt.ylabel('frequency')x%3Dnp.linspace(plt.axis()%5B0%5D,plt.axis()%5B1%5D)plt.plot(x,scs.norm.pdf(x,log_data.mean(),log_data.std()),'r',lw%3D2.0,label%3D'pdf')plt.legend()Figure 11-5. Histogram of log index levels and normal density functionFigure 11-6 also supports the hypothesis that the log index levels are normallydistributed:In%5B20%5D:sm.qqplot(log_data,line%3D's')plt.grid(True)plt.xlabel('theoretical quantiles')plt.ylabel('sample quantiles')","Figure 11-6. Quantile-quantile plot for log index levels","Thenormalityassumptionwithregardtoreturnsofsecuritiesiscen%E2%80%90traltoanumberofimportantfinancialtheories.Pythonprovidesefficientstatisticalandgraphicalmeanstotestwhethertimeseriesdata is normally distributed or not.","We are now pretty well equipped to attack real-world data and see how the normalityassumption does beyond the financial laboratory. We are going to analyze four historicaltime series: two stock indices (the German DAX index and the American S%26P 500 index)and two stocks (Yahoo! Inc. and Microsoft Inc.). The data management tool of choiceis pandas (cf. Chapter 6), so we begin with a few imports:In%5B21%5D:importpandasaspdimportpandas.io.dataaswebHere are the symbols for the time series we are interested in. The curious reader mightof course replace these with any other symbol of interest:In%5B22%5D:symbols%3D%5B'%5EGDAXI','%5EGSPC','YHOO','MSFT'%5DThe following reads only the Adj Closetime series data into a single DataFrame objectfor all symbols:In%5B23%5D:data%3Dpd.DataFrame()forsyminsymbols:data%5Bsym%5D%3Dweb.DataReader(sym,data_source%3D'yahoo',start%3D'1/1/2006')%5B'Adj Close'%5Ddata%3Ddata.dropna()In%5B24%5D:data.info()","Out%5B24%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E         DatetimeIndex: 2179 entries, 2006-01-03 00:00:00 to 2014-09-26 00:00:00         Data columns (total 4 columns):         %5EGDAXI    2179 non-null float64         %5EGSPC     2179 non-null float64         YHOO      2179 non-null float64         MSFT      2179 non-null float64         dtypes: float64(4)The four time series start at rather different absolute values:In%5B25%5D:data.head()Out%5B25%5D:              %5EGDAXI    %5EGSPC   YHOO   MSFT         Date         2006-01-03  5460.68  1268.80  40.91  22.09         2006-01-04  5523.62  1273.46  40.97  22.20         2006-01-05  5516.53  1273.48  41.53  22.22         2006-01-06  5536.32  1285.45  43.21  22.15         2006-01-09  5537.11  1290.15  43.42  22.11Figure 11-7 shows therefore the four time series in direct comparison, but normalizedto a starting value of 100:In%5B26%5D:(data/data.ix%5B0%5D*100).plot(figsize%3D(8,6))Figure 11-7. Evolution of stock and index levels over timeCalculating the log returns with pandas is a bit more convenient than with NumPy, sincewe can use the shift method:In%5B27%5D:log_returns%3Dnp.log(data/data.shift(1))log_returns.head()","Out%5B27%5D:               %5EGDAXI     %5EGSPC      YHOO      MSFT         Date         2006-01-03       NaN       NaN       NaN       NaN         2006-01-04  0.011460  0.003666  0.001466  0.004967         2006-01-05 -0.001284  0.000016  0.013576  0.000900         2006-01-06  0.003581  0.009356  0.039656 -0.003155         2006-01-09  0.000143  0.003650  0.004848 -0.001808Figure 11-8 provides all log returns in the form of histograms. Although not easy tojudge, one can guess that these frequency distributions might not be normal:In%5B28%5D:log_returns.hist(bins%3D50,figsize%3D(9,6))Figure 11-8. Histogram of respective log returnsAs a next step, consider the different statistics for the time series data sets. The kurtosisvalues seem to be especially far from normal for all four data sets:In%5B29%5D:forsyminsymbols:print%22%5CnResults for symbol %25s%22%25symprint30*%22-%22log_data%3Dnp.array(log_returns%5Bsym%5D.dropna())print_statistics(log_data)Out%5B29%5D: Results for symbol %5EGDAXI         ------------------------------              statistic           value         ------------------------------                   size      2178.00000                    min        -0.07739                    max         0.10797                   mean         0.00025                    std         0.01462","                   skew         0.02573               kurtosis         6.52461         Results for symbol %5EGSPC         ------------------------------              statistic           value         ------------------------------                   size      2178.00000                    min        -0.09470                    max         0.10957                   mean         0.00020                    std         0.01360                   skew        -0.32017               kurtosis        10.05425         Results for symbol YHOO         ------------------------------              statistic           value         ------------------------------                   size      2178.00000                    min        -0.24636                    max         0.39182                   mean        -0.00000                    std         0.02620                   skew         0.56530               kurtosis        31.98659         Results for symbol MSFT         ------------------------------              statistic           value         ------------------------------                   size      2178.00000                    min        -0.12476                    max         0.17039                   mean         0.00034                    std         0.01792                   skew         0.04262               kurtosis        10.18038We will inspect the data of two symbols via a qq plot. Figure 11-9shows the qq plot forthe S%26P 500. Obviously, the sample quantile values do not lie on a straight line, indi%E2%80%90cating %E2%80%9Cnonnormality.%E2%80%9D On the left and right sides there are many values that lie wellbelow the line and well above the line, respectively. In other words, the time series dataexhibits fat tails. This term refers to a (frequency) distribution where negative and pos%E2%80%90itive outliers are observed far more often than a normal distribution would imply. Thecode to generate this plot is as follows:In%5B30%5D:sm.qqplot(log_returns%5B'%5EGSPC'%5D.dropna(),line%3D's')plt.grid(True)plt.xlabel('theoretical quantiles')plt.ylabel('sample quantiles')","Figure 11-9. Quantile-quantile plot for S%26P 500 log returnsThe same conclusions can be drawn from Figure 11-10, presenting the data for theMicrosoft Inc. stock. There also seems to be strong evidence for a fat-tailed distribution:In%5B31%5D:sm.qqplot(log_returns%5B'MSFT'%5D.dropna(),line%3D's')plt.grid(True)plt.xlabel('theoretical quantiles')plt.ylabel('sample quantiles')Figure 11-10. Quantile-quantile plot for Microsoft log returnsAll this leads us finally to the formal normality tests:In%5B32%5D:forsyminsymbols:print%22%5CnResults for symbol %25s%22%25symprint32*%22-%22log_data%3Dnp.array(log_returns%5Bsym%5D.dropna())normality_tests(log_data)Out%5B32%5D: Results for symbol %5EGDAXI         --------------------------------","1.Cf. Markowitz, Harry (1952): %E2%80%9CPortfolio Selection.%E2%80%9D Journal of Finance, Vol. 7, 77-91.         Skew of data set           0.026         Skew test p-value          0.623         Kurt of data set           6.525         Kurt test p-value          0.000         Norm test p-value          0.000         Results for symbol %5EGSPC         --------------------------------         Skew of data set          -0.320         Skew test p-value          0.000         Kurt of data set          10.054         Kurt test p-value          0.000         Norm test p-value          0.000         Results for symbol YHOO         --------------------------------         Skew of data set           0.565         Skew test p-value          0.000         Kurt of data set          31.987         Kurt test p-value          0.000         Norm test p-value          0.000         Results for symbol MSFT         --------------------------------         Skew of data set           0.043         Skew test p-value          0.415         Kurt of data set          10.180         Kurt test p-value          0.000         Norm test p-value          0.000Throughout, the p-values of the different tests are all zero, strongly rejecting the testhypothesisthat the different sample data sets are normally distributed. This shows thatthe normal assumption for stock market returns-as, for example, embodied in thegeometric Brownian motion model-cannot be justified in general and that one mighthave to use richer models generating fat tails (e.g., jump diffusion models or modelswith stochastic volatility).","Modern or mean-variance portfolio theory (MPT) is a major cornerstone of financialtheory. Based on this theoretical breakthrough the Nobel Prize in Economics was awar%E2%80%90ded to its inventor, Harry Markowitz, in 1990. Although formulated in the 1950s,1 it isstill a theory taught to finance students and applied in practice today (often with someminor or major modifications). This section illustrates the fundamental principles ofthe theory.","Chapter 5 in the book by Copeland, Weston, and Shastri (2005) provides a good intro%E2%80%90duction to the formal topics associated with MPT. As pointed out previously, the as%E2%80%90sumption of normally distributed returns is fundamental to the theory:By looking only at mean and variance, we are necessarily assuming that no other statisticsare necessary to describe the distribution of end-of-period wealth. Unless investors havea special type of utility function (quadratic utility function), it is necessary to assume thatreturns have a normal distribution, which can be completely described by mean andvariance.","Let us begin our Pythonsession by importing a couple of by now well-known libraries:In%5B33%5D:importnumpyasnpimportpandasaspdimportpandas.io.dataaswebimportmatplotlib.pyplotasplt%25matplotlibinlineWe pick five different assets for the analysis: American tech stocks Apple Inc., Yahoo!Inc., and Microsoft Inc., as well as German Deutsche Bank AG and gold as a commodityvia an exchange-traded fund (ETF). The basic idea of MPT is diversification to achievea minimal portfolio risk or maximal portfolio returns given a certain level of risk. Onewould expect such results for the right combination of a large enough number of assetsand a certain diversity in the assets. However, to convey the basic ideas and to showtypical effects, these five assets shall suffice:In%5B34%5D:symbols%3D%5B'AAPL','MSFT','YHOO','DB','GLD'%5Dnoa%3Dlen(symbols)Using the DataReader function of pandas (cf. Chapter 6) makes getting the time seriesdata rather efficient. We are only interested, as in the previous example, in the Closeprices of each stock:In%5B35%5D:data%3Dpd.DataFrame()forsyminsymbols:data%5Bsym%5D%3Dweb.DataReader(sym,data_source%3D'yahoo',end%3D'2014-09-12')%5B'Adj Close'%5Ddata.columns%3DsymbolsFigure 11-11 shows the time series data in normalized fashion graphically:In%5B36%5D:(data/data.ix%5B0%5D*100).plot(figsize%3D(8,5))","Figure 11-11. Stock prices over timeMean-variance refers to the mean and variance of the (log) returns of the differentsecurities, which are calculated as follows:In%5B37%5D:rets%3Dnp.log(data/data.shift(1))Over the period of the time series data, we see significant differences in the annualizedperformance. We use a factor of 252 trading days to annualize the daily returns:In%5B38%5D:rets.mean()*252Out%5B38%5D: AAPL    0.266036         MSFT    0.114476         YHOO    0.196165         DB     -0.125170         GLD     0.016054         dtype: float64The covariance matrixfor the assets to be invested in is the central piece of the wholeportfolio selection process. pandas has a built-in method to generate the covariancematrix:In%5B39%5D:rets.cov()*252Out%5B39%5D:           AAPL      MSFT      YHOO        DB       GLD         AAPL  0.072813  0.020426  0.023254  0.041044  0.005234         MSFT  0.020426  0.049384  0.024247  0.046100  0.002105         YHOO  0.023254  0.024247  0.093349  0.051528 -0.000864         DB    0.041044  0.046100  0.051528  0.177477  0.008775         GLD   0.005234  0.002105 -0.000864  0.008775  0.032406","%E2%80%9CIn what follows, we assume that an investor is not allowed to set up short positions ina security. Only long positions are allowed, which means that 100%25 of the investor's","wealth has to be divided among the available assets in such a way that all positions arelong (positive) and that the positions add up to 100%25. Given the five securities, youcould for example invest equal amounts into every security (i.e., 20%25 of your wealth ineach). The following code generates five random numbers between 0 and 1 and thennormalizes the values such that the sum of all values equals 1:In%5B40%5D:weights%3Dnp.random.random(noa)weights/%3Dnp.sum(weights)In%5B41%5D:weightsOut%5B41%5D: array(%5B 0.0346395 ,  0.02726489,  0.2868883 ,  0.10396806,  0.54723926%5D)You can now check that the asset weights indeed add up to 1%3B i.e., %ED%9A%BAIwi %3D 1, where Iisthe number of assets and wi%E2%89%A5 0 is the weight of asset i. Equation 11-1 provides theformula for the expected portfolio return given the weights for the single securities. Thisis expected portfolio return in the sense that historical mean performance is assumedto be the best estimator for future (expected) performance. Here, the riare the state-dependent future returns (vector with return values assumed to be normally distribut%E2%80%90ed) and %ED%9C%87i is the expected return for security i. Finally, wT is the transpose of the weightsvector and %ED%9C%87 is the vector of the expected security returns.Equation 11-1. General formula for expected portfolio return%CE%BCp%3D%ED%90%80%E2%88%91Iwiri%3D%E2%88%91Iwi%ED%90%80ri%3D%E2%88%91Iwi%CE%BCi%3DwT%CE%BCTranslated into Pythonthis boils down to the following line of code, where we multiplyagain by 252 to get annualized return values:In%5B42%5D:np.sum(rets.mean()*weights)*252# expected portfolio returnOut%5B42%5D: 0.064385749262353215The second object of choice in MPT is the expected portfolio variance. The covariancebetween two securities is defined by %ED%9C%8Eij %3D %ED%9C%8Eji %3D ","(ri %E2%80%93 %ED%9C%87i)(rj %E2%80%93 %ED%9C%87j)). The variance of a securityis the special case of the covariance with itself: %CF%83i2%3D%ED%90%80ri%E2%88%92%CE%BCi2. Equation 11-2 providesthe covariance matrix for a portfolio of securities (assuming an equal weight of 1 forevery security).","Equation 11-2. Portfolio covariance matrix%CE%A3%3D%CF%8312%CF%8312%E2%8B%AF%CF%831I%CF%8321%CF%8322%E2%8B%AF%CF%832I%E2%8B%AE%E2%8B%AE%E2%8B%B1%E2%8B%AE%CF%83I1%CF%83I2%E2%8B%AF%CF%83I2Equipped with the portfolio covariance matrix, Equation 11-3 then provides the formulafor the expected portfolio variance.Equation 11-3. General formula for expected portfolio variance%CF%83p2%3D%ED%90%80r%E2%88%92%CE%BC2%3D%E2%88%91i%E2%88%88I%E2%88%91j%E2%88%88Iwiwj%CF%83ij%3DwT%CE%A3wIn Pythonthis all again boils down to a single line of code, making heavy use of NumPy'svectorization capabilities. The dot function gives the dot product of two vectors/matri%E2%80%90ces. The T or transpose method gives the transpose of a vector or matrix:In%5B43%5D:np.dot(weights.T,np.dot(rets.cov()*252,weights))# expected portfolio varianceOut%5B43%5D: 0.024929484097150213The (expected) portfolio standard deviation or volatility %CF%83p%3D%CF%83p2 is then only one squareroot away:In%5B44%5D:np.sqrt(np.dot(weights.T,np.dot(rets.cov()*252,weights)))# expected portfolio standard deviation/volatilityOut%5B44%5D: 0.15789073467797346","TheMPTexampleshowsagainhowefficientitiswithPythontotranslatemathematicalconcepts,likeportfolioreturnorportfoliovariance,intoexecutable,vectorizedcode(anargumentmadeinChapter 1).","This mainly completes the tool set for mean-variance portfolio selection. Of paramountinterest to investors is what risk-return profiles are possible for a given set of securities,and their statistical characteristics. To this end, we implement a Monte Carlo simulation(cf. Chapter 10) to generate random portfolio weight vectors on a larger scale. For everysimulated allocation, we record the resulting expected portfolio return and variance:In%5B45%5D:prets%3D%5B%5Dpvols%3D%5B%5Dforpinrange(2500):weights%3Dnp.random.random(noa)weights/%3Dnp.sum(weights)prets.append(np.sum(rets.mean()*weights)*252)pvols.append(np.sqrt(np.dot(weights.T,np.dot(rets.cov()*252,weights))))prets%3Dnp.array(prets)pvols%3Dnp.array(pvols)Figure 11-12 illustrates the results of the Monte Carlo simulation. In addition it providesresults for the so-called Sharpe ratio, defined as SR%E2%89%A1%CE%BCp%E2%88%92rf%CF%83p (i.e., the expected excessreturn of the portfolio) over the risk-free short rate rfdivided by the expected standarddeviation of the portfolio. For simplicity, we assume rf %3D 0:In%5B46%5D:plt.figure(figsize%3D(8,4))plt.scatter(pvols,prets,c%3Dprets/pvols,marker%3D'o')plt.grid(True)plt.xlabel('expected volatility')plt.ylabel('expected return')plt.colorbar(label%3D'Sharpe ratio')Figure 11-12. Expected return and volatility for different/random portfolio weightsIt is clear by inspection of Figure 11-12 that not all weight distributions perform wellwhen measured in terms of mean and variance. For example, for a fixed risk level of,say, 20%25, there are multiple portfolios that all show different returns. As an investor oneis generally interested in the maximum return given a fixed risk level or the minimum","2.An alternative to np.sum(x) - 1would be to write np.sum(x) %3D%3D 1taking into account that with Pythonthe Boolean True value equals 1 and the False value equals 0.risk given a fixed return expectation. This set of portfolios then makes up the so-calledefficient frontier. This is what we derive later in the section.","To make our lives a bit easier, first we have a convenience function giving back the majorportfolio statistics for an input weights vector/array:In%5B47%5D:defstatistics(weights):''' Returns portfolio statistics.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             weights : array-like                 weights for different securities in portfolio             Returns             %3D%3D%3D%3D%3D%3D%3D             pret : float                 expected portfolio return             pvol : float                 expected portfolio volatility             pret / pvol : float                 Sharpe ratio for rf%3D0             '''weights%3Dnp.array(weights)pret%3Dnp.sum(rets.mean()*weights)*252pvol%3Dnp.sqrt(np.dot(weights.T,np.dot(rets.cov()*252,weights)))returnnp.array(%5Bpret,pvol,pret/pvol%5D)The derivation of the optimal portfolios is a constrained optimization problem forwhich we use the function minimize from the scipy.optimizesublibrary (cf.Chapter 9):In%5B48%5D:importscipy.optimizeasscoThe minimization function minimizeis quite general and allows for (in)equality con%E2%80%90straints and bounds for the parameters. Let us start with the maximization of the Sharperatio. Formally, we minimize the negative value of the Sharpe ratio:In%5B49%5D:defmin_func_sharpe(weights):return-statistics(weights)%5B2%5DThe constraint is that all parameters (weights) add up to 1. This can be formulated asfollows using the conventions of the minimizefunction (cf. the documentation for thisfunction).2","In%5B50%5D:cons%3D(%7B'type':'eq','fun':lambdax:np.sum(x)-1%7D)We also bound the parameter values (weights) to be within 0 and 1. These values areprovided to the minimization function as a tuple of tuples in this case:In%5B51%5D:bnds%3Dtuple((0,1)forxinrange(noa))The only input that is missing for a call of the optimization function is a starting pa%E2%80%90rameter list (initial guesses for the weights). We simply use an equal distribution:In%5B52%5D:noa*%5B1./noa,%5DOut%5B52%5D: %5B0.2, 0.2, 0.2, 0.2, 0.2%5DCalling the function returns not only optimal parameter values, but much more. Westore the results in an object we call opts:In%5B53%5D:%25%25timeopts%3Dsco.minimize(min_func_sharpe,noa*%5B1./noa,%5D,method%3D'SLSQP',bounds%3Dbnds,constraints%3Dcons)Out%5B53%5D: CPU times: user 52 ms, sys: 0 ns, total: 52 ms         Wall time: 50.3 msHere are the results:In%5B54%5D:optsOut%5B54%5D:   status: 0          success: True             njev: 6             nfev: 42              fun: -1.0597540702789927                x: array(%5B  6.59141408e-01,   8.82635668e-02,   2.52595026e-01,                  8.34564622e-17,  -8.91214186e-17%5D)          message: 'Optimization terminated successfully.'              jac: array(%5B  3.27527523e-05,  -1.61930919e-04,  -2.88933516e-05,                  1.51561590e%2B00,   1.24186277e-03,   0.00000000e%2B00%5D)              nit: 6Our main interest lies in getting the optimal portfolio composition. To this end, weaccess the results object by providing the key of interest-i.e., xin our case. The opti%E2%80%90mization yields a portfolio that only consists of three out of the five assets:In%5B55%5D:opts%5B'x'%5D.round(3)Out%5B55%5D: array(%5B 0.659,  0.088,  0.253,  0.   , -0.   %5D)Using the portfolio weights from the optimization, the following statistics emerge:In%5B56%5D:statistics(opts%5B'x'%5D).round(3)Out%5B56%5D: array(%5B 0.235,  0.222,  1.06 %5D)The expected return is about 23.5%25, the expected volatility is about 22.2%25, and theresulting optimal Sharpe ratio is 1.06.","Next, let us minimize the variance of the portfolio. This is the same as minimizing thevolatility, but we will define a function to minimize the variance:In%5B57%5D:defmin_func_variance(weights):returnstatistics(weights)%5B1%5D**2Everything else can remain the same for the call of the minimize function:In%5B58%5D:optv%3Dsco.minimize(min_func_variance,noa*%5B1./noa,%5D,method%3D'SLSQP',bounds%3Dbnds,constraints%3Dcons)In%5B59%5D:optvOut%5B59%5D:   status: 0          success: True             njev: 9             nfev: 64              fun: 0.018286019968366075                x: array(%5B  1.07591814e-01,   2.49124471e-01,   1.09219925e-01,                  1.01101853e-17,   5.34063791e-01%5D)          message: 'Optimization terminated successfully.'              jac: array(%5B 0.03636634,  0.03643877,  0.03613905,  0.05222051,                  0.03676446,  0.        %5D)              nit: 9This time a fourth asset is added to the portfolio. This portfolio mix leads to the absoluteminimum variance portfolio:In%5B60%5D:optv%5B'x'%5D.round(3)Out%5B60%5D: array(%5B 0.108,  0.249,  0.109,  0.   ,  0.534%5D)For the expected return, volatility, and Sharpe ratio, we get:In%5B61%5D:statistics(optv%5B'x'%5D).round(3)Out%5B61%5D: array(%5B 0.087,  0.135,  0.644%5D)","The derivation of all optimal portfolios-i.e., all portfolios with minimum volatility fora given target return level (or all portfolios with maximum return for a given risk level)-is similar to the previous optimizations. The only difference is that we have to iterateover multiple starting conditions. The approach we take is that we fix a target returnlevel and derive for each such level those portfolio weights that lead to the minimumvolatility value. For the optimization, this leads to two conditions: one for the targetreturn level tretand one for the sum of the portfolio weights as before. The boundaryvalues for each parameter stay the same:In%5B62%5D:cons%3D(%7B'type':'eq','fun':lambdax:statistics(x)%5B0%5D-tret%7D,%7B'type':'eq','fun':lambdax:np.sum(x)-1%7D)bnds%3Dtuple((0,1)forxinweights)","For clarity, we define a dedicated function min_funcfor use in the minimization pro%E2%80%90cedure. It merely returns the volatility value from the statistics function:In%5B63%5D:defmin_func_port(weights):returnstatistics(weights)%5B1%5DWhen iterating over different target return levels (trets), one condition for the mini%E2%80%90mization changes. That is why the conditions dictionary is updated during every loop:In%5B64%5D:%25%25timetrets%3Dnp.linspace(0.0,0.25,50)tvols%3D%5B%5Dfortretintrets:cons%3D(%7B'type':'eq','fun':lambdax:statistics(x)%5B0%5D-tret%7D,%7B'type':'eq','fun':lambdax:np.sum(x)-1%7D)res%3Dsco.minimize(min_func_port,noa*%5B1./noa,%5D,method%3D'SLSQP',bounds%3Dbnds,constraints%3Dcons)tvols.append(res%5B'fun'%5D)tvols%3Dnp.array(tvols)Out%5B64%5D: CPU times: user 4.35 s, sys: 4 ms, total: 4.36 s         Wall time: 4.36 sFigure 11-13 shows the optimization results. Crosses indicate the optimal portfoliosgiven a certain target return%3B the dots are, as before, the random portfolios. In addition,the figure shows two larger stars: one for the minimum volatility/variance portfolio (theleftmost portfolio) and one for the portfolio with the maximum Sharpe ratio:In%5B65%5D:plt.figure(figsize%3D(8,4))plt.scatter(pvols,prets,c%3Dprets/pvols,marker%3D'o')# random portfolio compositionplt.scatter(tvols,trets,c%3Dtrets/tvols,marker%3D'x')# efficient frontierplt.plot(statistics(opts%5B'x'%5D)%5B1%5D,statistics(opts%5B'x'%5D)%5B0%5D,'r*',markersize%3D15.0)# portfolio with highest Sharpe ratioplt.plot(statistics(optv%5B'x'%5D)%5B1%5D,statistics(optv%5B'x'%5D)%5B0%5D,'y*',markersize%3D15.0)# minimum variance portfolioplt.grid(True)plt.xlabel('expected volatility')plt.ylabel('expected return')plt.colorbar(label%3D'Sharpe ratio')","Figure 11-13. Minimum risk portfolios for given return level (crosses)The efficient frontier is comprised of all optimal portfolios with a higher return than theabsolute minimum variance portfolio. These portfolios dominate all other portfolios interms of expected returns given a certain risk level.","In addition to risky securities like stocks or commodities (such as gold), there is ingeneral one universal, riskless investment opportunity available: cash or cash accounts.In an idealized world, money held in a cash account with a large bank can be consideredriskless (e.g., through public deposit insurance schemes). The downside is that such ariskless investment generally yields only a small return, sometimes close to zero.However, taking into account such a riskless asset enhances the efficient investmentopportunity set for investors considerably. The basic idea is that investors first determinean efficient portfolio of risky assets and then add the riskless asset to the mix. By ad%E2%80%90justing the proportion of the investor's wealth to be invested in the riskless asset it ispossible to achieve any risk-return profile that lies on the straight line (in the risk-returnspace) between the riskless asset and the efficient portfolio.Which efficient portfolio (out of the many options) is to be taken to invest in optimalfashion%3F It is the one portfolio where the tangent line of the efficient frontier goes exactlythrough the risk-return point of the riskless portfolio. For example, consider a risklessinterest rate of rf %3D 0.01. We look for that portfolio on the efficient frontier for whichthe tangent goes through the point (%ED%9C%8Ef,rf) %3D (0,0.01) in risk-return space.For the calculations to follow, we need a functional approximation and the first deriv%E2%80%90ative for the efficient frontier. We use cubic splines interpolation to this end (cf.Chapter 9):In%5B66%5D:importscipy.interpolateassci","For the spline interpolation, we only use the portfolios from the efficient frontier. Thefollowing code selects exactly these portfolios from our previously used sets tvols andtrets:In%5B67%5D:ind%3Dnp.argmin(tvols)evols%3Dtvols%5Bind:%5Derets%3Dtrets%5Bind:%5DThe new ndarray objects evols and erets are used for the interpolation:In%5B68%5D:tck%3Dsci.splrep(evols,erets)Via this numerical route we end up being able to define a continuously differentiablefunction f(x) for the efficient frontier and the respective first derivative function df(x):In%5B69%5D:deff(x):''' Efficient frontier function (splines approximation). '''returnsci.splev(x,tck,der%3D0)defdf(x):''' First derivative of efficient frontier function. '''returnsci.splev(x,tck,der%3D1)What we are looking for is a function t(x) %3D a %2B b %C2%B7 x describing the line that passesthrough the riskless asset in risk-return space and that is tangent to the efficient frontier.Equation 11-4 describes all three conditions that the function t(x) has to satisfy.Equation 11-4. Mathematical conditions for capital market linetx%3Da%2Bb%C2%B7xt0%3Drf%E2%87%94a%3Drftx%3Dfx%E2%87%94a%2Bb%C2%B7x%3Dfxt'x%3Df'x%E2%87%94b%3Df'xSince we do not have a closed formula for the efficient frontier or the first derivative ofit, we have to solve the system of equations in Equation 11-4numerically. To this end,we define a Python function that returns the values of all three equations given theparameter set p %3D (a,b,x):In%5B70%5D:defequations(p,rf%3D0.01):eq1%3Drf-p%5B0%5Deq2%3Drf%2Bp%5B1%5D*p%5B2%5D-f(p%5B2%5D)eq3%3Dp%5B1%5D-df(p%5B2%5D)returneq1,eq2,eq3The function fsolve from scipy.optimize is capable of solving such a system of equa%E2%80%90tions. We provide an initial parameterization in addition to the function equations.Note that success or failure of the optimization might depend on the initial parameter%E2%80%90","ization, which therefore has to be chosen carefully-generally by a combination of edu%E2%80%90cated guesses with trial and error:In%5B71%5D:opt%3Dsco.fsolve(equations,%5B0.01,0.5,0.15%5D)The numerical optimization yields the following values. As desired, we havea %3D rf %3D 0.01:In%5B72%5D:optOut%5B72%5D: array(%5B 0.01      ,  1.01498858,  0.22580367%5D)The three equations are also, as desired, zero:In%5B73%5D:np.round(equations(opt),6)Out%5B73%5D: array(%5B 0., -0., -0.%5D)Figure 11-14presents the results graphically: the star represents the optimal portfoliofrom the efficient frontier where the tangent line passes through the riskless asset point(0,rf%3D 0.01). The optimal portfolio has an expected volatility of 20.5%25 and an expectedreturn of 17.6%25. The plot is generated with the following code:In%5B74%5D:plt.figure(figsize%3D(8,4))plt.scatter(pvols,prets,c%3D(prets-0.01)/pvols,marker%3D'o')# random portfolio compositionplt.plot(evols,erets,'g',lw%3D4.0)# efficient frontiercx%3Dnp.linspace(0.0,0.3)plt.plot(cx,opt%5B0%5D%2Bopt%5B1%5D*cx,lw%3D1.5)# capital market lineplt.plot(opt%5B2%5D,f(opt%5B2%5D),'r*',markersize%3D15.0)plt.grid(True)plt.axhline(0,color%3D'k',ls%3D'--',lw%3D2.0)plt.axvline(0,color%3D'k',ls%3D'--',lw%3D2.0)plt.xlabel('expected volatility')plt.ylabel('expected return')plt.colorbar(label%3D'Sharpe ratio')","Figure 11-14. Capital market line and tangency portfolio (star) for risk-free rate of 1%25The portfolio weights of the optimal (tangent) portfolio are as follows. Only three ofthe five assets are in the mix:In%5B75%5D:cons%3D(%7B'type':'eq','fun':lambdax:statistics(x)%5B0%5D-f(opt%5B2%5D)%7D,%7B'type':'eq','fun':lambdax:np.sum(x)-1%7D)res%3Dsco.minimize(min_func_port,noa*%5B1./noa,%5D,method%3D'SLSQP',bounds%3Dbnds,constraints%3Dcons)In%5B76%5D:res%5B'x'%5D.round(3)Out%5B76%5D: array(%5B 0.684,  0.059,  0.257, -0.   ,  0.   %5D)","Principal component analysis(PCA) has become a popular tool in finance. Wikipediadefines the technique as follows:Principal component analysis (PCA) is a statistical procedure that uses orthogonal trans%E2%80%90formation to convert a set of observations of possibly correlated variables into a set ofvalues of linearly uncorrelated variables called principal components. The number ofprincipal components is less than or equal to the number of original variables. Thistransformation is defined in such a way that the first principal component has the largestpossible variance (that is, accounts for as much of the variability in the data as possible),and each succeeding component in turn has the highest variance possible under theconstraint that it is orthogonal to (i.e., uncorrelated with) the preceding components.Consider, for example, a stock index like the German DAX index, composed of 30different stocks. The stock price movements of all stocks taken together determine themovement in the index (via some well-documented formula). In addition, the stockprice movements of the single stocks are generally correlated, for example, due to gen%E2%80%90eral economic conditions or certain developments in certain sectors.For statistical applications, it is generally quite hard to use 30 correlated factors to ex%E2%80%90plain the movements of a stock index. This is where PCA comes into play. It derives","single, uncorrelated componentsthat are %E2%80%9Cwell suited%E2%80%9D to explain the movements in thestock index. One can think of these components as linear combinations (i.e., portfolios)of selected stocks from the index. Instead of working with 30 correlated index constit%E2%80%90uents, one can then work with maybe 5, 3, or even only 1 principal component.The example of this section illustrates the use of PCA in such a context. We retrieve datafor both the German DAX index and all stocks that make up the index. We then usePCA to derive principal components, which we use to construct what we call apca_index.First, some imports. In particular, we use the KernelPCAfunction of the scikit-learnmachine learning library (cf. the documentation for KernelPCA):In%5B1%5D:importnumpyasnpimportpandasaspdimportpandas.io.dataaswebfromsklearn.decompositionimportKernelPCA","The following listobject contains the 30 symbols for the stocks contained in the Ger%E2%80%90man DAX index, as well as the symbol for the index itself:In%5B2%5D:symbols%3D%5B'ADS.DE','ALV.DE','BAS.DE','BAYN.DE','BEI.DE','BMW.DE','CBK.DE','CON.DE','DAI.DE','DB1.DE','DBK.DE','DPW.DE','DTE.DE','EOAN.DE','FME.DE','FRE.DE','HEI.DE','HEN3.DE','IFX.DE','LHA.DE','LIN.DE','LXS.DE','MRK.DE','MUV2.DE','RWE.DE','SAP.DE','SDF.DE','SIE.DE','TKA.DE','VOW3.DE','%5EGDAXI'%5DWe work only with the closing values of each data set that we retrieve (for details onhow to retrieve stock data with pandas, see Chapter 6):In%5B3%5D:%25%25timedata%3Dpd.DataFrame()forsyminsymbols:data%5Bsym%5D%3Dweb.DataReader(sym,data_source%3D'yahoo')%5B'Close'%5Ddata%3Ddata.dropna()Out%5B3%5D: CPU times: user 408 ms, sys: 68 ms, total: 476 ms        Wall time: 5.61 sLet us separate the index data since we need it regularly:In%5B4%5D:dax%3Dpd.DataFrame(data.pop('%5EGDAXI'))The DataFrame object data now has log return data for the 30 DAX stocks:In%5B5%5D:data%5Bdata.columns%5B:6%5D%5D.head()Out%5B5%5D:             ADS.DE  ALV.DE  BAS.DE  BAYN.DE  BEI.DE  BMW.DE        Date","3.Note that we work here-and in the section to follow on Bayesian statistics-with absolute stock prices andnot with return data, which would be more statistically sound. The reason for this is that it simplifies intuitionand makes graphical plots easier to interpret. In real-world applications, you would use return data.        2010-01-04   38.51   88.54   44.85    56.40   46.44   32.05        2010-01-05   39.72   88.81   44.17    55.37   46.20   32.31        2010-01-06   39.40   89.50   44.45    55.02   46.17   32.81        2010-01-07   39.74   88.47   44.15    54.30   45.70   33.10        2010-01-08   39.60   87.99   44.02    53.82   44.38   32.65","Usually, PCA works with normalized data sets. Therefore, the following conveniencefunction proves helpful:In%5B6%5D:scale_function%3Dlambdax:(x-x.mean())/x.std()For the beginning, consider a PCA with multiple components (i.e., we do not restrictthe number of components):3In%5B7%5D:pca%3DKernelPCA().fit(data.apply(scale_function))The importance or explanatory power of each component is given by its Eigenvalue.These are found in an attribute of the KernelPCA object. The analysis gives too manycomponents:In%5B8%5D:len(pca.lambdas_)Out%5B8%5D: 655Therefore, let us only have a look at the first 10 components. The tenth componentalready has almost negligible influence:In%5B9%5D:pca.lambdas_%5B:10%5D.round()Out%5B9%5D: array(%5B 22816.,   6559.,   2535.,   1558.,    697.,    442.,    378.,                  255.,    183.,    151.%5D)We are mainly interested in the relative importance of each component, so we willnormalize these values. Again, we use a convenience function for this:In%5B10%5D:get_we%3Dlambdax:x/x.sum()In%5B11%5D:get_we(pca.lambdas_)%5B:10%5DOut%5B11%5D: array(%5B 0.6295725 ,  0.1809903 ,  0.06995609,  0.04300101,  0.01923256,                 0.01218984,  0.01044098,  0.00704461,  0.00505794,  0.00416612%5D)With this information, the picture becomes much clearer. The first component alreadyexplains about 60%25 of the variability in the 30 time series. The first five componentsexplain about 95%25 of the variability:In%5B12%5D:get_we(pca.lambdas_)%5B:5%5D.sum()","Out%5B12%5D: 0.94275246704834414","Next, we use PCA to construct a PCA (or factor) index over time and compare it withthe original index. First, we have a PCA index with a single component only:In%5B13%5D:pca%3DKernelPCA(n_components%3D1).fit(data.apply(scale_function))dax%5B'PCA_1'%5D%3Dpca.transform(-data)Figure 11-15 shows the results for normalized data-already not too bad, given therather simple application of the approach:In%5B14%5D:importmatplotlib.pyplotasplt%25matplotlibinlinedax.apply(scale_function).plot(figsize%3D(8,4))Figure 11-15. German DAX index and PCA index with one componentLet us see if we can improve the results by adding more components. To this end, weneed to calculate a weighted average from the single resulting components:In%5B15%5D:pca%3DKernelPCA(n_components%3D5).fit(data.apply(scale_function))pca_components%3Dpca.transform(-data)weights%3Dget_we(pca.lambdas_)dax%5B'PCA_5'%5D%3Dnp.dot(pca_components,weights)The results as presented in Figure 11-16are still %E2%80%9Cgood,%E2%80%9D but not that much better thanbefore-at least upon visual inspection:In%5B16%5D:importmatplotlib.pyplotasplt%25matplotlibinlinedax.apply(scale_function).plot(figsize%3D(8,4))","Figure 11-16. German DAX index and PCA indices with one and five componentsIn view of the results so far, we want to inspect the relationship between the DAX indexand the PCA index in a different way-via a scatter plot, adding date information to themix. First, we convert the DatetimeIndex of the DataFrameobject to a matplotlib-compatible format:In%5B17%5D:importmatplotlibasmplmpl_dates%3Dmpl.dates.date2num(data.index)mpl_datesOut%5B17%5D: array(%5B 733776.,  733777.,  733778., ...,  735500.,  735501.,  735502.%5D)This new date list can be used for a scatter plot, highlighting through different colorswhich date each data point is from. Figure 11-17 shows the data in this fashion:In%5B18%5D:plt.figure(figsize%3D(8,4))plt.scatter(dax%5B'PCA_5'%5D,dax%5B'%5EGDAXI'%5D,c%3Dmpl_dates)lin_reg%3Dnp.polyval(np.polyfit(dax%5B'PCA_5'%5D,dax%5B'%5EGDAXI'%5D,1),dax%5B'PCA_5'%5D)plt.plot(dax%5B'PCA_5'%5D,lin_reg,'r',lw%3D3)plt.grid(True)plt.xlabel('PCA_5')plt.ylabel('%5EGDAXI')plt.colorbar(ticks%3Dmpl.dates.DayLocator(interval%3D250),format%3Dmpl.dates.DateFormatter('%25d %25b %25y'))","Figure 11-17. DAX return values against PCA return values with linear regressionFigure 11-17reveals that there is obviously some kind of structural break sometime inthe middle of 2011. If the PCA index were to perfectly replicate the DAX index, wewould expect all the points to lie on a straight line and to see the regression line goingthrough these points. Perfection is hard to achieve, but we can maybe do better.To this end, let us divide the total time frame into two subintervals. We can then im%E2%80%90plement an early and a late regression:In%5B19%5D:cut_date%3D'2011/7/1'early_pca%3Ddax%5Bdax.index%3Ccut_date%5D%5B'PCA_5'%5Dearly_reg%3Dnp.polyval(np.polyfit(early_pca,dax%5B'%5EGDAXI'%5D%5Bdax.index%3Ccut_date%5D,1),early_pca)In%5B20%5D:late_pca%3Ddax%5Bdax.index%3E%3Dcut_date%5D%5B'PCA_5'%5Dlate_reg%3Dnp.polyval(np.polyfit(late_pca,dax%5B'%5EGDAXI'%5D%5Bdax.index%3E%3Dcut_date%5D,1),late_pca)Figure 11-18 shows the new regression lines, which indeed display the high explanatorypower both before our cutoff date and thereafter. This heuristic approach will be madea bit more formal in the next section on Bayesian statistics:In%5B21%5D:plt.figure(figsize%3D(8,4))plt.scatter(dax%5B'PCA_5'%5D,dax%5B'%5EGDAXI'%5D,c%3Dmpl_dates)plt.plot(early_pca,early_reg,'r',lw%3D3)plt.plot(late_pca,late_reg,'r',lw%3D3)plt.grid(True)plt.xlabel('PCA_5')plt.ylabel('%5EGDAXI')plt.colorbar(ticks%3Dmpl.dates.DayLocator(interval%3D250),format%3Dmpl.dates.DateFormatter('%25d %25b %25y'))","4.For a Python-based introduction into these and other fundamental concepts of Bayesian statistics, refer toDowney (2013).Figure 11-18. DAX index values against PCA index values with early and late regres%E2%80%90sion (regime switch)","Bayesian statistics nowadays is a cornerstone in empirical finance. This chapter cannotlay the foundations for all concepts of the field. You should therefore consult, if needed,a textbook like that by Geweke (2005) for a general introduction or Rachev (2008) forone that is financially motivated.","The most common interpretation of Bayes' formula in finance is the diachronic inter%E2%80%90pretation. This mainly states that over time we learn new information about certainvariables or parameters of interest, like the mean return of a time series. Equation 11-5states the theorem formally. Here, Hstands for an event, the hypothesis, and D repre%E2%80%90sents the data an experiment or the real world might present.4 On the basis of thesefundamental definitions, we have:%E2%80%A2p(H) is called the prior probability.%E2%80%A2p(D) is the probability for the data under any hypothesis, called the normalizingconstant.%E2%80%A2p(D%7CH) is the likelihood (i.e., the probability) of the data under hypothesis H.%E2%80%A2p(H%7CD) is the posterior probability%3B i.e., after we have seen the data.","Equation 11-5. Bayes's formulapHD%3DpHpDHpDConsider a simple example. We have two boxes, B1 and B2. Box B1 contains 20 blackballs and 70 red balls, while box B2 contains 40 black balls and 50 red balls. We randomlydraw a ball from one of the two boxes. Assume the ball is black. What are the probabilitiesfor the hypotheses %E2%80%9CH1: Ball is from box B1%E2%80%9D and %E2%80%9CH2: Ball is from box B2,%E2%80%9D respectively%3FBefore we randomly draw the ball, both hypotheses are equally likely. After it is clearthat the ball is black, we have to update the probability for both hypotheses accordingto Bayes' formula. Consider hypothesis H1:%E2%80%A2",": p(H1) %3D 0.5%E2%80%A2",": p(D) %3D 0.5 %C2%B7 0.2 %2B 0.5 %C2%B7 0.4 %3D 0.3%E2%80%A2",": p(D%7CH1) %3D 0.2This gives for the updated probability of H1pH1D%3D0.5%C2%B70.20.3%3D13.This result also makes sense intuitively. The probability for drawing a black ball frombox B2 is twice as high as for the same event happening with box B1. Therefore, havingdrawn a black ball, the hypothesis H2has with pH2D%3D23an updated probability twotimes as high as the updated probability for hypothesis H1.","With PyMC3 the Pythonecosystem provides a powerful and performant library to tech%E2%80%90nically implement Bayesian statistics. PyMC3 is (at the time of this writing) notpart ofthe Anaconda distribution recommended in Chapter 2. On a Linux or a Mac OS Xoperating system, the installation comprises mainly the following steps.First, you need to install the Theano compiler package needed for PyMC3 (cf. http://bit.ly/install_theano). In the shell, execute the following commands:%24 git clone git://github.com/Theano/Theano.git%24 sudo python Theano/python.py installOn a Mac OS X system you might need to add the following line to your .bash_profilefile (to be found in your home/user directory):export DYLD_FALLBACK_LIBRARY_PATH%3D %5C%24DYLD_FALLBACK_LIBRARY_PATH:/Library/anaconda/lib:Once Theano is installed, the installation of PyMC3 is straightforward:","5.This example and the one in the following subsection are from a presentation by Thomas Wiecki, one of thelead developers of PyMC3%3B he allowed me to use them for this chapter, for which I am most grateful.%24 git clone https://github.com/pymc-devs/pymc.git%24 cd pymc%24 sudo python setup.py installIf successful, you should be able to import the library named pymc as usual:In%5B22%5D:importwarningswarnings.simplefilter('ignore')importpymcaspmimportnumpyasnpnp.random.seed(1000)importmatplotlib.pyplotasplt%25matplotlibinline","PyMC3 is already a powerful library at the time of this writing. How%E2%80%90ever,itisstillinitsearlystages,soyoushouldexpectfurtheren%E2%80%90hancements,changestotheAPI,etc.Makesuretostayuptodateby regularly checking the website when using PyMC3.","Consider now an example where we have noisy data around a straight line:5In%5B23%5D:x%3Dnp.linspace(0,10,500)y%3D4%2B2*x%2Bnp.random.standard_normal(len(x))*2As a benchmark, consider first an ordinary least-squares regression given the noisy data,using NumPy's polyfit function (cf. Chapter 9). The regression is implemented asfollows:In%5B24%5D:reg%3Dnp.polyfit(x,y,1)# linear regressionFigure 11-19 shows the data and the regression line graphically:In%5B25%5D:plt.figure(figsize%3D(8,4))plt.scatter(x,y,c%3Dy,marker%3D'v')plt.plot(x,reg%5B1%5D%2Breg%5B0%5D*x,lw%3D2.0)plt.colorbar()plt.grid(True)plt.xlabel('x')plt.ylabel('y')","6.Cf. http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo. For example, the Monte Carlo algorithms usedthroughout the book and analyzed in detail in Chapter 10 all generate so-called Markov chains, since theimmediate next step/value only depends on the current state of the process and not on any other historicstate or value.Figure 11-19. Sample data points and regression lineThe result of the %E2%80%9Cstandard%E2%80%9D regression approach is fixed values for the parameters ofthe regression line:In%5B26%5D:regOut%5B26%5D: array(%5B 2.03384161,  3.77649234%5D)Note that the highest-order monomial factor (in this case, the slope of the regressionline) is at index level 0 and that the intercept is at index level 1. The original parameters2 and 4 are not perfectly recovered, but this of course is due to the noise included inthe data.Next, the Bayesian regression. Here, we assume that the parameters are distributed ina certain way. For example, consider the equation describing the regression line %C5%B7(x) %3D%ED%9B%BC %2B %ED%9B%BD %C2%B7 x. We now assume the following priors:%E2%80%A2%ED%9B%BC is normally distributed with mean 0 and a standard deviation of 20.%E2%80%A2%ED%9B%BD is normally distributed with mean 0 and a standard deviation of 20.For the likelihood, we assume a normal distribution with mean of %C5%B7(x) and a uniformlydistributed standard deviation between 0 and 10.A major element of Bayesian regression is (Markov Chain) Monte Carlo (MCMC)sampling.6 In principle, this is the same as drawing balls multiple times from boxes, asin the previous simple example-just in a more systematic, automated way.For the technical sampling, there are three different functions to call:","%E2%80%A2find_MAPfinds the starting point for the sampling algorithm by deriving the localmaximum a posteriori point.%E2%80%A2NUTSimplements the so-called %E2%80%9Cefficient No-U-Turn Sampler with dual averaging%E2%80%9D(NUTS) algorithm for MCMC sampling given the assumed priors.%E2%80%A2sampledraws a number of samples given the starting value from find_MAP and theoptimal step size from the NUTS algorithm.All this is to be wrapped into a PyMC3Model object and executed within a withstatement:In%5B27%5D:withpm.Model()asmodel:# model specifications in PyMC3# are wrapped in a with statement# define priorsalpha%3Dpm.Normal('alpha',mu%3D0,sd%3D20)beta%3Dpm.Normal('beta',mu%3D0,sd%3D20)sigma%3Dpm.Uniform('sigma',lower%3D0,upper%3D10)# define linear regressiony_est%3Dalpha%2Bbeta*x# define likelihoodlikelihood%3Dpm.Normal('y',mu%3Dy_est,sd%3Dsigma,observed%3Dy)# inferencestart%3Dpm.find_MAP()# find starting value by optimizationstep%3Dpm.NUTS(state%3Dstart)# instantiate MCMC sampling algorithmtrace%3Dpm.sample(100,step,start%3Dstart,progressbar%3DFalse)# draw 100 posterior samples using NUTS samplingHave a look at the estimates from the first sample:In%5B28%5D:trace%5B0%5DOut%5B28%5D: %7B'alpha': 3.8783781152509031,          'beta': 2.0148472296530033,          'sigma': 2.0078134493352975%7DAll three values are rather close to the original values (4, 2, 2). However, the wholeprocedure yields, of course, many more estimates. They are best illustrated with the helpof a trace plot, as in Figure 11-20-i.e., a plot showing the resulting posterior distributionfor the different parameters as well as all single estimates per sample. The posteriordistribution gives us an intuitive sense about the uncertainty in our estimates:In%5B29%5D:fig%3Dpm.traceplot(trace,lines%3D%7B'alpha':4,'beta':2,'sigma':2%7D)plt.figure(figsize%3D(8,8))","Figure 11-20. Trace plots for alpha, beta, and sigmaTaking only the alpha and betavalues from the regression, we can draw all resultingregression lines as shown in Figure 11-21:In%5B30%5D:plt.figure(figsize%3D(8,4))plt.scatter(x,y,c%3Dy,marker%3D'v')plt.colorbar()plt.grid(True)plt.xlabel('x')plt.ylabel('y')foriinrange(len(trace)):plt.plot(x,trace%5B'alpha'%5D%5Bi%5D%2Btrace%5B'beta'%5D%5Bi%5D*x)Figure 11-21. Sample data and regression lines from Bayesian regression","Having seen Bayesian regression with PyMC3in action with dummy data, we now moveon to real market data. In this context, we introduce yet another Pythonlibrary: zipline(cf. https://github.com/quantopian/zipline and https://pypi.python.org/pypi/zipline). zipline is a Pythonic, open source algorithmic trading library that powers the commu%E2%80%90nity backtesting platform Quantopian.It is also to be installed separately, e.g., by using pip:%24 pip install ziplineAfter installation, import zipline as well pytz and datetime as follows:In%5B31%5D:importwarningswarnings.simplefilter('ignore')importziplineimportpytzimportdatetimeasdtSimilar to pandas, zipline provides a convenience function to load financial data fromdifferent sources. Under the hood, zipline also uses pandas.The example we use is a %E2%80%9Cclassical%E2%80%9D pair trading strategy, namely with gold and stocksof gold mining companies. These are represented by ETFs with the following symbols,respectively:%E2%80%A2GLD%E2%80%A2GDXWe can load the data using zipline as follows:In%5B32%5D:data%3Dzipline.data.load_from_yahoo(stocks%3D%5B'GLD','GDX'%5D,end%3Ddt.datetime(2014,3,15,0,0,0,0,pytz.utc)).dropna()data.info()Out%5B32%5D: GLD         GDX         %3Cclass 'pandas.core.frame.DataFrame'%3E         DatetimeIndex: 1967 entries, 2006-05-22 00:00:00%2B00:00 to 2014-03-14 00         :00:00%2B00:00         Data columns (total 2 columns):         GDX    1967 non-null float64         GLD    1967 non-null float64         dtypes: float64(2)Figure 11-22 shows the historical data for both ETFs:In%5B33%5D:data.plot(figsize%3D(8,4))","Figure 11-22. Comovements of trading pairThe absolute performance differs significantly:In%5B34%5D:data.ix%5B-1%5D/data.ix%5B0%5D-1Out%5B34%5D: GDX   -0.216002         GLD    1.038285         dtype: float64However, both time series seem to be quite strongly positively correlated when inspect%E2%80%90ing Figure 11-22, which is also reflected in the correlation data:In%5B35%5D:data.corr()Out%5B35%5D:           GDX       GLD         GDX  1.000000  0.466962         GLD  0.466962  1.000000As usual, the DatetimeIndex object of the DataFrame object consists of Timestampobjects:In%5B36%5D:data.indexOut%5B36%5D: %3Cclass 'pandas.tseries.index.DatetimeIndex'%3E         %5B2006-05-22, ..., 2014-03-14%5D         Length: 1967, Freq: None, Timezone: UTCTo use the date-time information with matplotlib in the way we want to in the fol%E2%80%90lowing, we have to first convert it to an ordinal date representation:In%5B37%5D:importmatplotlibasmplmpl_dates%3Dmpl.dates.date2num(data.index)mpl_datesOut%5B37%5D: array(%5B 732453.,  732454.,  732455., ...,  735304.,  735305.,  735306.%5D)","7.Note also here that we are working with absolute price levels and not return data, which would be statisticallymore sound. For a real-world (trading) application, you would rather choose the return data to implementsuch an analysis.Figure 11-23shows a scatter plot of the time series data, plotting the GLD values againstthe GDX values and illustrating the dates of each data pair with different colorings:7In%5B38%5D:plt.figure(figsize%3D(8,4))plt.scatter(data%5B'GDX'%5D,data%5B'GLD'%5D,c%3Dmpl_dates,marker%3D'o')plt.grid(True)plt.xlabel('GDX')plt.ylabel('GLD')plt.colorbar(ticks%3Dmpl.dates.DayLocator(interval%3D250),format%3Dmpl.dates.DateFormatter('%25d %25b %25y'))Figure 11-23. Scatter plot of prices for GLD and GDXLet us implement a Bayesian regression on the basis of these two time series. The pa%E2%80%90rameterizations are essentially the same as in the previous example with dummy data%3Bwe just replace the dummy data with the real data we now have available:In%5B39%5D:withpm.Model()asmodel:alpha%3Dpm.Normal('alpha',mu%3D0,sd%3D20)beta%3Dpm.Normal('beta',mu%3D0,sd%3D20)sigma%3Dpm.Uniform('sigma',lower%3D0,upper%3D50)y_est%3Dalpha%2Bbeta*data%5B'GDX'%5D.valueslikelihood%3Dpm.Normal('GLD',mu%3Dy_est,sd%3Dsigma,observed%3Ddata%5B'GLD'%5D.values)start%3Dpm.find_MAP()step%3Dpm.NUTS(state%3Dstart)trace%3Dpm.sample(100,step,start%3Dstart,progressbar%3DFalse)","Figure 11-24 shows the results from the MCMC sampling procedure given the assump%E2%80%90tions about the prior probability distributions for the three parameters:In%5B40%5D:fig%3Dpm.traceplot(trace)plt.figure(figsize%3D(8,8))Figure 11-24. Trace plots for alpha, beta, and sigma based on GDX and GLD dataFigure 11-25adds all the resulting regression lines to the scatter plot from before. Allthe regression lines are pretty close to each other:In%5B41%5D:plt.figure(figsize%3D(8,4))plt.scatter(data%5B'GDX'%5D,data%5B'GLD'%5D,c%3Dmpl_dates,marker%3D'o')plt.grid(True)plt.xlabel('GDX')plt.ylabel('GLD')foriinrange(len(trace)):plt.plot(data%5B'GDX'%5D,trace%5B'alpha'%5D%5Bi%5D%2Btrace%5B'beta'%5D%5Bi%5D*data%5B'GDX'%5D)plt.colorbar(ticks%3Dmpl.dates.DayLocator(interval%3D250),format%3Dmpl.dates.DateFormatter('%25d %25b %25y'))","Figure 11-25. Scatter plot with %E2%80%9Csimple%E2%80%9D regression linesThe figure reveals a major drawback of the regression approach used: the approach doesnot take into account evolutions over time. That is, the most recent data is treated thesame way as the oldest data.As pointed out at the beginning of this section, the Bayesian approach in finance isgenerally most useful when seen as diachronic-i.e., in the sense that new data revealedover time allows for better regressions and estimates.To incorporate this concept in the current example, we assume that the regression pa%E2%80%90rameters are not only random and distributed in some fashion, but that they followsome kind of random walk over time. It is the same generalization used when makingthe transition in finance theory from random variables to stochastic processes (whichare essentially ordered sequences of random variables):To this end, we define a new PyMC3 model, this time specifying parameter values asrandom walks with the variance parameter values transformed to log space (for bettersampling characteristics).In%5B42%5D:model_randomwalk%3Dpm.Model()withmodel_randomwalk:# std of random walk best sampled in log spacesigma_alpha,log_sigma_alpha%3D %5Cmodel_randomwalk.TransformedVar('sigma_alpha',pm.Exponential.dist(1./.02,testval%3D.1),pm.logtransform)sigma_beta,log_sigma_beta%3D %5Cmodel_randomwalk.TransformedVar('sigma_beta',pm.Exponential.dist(1./.02,testval%3D.1),pm.logtransform)After having specified the distributions of the random walk parameters, we can proceedwith specifying the random walks for alpha and beta. To make the whole proceduremore efficient, 50 data points at a time share common coefficients:","In%5B43%5D:frompymc.distributions.timeseriesimportGaussianRandomWalk# to make the model simpler, we will apply the same coefficients# to 50 data points at a timesubsample_alpha%3D50subsample_beta%3D50withmodel_randomwalk:alpha%3DGaussianRandomWalk('alpha',sigma_alpha**-2,shape%3Dlen(data)/subsample_alpha)beta%3DGaussianRandomWalk('beta',sigma_beta**-2,shape%3Dlen(data)/subsample_beta)# make coefficients have the same length as pricesalpha_r%3Dnp.repeat(alpha,subsample_alpha)beta_r%3Dnp.repeat(beta,subsample_beta)The time series data sets have a length of 1,967 data points:In%5B44%5D:len(data.dropna().GDX.values)# a bit longer than 1,950Out%5B44%5D: 1967For the sampling to follow, the number of data points must be divisible by 50. Therefore,only the first 1,950 data points are taken for the regression:In%5B45%5D:withmodel_randomwalk:# define regressionregression%3Dalpha_r%2Bbeta_r*data.GDX.values%5B:1950%5D# assume prices are normally distributed# the mean comes from the regressionsd%3Dpm.Uniform('sd',0,20)likelihood%3Dpm.Normal('GLD',mu%3Dregression,sd%3Dsd,observed%3Ddata.GLD.values%5B:1950%5D)All these definitions are a bit more involved than before due to the use of random walksinstead of a single random variable. However, the inference steps with the MCMC re%E2%80%90main essentially the same. Note, though, that the computational burden increases sub%E2%80%90stantially since we have to estimate per random walk sample 1,950 / 50 %3D 39 parameterpairs (instead of 1, as before):In%5B46%5D:importscipy.optimizeasscowithmodel_randomwalk:# first optimize random walkstart%3Dpm.find_MAP(vars%3D%5Balpha,beta%5D,fmin%3Dsco.fmin_l_bfgs_b)# samplingstep%3Dpm.NUTS(scaling%3Dstart)trace_rw%3Dpm.sample(100,step,start%3Dstart,progressbar%3DFalse)In total, we have 100 estimates with 39 time intervals:","In%5B47%5D:np.shape(trace_rw%5B'alpha'%5D)Out%5B47%5D: (100, 39)We can illustrate the evolution of the regression factors alphaand beta over time byplotting a subset of the estimates and the average over all samples, as in Figure 11-26:In%5B48%5D:part_dates%3Dnp.linspace(min(mpl_dates),max(mpl_dates),39)In%5B49%5D:fig,ax1%3Dplt.subplots(figsize%3D(10,5))plt.plot(part_dates,np.mean(trace_rw%5B'alpha'%5D,axis%3D0),'b',lw%3D2.5,label%3D'alpha')foriinrange(45,55):plt.plot(part_dates,trace_rw%5B'alpha'%5D%5Bi%5D,'b-.',lw%3D0.75)plt.xlabel('date')plt.ylabel('alpha')plt.axis('tight')plt.grid(True)plt.legend(loc%3D2)ax1.xaxis.set_major_formatter(mpl.dates.DateFormatter('%25d %25b %25y'))ax2%3Dax1.twinx()plt.plot(part_dates,np.mean(trace_rw%5B'beta'%5D,axis%3D0),'r',lw%3D2.5,label%3D'beta')foriinrange(45,55):plt.plot(part_dates,trace_rw%5B'beta'%5D%5Bi%5D,'r-.',lw%3D0.75)plt.ylabel('beta')plt.legend(loc%3D4)fig.autofmt_xdate()Figure 11-26. Evolution of (mean) alpha and (mean) beta over time (updated estimatesover time)","BothwhenpresentingthePCAanalysisimplementationandforthisexample about Bayesian statistics, we've worked with absolute pricelevelsinsteadofrelative(log)returndata.Thisisforillustrationpurposes only, because the respective graphical results are easier tounderstandandinterpret(theyarevisually%E2%80%9Cmoreappealing%E2%80%9D).How%E2%80%90ever, for real-world financial applications you would instead rely onrelative return data.Using the mean alpha and betavalues, we can illustrate how the regression is updatedover time. Figure 11-27again shows the data points as a scatter plot. In addition, the 39regression lines resulting from the mean alphaand betavalues are displayed. It isobvious that updating over time increases the regression fit (for the current/most recentdata) tremendously-in other words, every time period needs its own regression:In%5B50%5D:plt.figure(figsize%3D(10,5))plt.scatter(data%5B'GDX'%5D,data%5B'GLD'%5D,c%3Dmpl_dates,marker%3D'o')plt.colorbar(ticks%3Dmpl.dates.DayLocator(interval%3D250),format%3Dmpl.dates.DateFormatter('%25d %25b %25y'))plt.grid(True)plt.xlabel('GDX')plt.ylabel('GLD')x%3Dnp.linspace(min(data%5B'GDX'%5D),max(data%5B'GDX'%5D))foriinrange(39):alpha_rw%3Dnp.mean(trace_rw%5B'alpha'%5D.T%5Bi%5D)beta_rw%3Dnp.mean(trace_rw%5B'beta'%5D.T%5Bi%5D)plt.plot(x,alpha_rw%2Bbeta_rw*x,color%3Dplt.cm.jet(256*i/39))Figure 11-27. Scatter plot with time-dependent regression lines (updated estimates)","This concludes the section on Bayesian regression, which shows that Pythonoffers withPyMC3 a powerful library to implement different approaches from Bayesian statistics.Bayesian regression in particular is a tool that has become quite popular and importantrecently in quantitative finance.","Statistics is not only an important discipline in its own right, but also provides indis%E2%80%90pensible tools for many other disciplines, like finance and the social sciences. It is im%E2%80%90possible to give a broad overview of statistics in a single chapter. This chapter thereforeconcentrates on four important topics, illustrating the use of Python and several statisticslibraries on the basis of realistic examples:Normality testsThe normality assumption with regard to financial market returns is an importantone for many financial theories and applications%3B it is therefore important to be ableto test whether certain time series data conforms to this assumption. As we haveseen-via graphical and statistical means-real-world return data generally is notnormally distributed.Modern portfolio theoryMPT, with its focus on the mean and variance/volatility of returns, can be consid%E2%80%90ered one of the major conceptual and intellectual successes of statistics in finance%3Bthe important concept of investment diversificationis beautifully illustrated in thiscontext.Principal component analysisPCA provides a pretty helpful method to reduce complexity for factor/componentanalysis tasks%3B we have shown that five principal components-constructed fromthe 30 stocks contained in the DAX index-suffice to explain more than 95%25 of theindex's variability.Bayesian regressionBayesian statistics in general (and Bayesian regression in particular) has become apopular tool in finance, since this approach overcomes shortcomings of other ap%E2%80%90proaches, as introduced in Chapter 9%3B even if the mathematics and the formalismare more involved, the fundamental ideas-like the updating of probability/distri%E2%80%90bution beliefs over time-are easily grasped intuitively.","The following online resources are helpful:","%E2%80%A2Information about the SciPy statistical functions is found here: http://docs.scipy.org/doc/scipy/reference/stats.html.%E2%80%A2Also consult the documentation of the statsmodels library: http://statsmodels.sourceforge.net/stable/.%E2%80%A2For the optimization functions used in this chapter, refer to http://docs.scipy.org/doc/scipy/reference/optimize.html.%E2%80%A2There is a short tutorial available for PyMC3%3B at the time of this writing the library isstill in early release mode and not yet fully documented.Useful references in book form are:%E2%80%A2Copeland, Thomas, Fred Weston, and Kuldeep Shastri (2005): Financial Theoryand Corporate Policy, 4th ed. Pearson, Boston, MA.%E2%80%A2Downey, Allen (2013): Think Bayes. O'Reilly, Sebastopol, CA.%E2%80%A2Geweke, John (2005): Contemporary Bayesian Econometrics and Statistics. JohnWiley %26 Sons, Hoboken, NJ.%E2%80%A2Rachev, Svetlozar et al. (2008): Bayesian Methods in Finance. John Wiley %26 Sons,Hoboken, NJ.","Microsoft Excel is probably the most successful data analytics platform of all times.- Kirat SinghIt is fair to say that Microsoft Excel-as part of Microsoft's Officesuite of productivitytools-is one of the most widely used tools and applications in the finance industry andthe finance functions of corporate and other institutions. What started out as a com%E2%80%90puterized version of paper spreadsheets has become a multipurpose tool for financialanalysis and financial application building (in addition to the many use cases in otherfields and industries).Spreadsheet applications, like Microsoft Exceland LibreOffice Calc, are characterizedby a few main features:OrganizationA workbookis a spreadsheet application file that is organized in single sheets thatin turn are organized in cells.DataData is generally stored in tabular form in single cells%3B the cells contain the dataitself (e.g., a floating-point number or a text string), formatting information fordisplay purposes (e.g., font type, color), and maybe some computer code (if, forexample, the data in the cell is the result of a numerical operation).FunctionalityGiven the data stored in single cells, you can do computational and other operationswith that data, like adding or multiplying integers.VisualizationData can be easily visualized, for example, as a pie chart.","ProgrammabilityModern spreadsheet applications allow highly flexible programmability, e.g., viaVisual Basic for Applications (VBA) within an Excel spreadsheet.ReferencesThe major tool for implementing functionality or writing, e.g., VBA code is the cellreference%3B every cell has unique coordinates (workbook, sheet name, column, androw) identifying the cell.This brief characterization might explain the popularity: all technical elements neededto implement financial analyses or applications are found in a single place. Thinking ofPython and the previous chapters, you need a couple of libraries and tools (Python,NumPy, matplotlib, PyTables, etc.) combined to have available all of the features justlisted.Such convenience and one-size-fits-all approaches generally come at a cost, though. Topick just one area, spreadsheets are notsuited to storing large amounts of data or datawith complex relationships. This is the reason why Microsoft Excelin the finance in%E2%80%90dustry has developed more as a general graphical user interface(GUI) %E2%80%9Conly.%E2%80%9D In manycases, it is mainly used to display and visualize data and aggregate information and toimplement ad hoc analyses. For example, there are interfaces available to get data fromleading data service providers, like Bloomberg and Thomson Reuters, into Excel (andmaybe the other way around).This chapter works on the assumption that Microsoft Excel is available on almost everydesktop or notebook computer and that it is used as a general GUI. In this sense, Pythoncan play the following roles:Manipulation toolUsing Python, you can interact with and manipulate Excel spreadsheets.Data processorPython can provide data to a spreadsheet and read data from a spreadsheet.Analytics enginePython can provide its whole analytics capabilities to spreadsheets, becoming a full-fledged substitute for VBA programming.","Fundamental Pythonlibraries to work with Excelspreadsheet files are xlrd and xlwt(cf. http://www.python-excel.org). Although quite popular, a major drawback of xlwt isthat it can only write spreadsheet files compatible with Microsoft Excel97/2000/XP/2003, OpenOffice.org Calc, and Gnumeric-i.e., those with the suffix .xls. Therefore,","1.Note that a simple mechanism to generate Excel spreadsheets from Python is to export data in the form ofa comma-separated value (CSV) file and to import this with Excel. This might sometimes be more efficientthan the ways presented in the following discussion.we also use the libraries xlsxwriter and OpenPyxl, which generate spreadsheet files inthe current .xslx format. We'll begin, then, with a few imports.In%5B1%5D:importnumpyasnpimportpandasaspdimportxlrd,xlwtimportxlsxwriterpath%3D'data/'","We start by generating a workbook with two sheets.1First, the Workbookobject wb. Notethat this is an in-memory version of the workbook only (so far):In%5B2%5D:wb%3Dxlwt.Workbook()In%5B3%5D:wbOut%5B3%5D: %3Cxlwt.Workbook.Workbook at 0x7f7dcc49df10%3EThe second step is to add one or multiple sheets to the Workbook object:In%5B4%5D:wb.add_sheet('first_sheet',cell_overwrite_ok%3DTrue)Out%5B4%5D: %3Cxlwt.Worksheet.Worksheet at 0x7f7dac9dde90%3EWe now have one Worksheet object, which has index number 0:In%5B5%5D:wb.get_active_sheet()Out%5B5%5D: 0To further work with the sheet, define an alias for it:In%5B6%5D:ws_1%3Dwb.get_sheet(0)ws_1Out%5B6%5D: %3Cxlwt.Worksheet.Worksheet at 0x7f7dac9dde90%3EOf course, these two steps-instantiation and alias definition-can be combined into asingle step:In%5B7%5D:ws_2%3Dwb.add_sheet('second_sheet')Both Worksheet objects are still empty. Therefore, let us generate a NumPyndarrayobjectcontaining some numbers:In%5B8%5D:data%3Dnp.arange(1,65).reshape((8,8))In%5B9%5D:data","Out%5B9%5D: array(%5B%5B 1,  2,  3,  4,  5,  6,  7,  8%5D,               %5B 9, 10, 11, 12, 13, 14, 15, 16%5D,               %5B17, 18, 19, 20, 21, 22, 23, 24%5D,               %5B25, 26, 27, 28, 29, 30, 31, 32%5D,               %5B33, 34, 35, 36, 37, 38, 39, 40%5D,               %5B41, 42, 43, 44, 45, 46, 47, 48%5D,               %5B49, 50, 51, 52, 53, 54, 55, 56%5D,               %5B57, 58, 59, 60, 61, 62, 63, 64%5D%5D)Using the writemethod and providing row and column information (with zero-basedindexing), data is easily written to a certain cell in a certain worksheet:In%5B10%5D:ws_1.write(0,0,100)# write 100 in cell %22A1%22This way, the sample data can be written %E2%80%9Cin bulk%E2%80%9D to the two Worksheet objects:In%5B11%5D:forcinrange(data.shape%5B0%5D):forrinrange(data.shape%5B1%5D):ws_1.write(r,c,data%5Bc,r%5D)ws_2.write(r,c,data%5Br,c%5D)The save method of the Workbook class allows us to save the whole Workbookobjectto disk:In%5B12%5D:wb.save(path%2B'workbook.xls')On Windows systems, the path might look like r%22C:%5Cpath%5Cdata%5Cworkbook.xls%22.","(The creation of spreadsheet files in the new format works essentially the same way.First, we create a Workbook object:In%5B13%5D:wb%3Dxlsxwriter.Workbook(path%2B'workbook.xlsx')Second, the Worksheet objects:In%5B14%5D:ws_1%3Dwb.add_worksheet('first_sheet')ws_2%3Dwb.add_worksheet('second_sheet')Third, we write data to the Worksheet objects:In%5B15%5D:forcinrange(data.shape%5B0%5D):forrinrange(data.shape%5B1%5D):ws_1.write(r,c,data%5Bc,r%5D)ws_2.write(r,c,data%5Br,c%5D)Fourth, we close the Workbook file object:In%5B16%5D:wb.close()In%5B17%5D:ll%24path*","Out%5B17%5D: -rw------- 1 yhilpisch 7375 Sep 28 18:18 data/chart.xlsx         -rw------- 1 yhilpisch 5632 Sep 28 18:18 data/workbook.xls         -rw------- 1 yhilpisch 6049 Sep 28 18:18 data/workbook.xlsxIf everything went well, the file opened in Microsoft Excel should look like Figure 12-1.Figure 12-1. Screenshot of workbook in Excelxlsxwriter has many more options to generate Workbook objects, for example withcharts. Consider the following code (cf. the xlsxwriter documentation):In%5B18%5D:wb%3Dxlsxwriter.Workbook(path%2B'chart.xlsx')ws%3Dwb.add_worksheet()# write cumsum of random values in first columnvalues%3Dnp.random.standard_normal(15).cumsum()ws.write_column('A1',values)# create a new chart objectchart%3Dwb.add_chart(%7B'type':'line'%7D)# add a series to the chartchart.add_series(%7B'values':'%3DSheet1!%24A%241:%24A%2415','marker':%7B'type':'diamond'%7D,%7D)# series with markers (here: diamond)# insert the chartws.insert_chart('C1',chart)","wb.close()The resulting spreadsheet file is shown as a screenshot in Figure 12-2.Figure 12-2. Screenshot of workbook in Excel with a chart","The sister library xlrd is responsible for reading data from spreadsheet files (i.e.,workbooks):In%5B19%5D:book%3Dxlrd.open_workbook(path%2B'workbook.xlsx')In%5B20%5D:bookOut%5B20%5D: %3Cxlrd.book.Book at 0x7f7dabec4890%3EOnce a workbook is opened, the sheet_namesmethod provides the names of all Worksheet objects in this particular Workbook object:In%5B21%5D:book.sheet_names()Out%5B21%5D: %5Bu'first_sheet', u'second_sheet'%5DWorksheets can be accessed via their names or index values:In%5B22%5D:sheet_1%3Dbook.sheet_by_name('first_sheet')sheet_2%3Dbook.sheet_by_index(1)sheet_1","Out%5B22%5D: %3Cxlrd.sheet.Sheet at 0x7f7dabec4a10%3EIn%5B23%5D:sheet_2.nameOut%5B23%5D: u'second_sheet'Important attributes of a Worksheet object are ncols and nrows, indicating the numberof columns and rows, respectively, that contain data:In%5B24%5D:sheet_1.ncols,sheet_1.nrowsOut%5B24%5D: (8, 8)Single cells-i.e. Cellobjects-are accessed via the cellmethod, providing the num%E2%80%90bers for both the row and the column (again, numbering is zero-based). The valueattribute then gives the data stored in this particular cell:In%5B25%5D:cl%3Dsheet_1.cell(0,0)cl.valueOut%5B25%5D: 1.0The attribute ctype gives the cell type:In%5B26%5D:cl.ctypeOut%5B26%5D: 2Table 12-1 lists all Excel cell types.Table 12-1. Excel cell types","Python","XL_CELL_EMPTY0Empty stringXL_CELL_TEXT1A Unicode stringXL_CELL_NUMBER2floatXL_CELL_DATE3floatXL_CELL_BOOLEAN4int (1 %3D TRUE, 0 %3D FALSE)XL_CELL_ERROR5int representing internal Excel codesXL_CELL_BLANK6Empty string, only when formatting_info%3DTrueSimilarly, you can access whole rows by providing the number of the row to the rowmethod:In%5B27%5D:sheet_2.row(3)Out%5B27%5D: %5Bnumber:25.0,          number:26.0,          number:27.0,          number:28.0,          number:29.0,          number:30.0,","          number:31.0,          number:32.0%5DAnd, analogously, whole columns:In%5B28%5D:sheet_2.col(3)Out%5B28%5D: %5Bnumber:4.0,          number:12.0,          number:20.0,          number:28.0,          number:36.0,          number:44.0,          number:52.0,          number:60.0%5DThe methods row_values and col_values only deliver the values contained in the re%E2%80%90spective row or column:In%5B29%5D:sheet_1.col_values(3,start_rowx%3D3,end_rowx%3D7)Out%5B29%5D: %5B28.0, 29.0, 30.0, 31.0%5DIn%5B30%5D:sheet_1.row_values(3,start_colx%3D3,end_colx%3D7)Out%5B30%5D: %5B28.0, 36.0, 44.0, 52.0%5DTo read out all the data in a Worksheet object, just iterate over all columns and rowsthat contain data:In%5B31%5D:forcinrange(sheet_1.ncols):forrinrange(sheet_1.nrows):print'%25i'%25sheet_1.cell(r,c).value,printOut%5B31%5D: 1 2 3 4 5 6 7 8         9 10 11 12 13 14 15 16         17 18 19 20 21 22 23 24         25 26 27 28 29 30 31 32         33 34 35 36 37 38 39 40         41 42 43 44 45 46 47 48         49 50 51 52 53 54 55 56         57 58 59 60 61 62 63 64","There is yet another library to generate and read Excel spreadsheet files in .xlsxformatwith Python: OpenPyxl. This library allows us to both create spreadsheet files and readfrom them. In addition, while basic usage is similar to the other libraries, the interfaceis in some cases a bit more Pythonic and might therefore be worth taking a look at.Import the library as follows:In%5B32%5D:importopenpyxlasoxl","Let us proceed as before. First, generate a Workbook object:In%5B33%5D:wb%3Doxl.Workbook()Second, create a Worksheet object:In%5B34%5D:ws%3Dwb.create_sheet(index%3D0,title%3D'oxl_sheet')Third, write the data to the worksheet:In%5B35%5D:forcinrange(data.shape%5B0%5D):forrinrange(data.shape%5B1%5D):ws.cell(row%3Dr,column%3Dc).value%3Ddata%5Bc,r%5D# creates a Cell object and assigns a valueFourth, close the file object:In%5B36%5D:wb.save(path%2B'oxl_book.xlsx')With OpenPyxl, you can also read workbooks:In%5B37%5D:wb%3Doxl.load_workbook(path%2B'oxl_book.xlsx')Now, single cells are easily accessed via their cell names:In%5B38%5D:ws%3Dwb.get_active_sheet()In%5B39%5D:cell%3Dws%5B'B4'%5DIn%5B40%5D:cell.columnOut%5B40%5D: 'B'In%5B41%5D:cell.rowOut%5B41%5D: 4In%5B42%5D:cell.valueOut%5B42%5D: 12Similarly, you can access cell ranges as in Excel:In%5B43%5D:ws%5B'B1':'B4'%5DOut%5B43%5D: ((%3CCell oxl_sheet.B1%3E,),          (%3CCell oxl_sheet.B2%3E,),          (%3CCell oxl_sheet.B3%3E,),          (%3CCell oxl_sheet.B4%3E,))In%5B44%5D:forcellinws%5B'B1':'B4'%5D:printcell%5B0%5D.valueOut%5B44%5D: 9         10         11         12There is also a rangemethod to which you can provide the cell range in Excel syntaxas a string:","In%5B45%5D:ws.range('B1:C4')# same as ws%5B'B1':'C4'%5DOut%5B45%5D: ((%3CCell oxl_sheet.B1%3E, %3CCell oxl_sheet.C1%3E),          (%3CCell oxl_sheet.B2%3E, %3CCell oxl_sheet.C2%3E),          (%3CCell oxl_sheet.B3%3E, %3CCell oxl_sheet.C3%3E),          (%3CCell oxl_sheet.B4%3E, %3CCell oxl_sheet.C4%3E))In%5B46%5D:forrowinws.range('B1:C4'):forcellinrow:printcell.value,printOut%5B46%5D: 9 17         10 18         11 19         12 20Refer to the library's website for more details.","Chapter 7shows how to interact with Excel spreadsheet files using the pandas library.Let us use these approaches to read the data written with the xlwtlibrary. We need aDataFrameobject for each sheet. With header%3DNone, pandasdoes not interpret the firstdata row as the header for the data set:In%5B47%5D:df_1%3Dpd.read_excel(path%2B'workbook.xlsx','first_sheet',header%3DNone)df_2%3Dpd.read_excel(path%2B'workbook.xlsx','second_sheet',header%3DNone)To recover the column names/values of the spreadsheet file, let us generate a list withcapital letters as column names for the DataFrame objects:In%5B48%5D:importstringcolumns%3D%5B%5Dforcinrange(data.shape%5B0%5D):columns.append(string.uppercase%5Bc%5D)columnsOut%5B48%5D: %5B'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'%5DWe pass this list as the new column names to the two objects:In%5B49%5D:df_1.columns%3Dcolumnsdf_2.columns%3DcolumnsIndeed, the output of the two DataFrame objects now resembles the spreadsheet stylepretty well:In%5B50%5D:df_1Out%5B50%5D:    A   B   C   D   E   F   G   H         0  1   9  17  25  33  41  49  57","         1  2  10  18  26  34  42  50  58         2  3  11  19  27  35  43  51  59         3  4  12  20  28  36  44  52  60         4  5  13  21  29  37  45  53  61         5  6  14  22  30  38  46  54  62         6  7  15  23  31  39  47  55  63         7  8  16  24  32  40  48  56  64In%5B51%5D:df_2Out%5B51%5D:     A   B   C   D   E   F   G   H         0   1   2   3   4   5   6   7   8         1   9  10  11  12  13  14  15  16         2  17  18  19  20  21  22  23  24         3  25  26  27  28  29  30  31  32         4  33  34  35  36  37  38  39  40         5  41  42  43  44  45  46  47  48         6  49  50  51  52  53  54  55  56         7  57  58  59  60  61  62  63  64Similarly, pandas allows us to write the data to Excel spreadsheet files:In%5B52%5D:df_1.to_excel(path%2B'new_book_1.xlsx','my_sheet')Note that when writing DataFrame objects to spreadsheet files pandas adds both columnnames and index values, as seen in Figure 12-3.Of course, pandas-generated Excel workbooks can be read as before with the xlrdlibrary:In%5B53%5D:wbn%3Dxlrd.open_workbook(path%2B'new_book_1.xlsx')In%5B54%5D:wbn.sheet_names()Out%5B54%5D: %5Bu'my_sheet'%5DTo write multiple DataFrame objects to a single spreadsheet file, one needs an ExcelWriter object:In%5B55%5D:wbw%3Dpd.ExcelWriter(path%2B'new_book_2.xlsx')df_1.to_excel(wbw,'first_sheet')df_2.to_excel(wbw,'second_sheet')wbw.save()Let us inspect if we indeed have generated the two sheets in the single spreadsheet file:In%5B56%5D:wbn%3Dxlrd.open_workbook(path%2B'new_book_2.xlsx')In%5B57%5D:wbn.sheet_names()Out%5B57%5D: %5Bu'first_sheet', u'second_sheet'%5D","Figure 12-3. Screenshot of workbook in Excel written with pandasAs a final use case for pandas and Excel, consider the reading and writing of largeramounts of data. Although this is not a fast operation, it might be useful in some cir%E2%80%90cumstances. First, the sample data to be used:In%5B58%5D:data%3Dnp.random.rand(20,100000)In%5B59%5D:data.nbytesOut%5B59%5D: 16000000Second, generate a DataFrame object out of the sample data:In%5B60%5D:df%3Dpd.DataFrame(data)Third, write it as an Excel file to the disk:In%5B61%5D:%25timedf.to_excel(path%2B'data.xlsx','data_sheet')Out%5B61%5D: CPU times: user 1min 25s, sys: 460 ms, total: 1min 26s         Wall time: 1min 25sThis takes quite a while. For comparison, see how fast native storage of the NumPyndarray object is (on an SSD drive):In%5B62%5D:%25timenp.save(path%2B'data',data)Out%5B62%5D: CPU times: user 8 ms, sys: 20 ms, total: 28 ms         Wall time: 159 ms","In%5B63%5D:ll%24path*Out%5B63%5D: -rw------- 1 yhilpisch     7372 Sep 28 18:18 data/chart.xlsx         -rw------- 1 yhilpisch 16000080 Sep 28 18:20 data/data.npy         -rw------- 1 yhilpisch  3948600 Sep 28 18:20 data/data.xlsx         -rw------- 1 yhilpisch     5828 Sep 28 18:18 data/new_book_1.xlsx         -rw------- 1 yhilpisch     6688 Sep 28 18:18 data/new_book_2.xlsx         -rw------- 1 yhilpisch     6079 Sep 28 18:18 data/oxl_book.xlsx         -rw------- 1 yhilpisch     5632 Sep 28 18:18 data/workbook.xls         -rw------- 1 yhilpisch     6049 Sep 28 18:18 data/workbook.xlsxFourth, read it from disk. This is significantly faster than writing it:In%5B64%5D:%25timedf%3Dpd.read_excel(path%2B'data.xlsx','data_sheet')Out%5B64%5D: CPU times: user 6.53 s, sys: 44 ms, total: 6.58 s         Wall time: 6.51 sHowever, see again the speed difference compared to native storage:In%5B65%5D:%25timedata%3Dnp.load(path%2B'data.npy')Out%5B65%5D: CPU times: user 16 ms, sys: 8 ms, total: 24 ms         Wall time: 40.5 msIn%5B66%5D:data,df%3D0.0,0.0!rm%24path*","The previous section shows how to generate, read, and manipulate Excel spreadsheetfiles (i.e., workbooks). Although there are some beneficial use cases, Python is not theonly way, and sometimes also not the best way, to achieve the results presented there.Much more interesting is to expose the analytical power of Python to Excelspread%E2%80%90sheets. However, this is a technically more demanding task. For example, the Pythonlibrary PyXLLprovides means to expose Pythonfunctions via so-called Exceladd-ins,Microsoft's technology to enhance the functionality of Excel. Additionally, the companyDataNitro provides a solution that allows the full integration of Python and Excel andmakes Python a full substitute for VBA programming. Both solutions, however, arecommercial products that need to be licensed.In what follows, we provide an overview of how to use DataNitro for Excel scripting,since this is a rather flexible approach to integrating Python with Excel.","DataNitroworks on Windows operating systems and Excelinstallations only. On MacOS systems it can be used in a Windows virtual machine environment. It is compatible","with Office 2007 and higher. Refer to the website http://www.datanitro.com for furtherinstructions on how to get a (trial) license for the solution and how to install it.When installing DataNitro you have the option to install Python as well. However, ifyou have already installed Anaconda (cf. Chapter 2), for example, there is no need toinstall another Python version or distribution. You then just have to customize theDataNitro solution (via the Settings menu) to use the existing Anaconda installation.DataNitro works with all Python versions 2.6 and higher as well as with versions 3.x.If successfully installed, you then find the DataNitroribbon within Excel, as displayedin Figure 12-4.Figure 12-4. Screenshot of Excel with DataNitro ribbon","There are two main methods to combine DataNitro with Excel:ScriptingWith this method, you control Excel spreadsheets via Python scripts, similar to theapproach presented in the previous section.User-defined functionsUsing this approach, you expose your own Python functions to Excel in such a waythat they can be called from within Excel.Both methods need an installation of the DataNitrosolution to work-i.e., you cannotdistribute something that you have worked on to somebody else who does not have theDataNitro solution installed.","Open the previously generated spreadsheet file workbook.xlsx in Excel. We want towork with DataNitro and this particular file. When you then click on the Python Shellsymbol in the DataNitro ribbon, your screen should look like Figure 12-5.Figure 12-5. Screenshot of Excel with DataNitro IPython shellA simple session could then look like the following:In%5B1%5D:Cell(%22B1%22)Out%5B1%5D:B1In%5B2%5D:Cell(%22B1%22).valueOut%5B2%5D:9In%5B3%5D:Cell(%22B1%22).value%3D'Excel with Python'# this immediately changes the (displayed) value# in the spreadsheetIn%5B4%5D:Cell(%22B1%22).valueOut%5B4%5D:u'Excel with Python'In the same way as you change the value attribute of a Cell object, you can assign aformula to it:In%5B5%5D:Cell(%22A9%22).formula%3D'%3DSum(A1:A8)'In%5B6%5D:Cell(%22A9%22).valueOut%5B6%5D:36Table 12-2 lists the attributes of the DataNitroCell object.","Table 12-2. DataNitro Cell attributes","rowRow of the cellcolColumn of the cellpositionPosition as a (row,col) tuplesheetName of the sheet the cell is innameName of the cell in Excel fashionvalueValue of the cellverticalAll cell values including and below the cellvertical_rangeExcel range for all cells including and below the cellhorizontalAll cell values including and right of the cellhorizontal_rangeExcel range for all cells including and right of the celltableAll values including and below/right of the cell as nested list objecttable_rangeExcel range for table objectformulaExcel formulacommentComment attached to cellhyperlinkHyperlink or email address as string objectalignmentText/value alignment for displaycolorCell colordfLets you write a pandasDataFrame object directly to the spreadsheetTable 12-3shows typesetting options for the Cellobject. All options are attributes ofthe font object, which is a property of the Cell object. For example, this:Cell(%22A1%22).font.size %3D 15sets a (new) font size.Table 12-3. DataNitro Cell typesetting options","sizeFont sizecolorFont colorboldBold font via Cell(%22A1%22).font.bold%3DTrueitalicItalic fontunderlineUnderlines textstrikethroughPuts strikethrough line through textsubscriptSubscripts textsuperscriptSuperscripts text","Finally, there are also a couple of methods for the Cell object. They are listed inTable 12-4.Table 12-4. DataNitro Cell methods","clearResets all properties/attributes of the cellcopy_fromCopies all properties/attributes from another cellcopy_format_fromCopies all properties/attributes from another cell except value and formulais_emptyReturns True if emptyoffsetReturns cell object given relative offset as (row,col) tuplesubtractionSubtraction gives the offset%3B e.g., Cell(%22B4%22) - Cell(%22A2%22) gives (2, 1)printGives name and sheet of cellset_nameSets named range in Excel%3B e.g., Cell(%22A1%22).set_name(%22upper_left%22)Often, it is helpful to work with CellRange instead of Cellobjects only. One can thinkof this as an approach to vectorize certain operations on multiple Cell objects. Considerthe following examples, still based on the same spreadsheet file workbook.xlsxwithour previous changes:In%5B6%5D:CellRange(%22A1:A8%22).valueOut%5B6%5D:%5B1,2,3,4,5,6,7,8%5DIn%5B7%5D:CellRange(%22A1:A8%22).value%3D1# like broadcastingIn%5B8%5D:CellRange(%22A1:A8%22).valueOut%5B8%5D:%5B1,1,1,1,1,1,1,1%5DIn%5B9%5D:CellRange(%22A1:A8%22).value%3D2*%5B1,2,3,4%5DIn%5B10%5D:CellRange(%22A1:A8%22).valueOut%5B10%5D:%5B1,2,3,4,1,2,3,4%5DIn%5B11%5D:Cell(%22A9%22).valueOut%5B11%5D:20# value of Sum function is# automatically updatedOf course, you can also use CellRange for iteration:In%5B12%5D:forcellinCellRange(%22A1:B2%22):....:printcell.name,cell.value....:A11B1PythonwithExcelA22B210","2.Cf. Chapter 4 for a similar discussion in the context of NumPyndarray objects and the benefits of vectorization.The rule of thumb there as well as here is to avoid loops on the Python level.The majority of the Cell attributes and methods can also be used with CellRange.When writing complex Python scripts for interaction with Excel spreadsheets, perfor%E2%80%90mancemight be an issue. Basically, performance is bound by Excel input/output (I/O)speed. The following rules should be followed whenever possible:Reading/writingDo not alternate reading with writing operations, since this might lower perfor%E2%80%90mance significantly.VectorizationUse CellRange objects or Cell().table objects to read and write data in (large)blocks instead of loops.2Use PythonFor example, when you have to transform a data block, it is better to read it in totalwith Python, to manipulate it with Python, and to write it back to the spreadsheetas a block%3B cell-by-cell operations can be really slow.Store data in PythonStore values in Python when possible rather than rereading them, especially forperformance-critical loops or similar operations.See the relevant sections in the DataNitrodocumentation for details on how to workwith whole Worksheet and Workbook objects.","A special topic when scripting Excel spreadsheets with DataNitro is plotting data con%E2%80%90tained in a spreadsheet with Python instead of using Excel's plotting capabilities.Example 12-1 shows a Python script that is only executable if DataNitro is installed. Itretrieves Apple Inc. stock price data with the DataReader function from pandas(cf.Chapter 6), writes the data to a newly generated Workbook object, and then plots thedata stored in the respective Worksheet object with Python-i.e., with the help of DataNitro's matplotlib.pyplot wrapper nitroplot-and exposes the result to the spread%E2%80%90sheet.Example 12-1. Plotting data stored in a spreadsheet with DataNitro and displaying amatplotlib plot in the same spreadsheet## Plotting with DataNitro in Excel# dn_plotting.py#","importpandas.io.dataaswebimportnitroplotasnplt# wrapper for matplotlib.pyplot (plt)# make a new workbookwb%3Dnew_wkbk()active_wkbk(wb)rename_sheet(%22Sheet1%22,%22Apple_Stock%22)# read Apple Inc. stock dataaapl%3Dweb.DataReader('aapl',data_source%3D'yahoo')%5B%5B'Open','Close'%5D%5D# write the data to the new workbookCell(%22A1%22).df%3Daapl# generate matplotlib plotnplt.figure(figsize%3D(8,4))nplt.plot(Cell(%22A2%22).vertical,Cell(%22C2%22).vertical,label%3D'AAPL')nplt.legend(loc%3D0)nplt.grid(True)nplt.xticks(rotation%3D35)# expose plot to Excel spreadsheetnplt.graph()# as plt.show()# save the new workbook with data and plotsave('dn_plot.xlsx')From a DataNitroIPython shell, execute the script with:In%5B1%5D:%25rundn_plotting.pyIf the script is successfully executed, the workbook/worksheet in Excel should look asdisplayed in Figure 12-6.","Figure 12-6. Screenshot of Excel with DataNitro plot of Apple stock price data","From a finance point of view, it seems most interesting to expose user-defined functions(UDFs) via DataNitro to Excel. This option has to be enabled in the Settings menuof DataNitro. Once this is enabled, you can import a Python script with DataNitrocalled functions.py. All Pythonfunctions included in this file-and they have to be inthis particular file-will then be directly callable from Excel. Consider the by now well-known function to value European call options in the Black-Scholes-Merton model inExample 12-2.Example 12-2. Python script for import with DataNitro into Excel## Valuation of European call options in BSM model# for use with DataNitro and Excel spreadsheets# functions.py## analytical Black-Scholes-Merton (BSM) formuladefbsm_call_value(S0,K,T,r,sigma):''' Valuation of European call option in BSM model.    Analytical formula.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    S0 : float        initial stock/index level","    K : float        strike price    T : float        time-to-maturity (for t%3D0)    r : float        constant risk-free short rate    sigma : float        volatility factor in diffusion term    Returns    %3D%3D%3D%3D%3D%3D%3D    value : float        present value of the European call option    '''frommathimportlog,sqrt,expfromscipyimportstatsS0%3Dfloat(S0)d1%3D(log(S0/K)%2B(r%2B0.5*sigma**2)*T)/(sigma*sqrt(T))d2%3D(log(S0/K)%2B(r-0.5*sigma**2)*T)/(sigma*sqrt(T))value%3D(S0*stats.norm.cdf(d1,0.0,1.0)-K*exp(-r*T)*stats.norm.cdf(d2,0.0,1.0))returnvalueIf this script is imported via DataNitro, with UDFs enabled, you can use the valuationformula from Excel. In practice, you can then type the following into an Excel cell:%3D bsm_call_value(B1, 100, 2.0, B4, B5)This is the same then as with any other Excel formula. For example, have a look atFigure 12-7. In the upper-left corner you see a parameterization of the Black-Scholes-Merton model. When you click on Insert Functionin the FORMULAStab of Excel, youcan enter a function dialog for the option valuation formula from Python(you find itunder DataNitro functions). Once you have provided references to the cells contain%E2%80%90ing the single parameter values, Python calculates the option value and returns the resultto Excel.","Figure 12-7. Screenshot of Excel function dialog for Python functionAlthough this function is not that computationally demanding, it illustrates how toharness the analytical power of Python from Excel and how to expose the results directlyto Excel (cells). Similarly, see Figure 12-8. Here, we use a parameter grid to calculatemultiple option values at once. The formula in cell D11 then takes on the form:%3D bsm_call_value(%24B%241, %24A11, D%248, %24B%244, %24B%245)Figure 12-8. Screenshot of Excel with parameter grid for European option values","Whereas in a previous example we plotted data contained in an Excel spreadsheet, wecan now also plot data generated with Python in our spreadsheet. Figure 12-9 shows a3D plot generated with Excel for the European option value surface.Figure 12-9. Screenshot of Excel with European option value surface plot","At the time of this writing, a new contender in the Python-Excelintegration world hasemerged: xlwings. xlwings provides almost all the functionality for interacting withand scripting Excel spreadsheets with Python. It is, in contrast to the DataNitro solu%E2%80%90tion, an open source library and can be freely shipped with any spreadsheet. The receiverof an xlwings%E2%80%9Cpowered%E2%80%9D spreadsheet only needs a (minimal) Python installation. Oneadvantage of xlwings is that it works with Excel both on Windows and Apple/Macoperating systems. In addition, it is well documented, although it is only in early release(0.3 at the time of this writing). The whole solution and approach look promising. andanybody interested in integrating Python and Excel should give it a try.","There are several options to integrate Python with Excel. Some Python libraries-likexlwt or xlsxwriter-allow the creation of Excel spreadsheets. Other libraries like xlrdallow the reading of arbitrary spreadsheet files, or they allow both reading and writingof spreadsheet files.","pandasis, at least for some tasks, also helpful. For example, it is useful when it comesto writing larger data sets to a spreadsheet file or when it comes to reading data storedin such a file format.The most powerful solution, however, at the time of this writing is the one by DataNitrothat offers a tight integration of both worlds. It has similar (or even better) spreadsheetmanipulation capabilities than other libraries. In addition, DataNitroallows us, forexample, to expose Python plots to Excel spreadsheets. More importantly, it allows usto define user-defined Pythonfunctions (UDFs) for usage with Excel that are callablein the same way as Excel's built-in functions are. xlwings, a new, open source librarythat has been made available recently, is similar in scope and capabilities to the DataNitro solution.In particular, the DataNitro and xlwings approaches allow us to use Excel as a flexibleand powerful general GUI-available on almost every computer in the financeindustry-and combine it with the analytical capabilities of Python. The best of bothworlds, so to say.","For all libraries and solutions presented in this chapter, there are helpful web resourcesavailable:%E2%80%A2For xlrd and xlwt, see http://www.python-excel.orgfor the online documentation%3Bthere is also a tutorial available in PDFformat at http://www.simplistix.co.uk/presentations/python-excel.pdf.%E2%80%A2xlsxwriteris nicely documented on the website http://xlsxwriter.readthedocs.org.%E2%80%A2OpenPyxl has its home here: http://pythonhosted.org/openpyxl/.%E2%80%A2For detailed information about PyXLL, see https://www.pyxll.com.%E2%80%A2Free trials and detailed documentation for DataNitro can be found at http://www.datanitro.com.%E2%80%A2You can find the documentation and everything else you need regarding xlwingsat http://xlwings.org.","First, solve the problem. Then, write the code.- Jon JohnsonObject orientation has its fans and critics. Referring to the quote above, object-orientedimplementation styles might provide the most benefit when they are applied by pro%E2%80%90grammers who really understand the problem at hand and when there is much to gainfrom abstraction and generalization. On the other hand, if you do not know what exactlyto do, a different, more interactive and exploratory programming style, like proceduralprogramming, might be a better choice.In this chapter, we do not want to discuss the risks and merits of using object orientation.We take it for granted that this approach has its place when it comes to the developmentof more complex financial applications (cf. the project implemented in Part IIIof thebook) and that it brings along a number of measurable benefits in these cases. When itcomes to building graphical user interfaces (GUIs), object orientation in general is aconditio sine qua non.Therefore, we combine the two topics in this chapter and introduce first fundamentalconcepts of Python classes and objects. Equipped with this knowledge, it is much easierto introduce the development of GUIs.","Wikipedia provides the following definition for object-oriented programming:Object-oriented programming (OOP) is a programming paradigm that represents con%E2%80%90cepts as %E2%80%9Cobjects%E2%80%9D that have data fields (attributes that describe the object) and associatedprocedures known as methods. Objects, which are usually instances of classes, are usedto interact with one another to design applications and computer programs.","This already provides the main technical terms that are also used in the Python worldfor classes and objects and that will be made clearer in the remainder of this section.","We start by defining a new class (of objects). To this end, use the statement class, whichis applied like a def statement for function definitions. The following code defines anew Python class named ExampleOne. This class does nothing but %E2%80%9Cexist.%E2%80%9D The passcommand simply does what its name says-it passes and does nothing:In%5B1%5D:classExampleOne(object):passHowever, the existence of the class ExampleOne allows us to generate instances of theclass as new Python objects:In%5B2%5D:c%3DExampleOne()In addition, since this class inherits from the general object class, it already has somebatteries included. For example, the following provides the string representation of thenewly generated object based on our class:In%5B3%5D:c.__str__()Out%5B3%5D: '%3C__main__.ExampleOne object at 0x7f8fcc28ef10%3E'We can also use typeto learn about the type of the object-in this case, an instance ofthe class ExampleOne:In%5B4%5D:type(c)Out%5B4%5D: __main__.ExampleOneLet us now define a class that has two attributes, say, a and b. To this end, we define aspecial method called init that is automatically invoked at every instantiation of theclass. Note that the object itself-i.e., by Python convention, self-is also a parameterof this function:In%5B5%5D:classExampleTwo(object):def__init__(self,a,b):self.a%3Daself.b%3DbInstantiating the new class ExampleTwo now takes two values, one for attribute aandone for attribute b. Note in the preceding definition that these attributes are referencedinternally (i.e., in the class definition) by self.a and self.b, respectively:In%5B6%5D:c%3DExampleTwo(1,'text')Similarly, we can access the values of the attributes of the object c as follows:In%5B7%5D:c.a","Out%5B7%5D: 1In%5B8%5D:c.bOut%5B8%5D: 'text'We can also overwrite our initial values by simply assigning new values to the attributes:In%5B9%5D:c.a%3D100In%5B10%5D:c.aOut%5B10%5D: 100Python is quite flexible when it comes to the use of classes and objects. For example,attributes of an object can be defined even after instantiation, as the following exampleillustrates:In%5B11%5D:c%3DExampleOne()In%5B12%5D:c.first_name%3D'Jason'c.last_name%3D'Bourne'c.movies%3D4In%5B13%5D:printc.first_name,c.last_name,c.moviesOut%5B13%5D: Jason Bourne 4The class definition that follows introduces methods for classes. In this case, the classExampleThreeis the same as ExampleTwoapart from the fact that there is a definitionfor a custom method, addition:In%5B14%5D:classExampleThree(object):def__init__(self,a,b):self.a%3Daself.b%3Dbdefaddition(self):returnself.a%2Bself.bInstantiation works as before. This time we use only integers for the attributes a and b:In%5B15%5D:c%3DExampleThree(10,15)A call of the method additionthen returns the sum of the two attribute values (as longas it is defined, given the types of the attributes):In%5B16%5D:c.addition()Out%5B16%5D: 25In%5B17%5D:c.a%2B%3D10c.addition()Out%5B17%5D: 35One of the advantages of the object-oriented programming paradigm is reusability. Aspointed out, the class definitions for ExampleTwo and ExampleThree are only different","with respect to the custom method definition. Another way of defining the class ExampleThree is therefore to use the class ExampleTwo and to inherit from this class thedefinition of the special function init:In%5B18%5D:classExampleFour(ExampleTwo):defaddition(self):returnself.a%2Bself.bThe behavior of instances of class ExampleFour is now exactly the same as that of in%E2%80%90stances of class ExampleThree:In%5B19%5D:c%3DExampleFour(10,15)In%5B20%5D:c.addition()Out%5B20%5D: 25Pythonallows for multiple inheritances. However, one should be careful with regard toreadability and maintainability, especially by others:In%5B21%5D:classExampleFive(ExampleFour):defmultiplication(self):returnself.a*self.bIn%5B22%5D:c%3DExampleFive(10,15)In%5B23%5D:c.addition()Out%5B23%5D: 25In%5B24%5D:c.multiplication()Out%5B24%5D: 150For example, custom method definitions do not necessarily need to be included in theclass definition itself. They can be placed somewhere else, and as long as they are in theglobal namespace, they can be used within a class definition. The following code illus%E2%80%90trates this approach:In%5B25%5D:defmultiplication(self):returnself.a*self.bIn%5B26%5D:classExampleSix(ExampleFour):multiplication%3DmultiplicationAnd again, the instance of the class ExampleSix behaves exactly the same as the instanceof the earlier class ExampleFive:In%5B27%5D:c%3DExampleSix(10,15)In%5B28%5D:c.addition()Out%5B28%5D: 25In%5B29%5D:c.multiplication()Out%5B29%5D: 150","It might be helpful to have (class/object) private attributes. These are generally indicatedby one or two leading underscores, as the following class definition illustrates:In%5B30%5D:classExampleSeven(object):def__init__(self,a,b):self.a%3Daself.b%3Dbself.__sum%3Da%2Bbmultiplication%3Dmultiplicationdefaddition(self):returnself.__sumThe behavior is the same as before when it comes to a call of the method addition:In%5B31%5D:c%3DExampleSeven(10,15)In%5B32%5D:c.addition()Out%5B32%5D: 25Here, you cannot directly access the private attribute sum. However, via the followingsyntax, it is still possible:In%5B33%5D:c._ExampleSeven__sumOut%5B33%5D: 25As the class ExampleSevenis defined, one must be careful with the inner workings. Forexample, a change of an attribute value does not change the result of the additionmethod call:In%5B34%5D:c.a%2B%3D10c.aOut%5B34%5D: 20In%5B35%5D:c.addition()Out%5B35%5D: 25This, of course, is due to the fact that the private attribute is not updated:In%5B36%5D:c._ExampleSeven__sumOut%5B36%5D: 25Calling the multiplication method, however, works as desired:In%5B37%5D:c.multiplication()Out%5B37%5D: 300To conclude the introduction into the main concepts of Python classes and objects, wewant to pick one other special method of importance: the iter method. It is calledwhenever an iteration over an instance of a class is asked for. To begin with, define a listof first names as follows:","In%5B38%5D:name_list%3D%5B'Sandra','Lilli','Guido','Zorro','Henry'%5DIn Python it is usual to iterate over such lists directly-i.e., without the use of integercounters or indexes:In%5B39%5D:fornameinname_list:printnameOut%5B39%5D: Sandra         Lilli         Guido         Zorro         HenryWe are now going to define a new Python class that also returns values from a list, butthe list is sorted before the iterator starts returning values from the list. The class sorted_list contains the following definitions:initTo initialize the attribute elements we expect a list object, which we sort at in%E2%80%90stantiation.iterThis special method is called whenever an iteration is desired%3B it needs a definitionof a next method.nextThis method defines what happens per iteration step%3B it starts at index valueself.position %3D -1 and increases the value by 1 per call%3B it then returns the valueof elements at the current index value of self.position.The class definition looks like this:In%5B40%5D:classsorted_list(object):def__init__(self,elements):self.elements%3Dsorted(elements)# sorted list objectdef__iter__(self):self.position%3D-1returnselfdefnext(self):ifself.position%3D%3Dlen(self.elements)-1:raiseStopIterationself.position%2B%3D1returnself.elements%5Bself.position%5DInstantiate the class now with the name_list object:In%5B41%5D:sorted_name_list%3Dsorted_list(name_list)The outcome is as desired-iterating over the new object returns the elements in al%E2%80%90phabetical order:","In%5B42%5D:fornameinsorted_name_list:printnameOut%5B42%5D: Guido         Henry         Lilli         Sandra         ZorroIn principle, we have replicated a call of the function sorted, which takes as input alist object and returns as output a list object:In%5B43%5D:type(sorted(name_list))Out%5B43%5D: listIn%5B44%5D:fornameinsorted(name_list):printnameOut%5B44%5D: Guido         Henry         Lilli         Sandra         ZorroOur approach, however, works on a completely new type of object-namely, a sorted_list:In%5B45%5D:type(sorted_name_list)Out%5B45%5D: __main__.sorted_listThis concludes the rather concise introduction into selected concepts of object orien%E2%80%90tation in Python. In the following discussion, these concepts are illustrated by intro%E2%80%90ductory financial use cases. In addition, Part IIImakes extensive use of object-orientedprogramming to implement a derivatives analytics library.","One of the most fundamental concepts in finance is discounting. Since it is so funda%E2%80%90mental, it might justify the definition of a discounting class. In a constant short rateworld with continuous discounting, the factor to discount a future cash flow due at datet %3E 0 to the present t %3D 0 is defined by D0t%3De%E2%88%92rt.Consider first the following function definition, which returns the discount factor fora given future date and a value for the constant short rate. Note that a NumPy universalfunction is used in the function definition for the exponential function to allow forvectorization:In%5B46%5D:importnumpyasnpdefdiscount_factor(r,t):''' Function to calculate a discount factor.","             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             r : float                 positive, constant short rate             t : float, array of floats                 future date(s), in fraction of years%3B                 e.g. 0.5 means half a year from now             Returns             %3D%3D%3D%3D%3D%3D%3D             df : float                 discount factor             '''df%3Dnp.exp(-r*t)# use of NumPy universal function for vectorizationreturndfFigure 13-1illustrates how the discount factors behave for different values for the con%E2%80%90stant short rate over five years. The factors for t %3D 0 are all equal to 1%3B i.e., %E2%80%9Cno discounting%E2%80%9Dof today's cash flows. However, given a short rate of 10%25 and a cash flow due in fiveyears, the cash flow would be discounted to a value slightly above 0.6 per currency unit(i.e., to 60%25). We generate the plot as follows:In%5B47%5D:importmatplotlib.pyplotasplt%25matplotlibinlineIn%5B48%5D:t%3Dnp.linspace(0,5)forrin%5B0.01,0.05,0.1%5D:plt.plot(t,discount_factor(r,t),label%3D'r%3D%254.2f'%25r,lw%3D1.5)plt.xlabel('years')plt.ylabel('discount factor')plt.grid(True)plt.legend(loc%3D0)For comparison, now let us look at the class-based implementation approach. We callit short_rate since this is the central entity/object and the derivation of discount factorsis accomplished via a method call:","Figure 13-1. Discount factors for different short rates over five yearsIn%5B49%5D:classshort_rate(object):''' Class to model a constant short rate object.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             name : string                 name of the object             rate : float                 positive, constant short rate             Methods             %3D%3D%3D%3D%3D%3D%3D             get_discount_factors :                 returns discount factors for given list/array                 of dates/times (as year fractions)             '''def__init__(self,name,rate):self.name%3Dnameself.rate%3Dratedefget_discount_factors(self,time_list):''' time_list : list/array-like '''time_list%3Dnp.array(time_list)returnnp.exp(-self.rate*time_list)To start with, define sr to be an instance of the class short_rate:In%5B50%5D:sr%3Dshort_rate('r',0.05)In%5B51%5D:sr.name,sr.rateOut%5B51%5D: ('r', 0.05)To get discount factors from the new object, a time list with year fractions is needed:In%5B52%5D:time_list%3D%5B0.0,0.5,1.0,1.25,1.75,2.0%5D# in year fractionsIn%5B53%5D:sr.get_discount_factors(time_list)","Out%5B53%5D: array(%5B 1.        ,  0.97530991,  0.95122942,  0.93941306,  0.91621887,                 0.90483742%5D)Using this object, it is quite simple to generate a plot as before (see Figure 13-2). Themajor difference is that we first update the attribute rateand then provide the time listt to the method get_discount_factors:In%5B54%5D:forrin%5B0.025,0.05,0.1,0.15%5D:sr.rate%3Drplt.plot(t,sr.get_discount_factors(t),label%3D'r%3D%254.2f'%25sr.rate,lw%3D1.5)plt.xlabel('years')plt.ylabel('discount factor')plt.grid(True)plt.legend(loc%3D0)Figure 13-2. Discount factors for different short rates over five yearsGenerally, discount factors are %E2%80%9Conly%E2%80%9D a means to an end. For example, you might wantto use them to discount future cash flows. With our short rate object, this is an easyexercise when we have the cash flows and the dates/times of their occurrence available.Consider the following cash flow example, where there is a negative cash flow today andpositive cash flows after one year and two years, respectively. This could be the cashflow profile of an investment opportunity:In%5B55%5D:sr.rate%3D0.05cash_flows%3Dnp.array(%5B-100,50,75%5D)time_list%3D%5B0.0,1.0,2.0%5DWith the time_list object, discount factors are only one method call away:In%5B56%5D:disc_facts%3Dsr.get_discount_factors(time_list)In%5B57%5D:disc_factsOut%5B57%5D: array(%5B 1.        ,  0.95122942,  0.90483742%5D)","Present values for all cash flows are obtained by multiplying the discount factors by thecash flows:In%5B58%5D:# present valuesdisc_facts*cash_flowsOut%5B58%5D: array(%5B-100.        ,   47.56147123,   67.86280635%5D)A typical decision rule in investment theory says that a decision maker should investinto a project whenever the net present value (NPV), given a certain (short) rate rep%E2%80%90resenting the opportunity costs of the investment, is positive. In our case, the NPV issimply the sum of the single present values:In%5B59%5D:# net present valuenp.sum(disc_facts*cash_flows)Out%5B59%5D: 15.424277577732667Obviously, for a short rate of 5%25 the investment should be made. What about a rate of15%25%3F Then the NPV becomes negative, and the investment should not be made:In%5B60%5D:sr.rate%3D0.15np.sum(sr.get_discount_factors(time_list)*cash_flows)Out%5B60%5D: -1.4032346276182679","With the experience gained through the previous example, the definition of anotherclass to model a cash flow series should be straightforward. This class should providemethods to give back a list/array of present values and also the net present value for agiven cash flow series-i.e., cash flow values and dates/times:In%5B61%5D:classcash_flow_series(object):''' Class to model a cash flow series.             Attributes             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             name : string                 name of the object             time_list : list/array-like                 list of (positive) year fractions             cash_flows : list/array-like                 corresponding list of cash flow values             short_rate : instance of short_rate class                 short rate object used for discounting             Methods             %3D%3D%3D%3D%3D%3D%3D             present_value_list :                 returns an array with present values             net_present_value :                 returns NPV for cash flow series","             '''def__init__(self,name,time_list,cash_flows,short_rate):self.name%3Dnameself.time_list%3Dtime_listself.cash_flows%3Dcash_flowsself.short_rate%3Dshort_ratedefpresent_value_list(self):df%3Dself.short_rate.get_discount_factors(self.time_list)returnnp.array(self.cash_flows)*dfdefnet_present_value(self):returnnp.sum(self.present_value_list())We use all objects from the previous example to instantiate the class:In%5B62%5D:sr.rate%3D0.05cfs%3Dcash_flow_series('cfs',time_list,cash_flows,sr)In%5B63%5D:cfs.cash_flowsOut%5B63%5D: array(%5B-100,   50,   75%5D)In%5B64%5D:cfs.time_listOut%5B64%5D: %5B0.0, 1.0, 2.0%5DWe can now compare the present values and the NPV with the results from before.Fortunately, we get the same results:In%5B65%5D:cfs.present_value_list()Out%5B65%5D: array(%5B-100.        ,   47.56147123,   67.86280635%5D)In%5B66%5D:cfs.net_present_value()Out%5B66%5D: 15.424277577732667There is further potential to generalize the steps of the previous example. One optionis to define a new class that provides a method for calculating the NPV for differentshort rates-i.e., a sensitivity analysis. We use, of course, the cash_flow_seriesclass toinherit from:In%5B67%5D:classcfs_sensitivity(cash_flow_series):defnpv_sensitivity(self,short_rates):npvs%3D%5B%5Dforrateinshort_rates:sr.rate%3Dratenpvs.append(self.net_present_value())returnnp.array(npvs)In%5B68%5D:cfs_sens%3Dcfs_sensitivity('cfs',time_list,cash_flows,sr)For example, defining a list containing different short rates, we can easily compare theresulting NPVs:In%5B69%5D:short_rates%3D%5B0.01,0.025,0.05,0.075,0.1,0.125,0.15,0.2%5D","In%5B70%5D:npvs%3Dcfs_sens.npv_sensitivity(short_rates)npvsOut%5B70%5D: array(%5B 23.01739219,  20.10770244,  15.42427758,  10.94027255,                  6.64667738,   2.53490386,  -1.40323463,  -8.78945889%5D)Figure 13-3 shows the result graphically. The thicker horizontal line (at 0) shows thecutoff point between a profitable investment and one that should be dismissed giventhe respective (short) rate:In%5B71%5D:plt.plot(short_rates,npvs,'b')plt.plot(short_rates,npvs,'ro')plt.plot((0,max(short_rates)),(0,0),'r',lw%3D2)plt.grid(True)plt.xlabel('short rate')plt.ylabel('net present value')Figure 13-3. Net present values of cash flow list for different short rates","For the majority of computer users, as compared to developers or data scientists, agraphical user interface (GUI) is what they are used to. Such a GUI does not only bringalong visual appeal and simplicity%3B it also allows us to guide and control user interactionmuch better than alternative approaches like interactive scripting, or use of a commandline interface or shell. In what follows, we build on the examples of the previous sectionand build simple GUIs for our short rate and cash flow series classes.To build the GUIs we use the traitslibrary, documentation of which you can find athttp://code.enthought.com/projects/traits/docs/html/index.html. traitsis generallyused for rapid GUI building on top of existing classes and only seldom for more complexapplications. In what follows, we will reimplement the two example classes from before,taking into account that we want to use a GUI for interacting with instances of therespective classes.","To start, we need to import the traits.api sublibrary:In%5B72%5D:importnumpyasnpimporttraits.apiastrapiFor the definition of our new short_rateclass, we use the HasTraitsclass to inheritfrom. Also note in the following class definition that traits has its own data types,which are generally closely intertwined with visual elements of a GUI-to put it differ%E2%80%90ently, traits knows which graphical elements (e.g., for a text field) to use to build aGUI (semi)automatically:In%5B73%5D:classshort_rate(trapi.HasTraits):name%3Dtrapi.Strrate%3Dtrapi.Floattime_list%3Dtrapi.Array(dtype%3Dnp.float,shape%3D(5,))defget_discount_factors(self):returnnp.exp(-self.rate*self.time_list)Instantiation of such a traits-based class is done as usual:In%5B74%5D:sr%3Dshort_rate()However, via a call of the method configure_traits (inherited from HasTraits) a GUIis automatically generated, and we can use this GUI to input values for the attributes ofthe new object sr:In%5B75%5D:sr.configure_traits()Figure 13-4 shows such a simple GUI, which in this case is still empty (i.e., no inputvalues have been put in the different fields). Note that the lower five fields all belong to%E2%80%9CTime list%E2%80%9D-this layout is generated by default.Figure 13-5 shows the same simple GUI, this time however with values in every singlefield. Pushing the OK button assigns the values from the input fields to the respectiveattributes of the object.","Figure 13-4. Screenshot of traits GUI (empty)Figure 13-5. Screenshot of traits GUI (with data)In effect, this gives the same results as the following lines of code:In%5B76%5D:sr.name%3D'sr_class'sr.rate%3D0.05sr.time_list%3D%5B0.0,0.5,1.0,1.5,2.0%5D","By providing the traits-specific data types, traits is able to generate the correct visualelements to accomplish these operations via a GUI-i.e. a text input field for sr.nameand five input elements for the list object sr.time_list.The behavior of the new object after the input operations is the same as with ourshort_rate from the previous section:In%5B77%5D:sr.rateOut%5B77%5D: 0.05In%5B78%5D:sr.time_listOut%5B78%5D: array(%5B 0. ,  0.5,  1. ,  1.5,  2. %5D)In%5B79%5D:sr.get_discount_factors()Out%5B79%5D: array(%5B 1.        ,  0.97530991,  0.95122942,  0.92774349,  0.90483742%5D)","So far, the new short_rate class using traits allows us to input data for initializingattributes of an instance of the class. However, a GUI usually is also used to presentresults. You would generally want to avoid providing input data via a GUI and thenmaking the user access the results via interactive scripting. To this end, we need anothersublibrary, traitsui.api:In%5B80%5D:importtraits.apiastrapiimporttraitsui.apiastruiThis sublibrary allows us to generate different views on the same class/object. It alsoprovides more options for, e.g., labeling and formatting. The key in the following classdefinition is what happens when the Updatebutton is pushed. In this case, the privatemethod %5C_update%5C_firedis called, which updates the list object containing the dis%E2%80%90count factors. This updated list is then displayed in the GUI window. A prerequisite forthis is that all input parameters have been made available by the user:In%5B81%5D:classshort_rate(trapi.HasTraits):name%3Dtrapi.Strrate%3Dtrapi.Floattime_list%3Dtrapi.Array(dtype%3Dnp.float,shape%3D(1,5))disc_list%3Dtrapi.Array(dtype%3Dnp.float,shape%3D(1,5))update%3Dtrapi.Buttondef_update_fired(self):self.disc_list%3Dnp.exp(-self.rate*self.time_list)v%3Dtrui.View(trui.Group(trui.Item(name%3D'name'),trui.Item(name%3D'rate'),trui.Item(name%3D'time_list',label%3D'Insert Time List Here'),trui.Item('update',show_label%3DFalse),trui.Item(name%3D'disc_list',label%3D'Press Update for Factors'),","show_border%3DTrue,label%3D'Calculate Discount Factors'),buttons%3D%5Btrui.OKButton,trui.CancelButton%5D,resizable%3DTrue)Again, instantiation and configuration are achieved as before:In%5B82%5D:sr%3Dshort_rate()In%5B83%5D:sr.configure_traits()Figure 13-6 shows the new, enhanced GUI, which is still empty. You see the new ele%E2%80%90ments, like the Update button and the output fields for the discount factors.Figure 13-6. Screenshot of traits GUI with updating (empty)Figure 13-7 illustrates what happens with this new GUI %E2%80%9Cin action.%E2%80%9D Providing valuesfor the object attributes and pushing the Updatebutton returns the calculated discountfactors-this time within the GUI window.The following Python code shows step-by-step the equivalent operations without a GUI.First, the assigning of values to the attributes:In%5B84%5D:sr.name%3D'sr_class'sr.rate%3D0.05sr.time_list%3Dnp.array((%5B0.0,0.5,1.0,1.5,2.0%5D,),dtype%3Dnp.float32)Second, the update of the list object containing the discount factors:In%5B85%5D:sr._update_fired()","Figure 13-7. Screenshot of traits GUI with updating (after update)Finally, the output of the calculated/updated list with the discount factors:In%5B86%5D:sr.disc_listOut%5B86%5D: array(%5B%5B 1.        ,  0.97530991,  0.95122942,  0.92774349,  0.90483742         %5D%5D)","The last example in this section is about the cash_flow_seriesclass. In principle, wehave seen in the previous example the basic workings of traits when it comes to pre%E2%80%90senting results within a GUI window. Here, we only want to add some twists to the story:for example, a slider to easily change the value for the short rate. In the class definitionthat follows, this is accomplished by using the Range function, where we provide aminimum, a maximum, and a default value. There are also more output fields to accountfor the calculation of the present values and the net present value:In%5B87%5D:classcash_flow_series(trapi.HasTraits):name%3Dtrapi.Strshort_rate%3Dtrapi.Range(0.0,0.5,0.05)time_list%3Dtrapi.Array(dtype%3Dnp.float,shape%3D(1,6))cash_flows%3Dtrapi.Array(dtype%3Dnp.float,shape%3D(1,6))disc_values%3Dtrapi.Array(dtype%3Dnp.float,shape%3D(1,6))present_values%3Dtrapi.Array(dtype%3Dnp.float,shape%3D(1,6))net_present_value%3Dtrapi.Floatupdate%3Dtrapi.Buttondef_update_fired(self):self.disc_values%3Dnp.exp(-self.short_rate*self.time_list)self.present_values%3Dself.disc_values*self.cash_flowsself.net_present_value%3Dnp.sum(self.present_values)v%3Dtrui.View(trui.Group(trui.Item(name%3D'name'),","trui.Item(name%3D'short_rate'),trui.Item(name%3D'time_list',label%3D'Time List'),trui.Item(name%3D'cash_flows',label%3D'Cash Flows'),trui.Item('update',show_label%3DFalse),trui.Item(name%3D'disc_values',label%3D'Discount Factors'),trui.Item(name%3D'present_values',label%3D'Present Values'),trui.Item(name%3D'net_present_value',label%3D'Net Present Value'),show_border%3DTrue,label%3D'Calculate Present Values'),buttons%3D%5Btrui.OKButton,trui.CancelButton%5D,resizable%3DTrue)Apart from the slightly more complex class definition, the usage is still the same:In%5B88%5D:cfs%3Dcash_flow_series()In%5B89%5D:cfs.configure_traits()Figure 13-8 shows the new GUI without any actions taken so far (i.e., empty). Noticethe slider and all the new fields for the cash flow values, the present values, and the netpresent value.Figure 13-9shows a version of the GUI where input data has been typed in already, butno other action has taken place.Finally, Figure 13-10presents the GUI with both input data and results data-i.e., afterpushing the Updatebutton. Although this is still quite a simple example, the result canalmost be considered an application. We have:InputThe GUI allows for inputting data to initialize all object attributes.LogicThere is application logic that calculates discount factors, present values, andan NPV.OutputThe GUI presents the results of applying the logic to the input data.","Figure 13-8. Screenshot of traits GUI for Cash Flow Series (empty)Figure 13-9. Screenshot of traits GUI for Cash Flow Series (with input data)","Figure 13-10. Screenshot of traits GUI for Cash Flow Series (with results)","Object-oriented paradigms are an indispensible tool for modern application develop%E2%80%90ment. Python provides a rather flexible framework for the definition of customer-defined classes and for working with instances of these classes. This chapter providesonly the fundamentals of Python class definitions-Part IIIof the book illustrates theuse of (financial) Python classes in a more complex and realistic application scenario.Modern application design generally builds on graphicaluser interfaces. The efficientbuilding of GUIs therefore is generally quite important, even in a rapid applicationdevelopment scenario. This chapter uses the traits library, which allows simple andefficient building of GUIs based on a Pythonic, object-oriented approach. Thesubsequent chapter shows how to build GUIs based on web technologies, a technicalalternative nowadays even used for in-house applications in financial institutions.","The following web resources are good starting points for Python classes and objectorientation, and for traits:%E2%80%A2The Python class documentation: https://docs.python.org/2/tutorial/classes.html","%E2%80%A2The traits documentation: http://code.enthought.com/projects/traits/docs/html/index.htmlHelpful resources in book form are:%E2%80%A2Downey, Allen (2012): Think Python. O'Reilly, Sebastopol, CA.%E2%80%A2Goodrich, Michael et al. (2013): Data Structures and Algorithms in Python.JohnWiley %26 Sons, Hoboken, NJ.%E2%80%A2Langtangen, Hans Petter (2009): A Primer on Scientific Programming with Python.Springer Verlag, Berlin, Heidelberg.","I have been quoted saying that, in the future, all companies will beInternet companies. I still believe that. More than ever, really.- Andrew GroveThe Internet, or the Web, has evolved from some separate world into something that iseverywhere and in everything. It has become a technology platform enabling a multitudeof different use cases. From a finance perspective, the following seem particularly note%E2%80%90worthy:Data provision/gatheringWeb technology allows the provision of data and the gathering thereof in a simpli%E2%80%90fied manner and generally at reduced costs%3B it also speeds up in general all associatedprocesses. Large financial data providers, like Bloomberg and Thomson Reuters,rely heavily on the Web and related technologies to provide the financial world withdata in real time.Trading/buying/sellingUsing the Web also facilitates trading of financial securities%3B even private investorstoday have access to professional trading facilities (e.g., online brokers like Inter%E2%80%90active Brokers) and can trade securities in real time.Application providingModels like Software-as-a-Service (SaaS) allow both small companies, like startups,and large ones to provide applications in an efficient manner%3B even the smallestoutfit can today reach a global target audience at very little cost. Large corporationsbenefit from web technologies, for example, when they use them to provide internalapplications that are accessible and usable via any standard web browser, insteadof installing such applications on hundreds or even thousands of differentmachines.","CommunicationOf course, the Web facilitates communication within organizations and across or%E2%80%90ganizations%3B the majority of today's business and financial communication hasmoved from paper to the Web.Commoditization/scalabilityRecent web technologies also allow for better virtualization, making web serversand servers in general a commodity that everybody can rent at rather low variablecosts and that is easily scalable when requirements change%3B computing power andstorage capacity become more and more comparable to electricity, which we are allused to getting from the plug sockets at home.Again, Pythonfor the Web is a broad topic in itself that cannot be covered by a singlechapter in this book. However, this chapter is able to cover a number of important topicsfrom a finance perspective. In particular, it covers:Web protocolsThe first section shows how to transfer files via FTPand how to access websites viaHTTP.Web plottingWeb technologies generally allow for better interactivity and for better real-timesupport than standard approaches, for example, for plotting data%3B the second sec%E2%80%90tion introduces the plotting library Bokehto generate interactive web plots and torealize real-time plotting of financial data.Web applicationsOne of Python's strengths is its powerful web frameworks to develop web-basedapplications%3B one that is really Pythonic and that has become quite popular recentlyis Flask. This chapter illustrates techniques for developing web-based applicationsusing this framework.Web servicesWeb services have become an important aspect of web-enabled applications%3B thelast section shows how to develop a simple web service for the valuation of Europeanoptions on the VSTOXX volatility index.","This section gives a rather brief overview of selected Python libraries for working withweb technologies and protocols. Several topics, like the handling of email functionalitywith Python, are not touched upon.","1.For details and background refer to http://en.wikipedia.org/wiki/Ftp.","The File Transfer Protocol(FTP) is, as the name suggests, a protocol to transfer filesover the Web.1Python provides a dedicated library to work with FTP called ftplib:In%5B1%5D:importftplibimportnumpyasnpIn what follows, we will connect to an FTP server, log in, transfer a file to the server,transfer it back to the local machine, and delete the file on the server. First, the con%E2%80%90nection:In%5B2%5D:ftp%3Dftplib.FTP('quant-platform.com')Not every FTP server is password protected, but this one is:In%5B3%5D:ftp.login(user%3D'python',passwd%3D'python')Out%5B3%5D: '230 Login successful.'To have a file that we can transfer, we generate a NumPyndarray object with some randomdata and save it to disk:In%5B4%5D:np.save('./data/array',np.random.standard_normal((100,100)))For the FTP file transfer to follow, we have to open the file for reading:In%5B5%5D:f%3Dopen('./data/array.npy','r')This open file can now be written, choosing here binary transfer, by the STORcommandin combination with the target filename:In%5B6%5D:ftp.storbinary('STOR array.npy',f)Out%5B6%5D: '226 Transfer complete.'Let us have a look at the directory of the FTP server. Indeed, the file was transferred:In%5B7%5D:ftp.retrlines('LIST')Out%5B7%5D: -rw-------    1 1001     1001        80080 Sep 29 11:05 array.npy        '226 Directory send OK.'The other way around is pretty similar. To retrieve a distant file and to save it to disk,we need to open a new file, this time in write mode:In%5B8%5D:f%3Dopen('./data/array_ftp.npy','wb').writeAgain, we choose binary transfer, and we use the RETR command for retrieving the filefrom the FTP server:In%5B9%5D:ftp.retrbinary('RETR array.npy',f)","Out%5B9%5D: '226 Transfer complete.'Since we do not need the file on the server anymore, we can delete it:In%5B10%5D:ftp.delete('array.npy')Out%5B10%5D: '250 Delete operation successful.'In%5B11%5D:ftp.retrlines('LIST')Out%5B11%5D: '226 Directory send OK.'Finally, we should close the connection to the FTP server:In%5B12%5D:ftp.close()In the local directory there are now two files, the one that was generated locally and theone generated by retrieving the file from the server:In%5B13%5D:!ls-n./data%3C%3C%3C%3C%3C%3C%3C HEADOut%5B13%5D: insgesamt 156         -rw------- 1 1000 1000 77824 Sep 15 08:14 array_ftp.npy         -rw------- 1 1000 1000 80080 Sep 15 08:14 array.npy%3D%3D%3D%3D%3D%3D%3DOut%5B13%5D: insgesamt 156         -rw------- 1 1000 1000 77824 Sep 29 17:05 array_ftp.npy         -rw------- 1 1000 1000 80080 Sep 29 17:05 array.npy%3E%3E%3E%3E%3E%3E%3E 798603793467fffcd06a9df88edf091e339dec37In%5B14%5D:!rm-f./data/arr*# cleanup directoryAll that has happened so far was done without encryption (i.e., was fully insecure). Bothlogin information and data were transferred in readable form. However, for most ap%E2%80%90plications such operations should be encrypted so others are not able to read the dataand/or steal the login information and do even worse things.ftplib can connect to FTP servers securely via the function FTP_TLS. Once such a secureconnection is established, all other operations remain the same:In%5B15%5D:ftps%3Dftplib.FTP_TLS('quant-platform.com')In%5B16%5D:ftps.login(user%3D'python',passwd%3D'python')Out%5B16%5D: '230 Login successful.'In%5B17%5D:ftps.prot_p()Out%5B17%5D: '200 PROT now Private.'In%5B18%5D:ftps.retrlines('LIST')Out%5B18%5D: '226 Directory send OK.'In%5B19%5D:ftps.close()","2.For details and background refer to http://en.wikipedia.org/wiki/Http.3.This example is for illustration purposes only. In general, you would want to use specialized libraries such aslxml or Beautiful Soup.","Another important protocol, if not the most important one on the Web, is the HyperTextTransfer Protocol (HTTP).2This protocol is used whenever a (HTML-based) web pageis displayed in the browser. The Python library to work with HTTP is called httplib:In%5B20%5D:importhttplibAs with FTP, we first need a connection to the HTTP server:In%5B21%5D:http%3Dhttplib.HTTPConnection('hilpisch.com')Once the connection is established, we can send requests, for example asking for theindex.htm page (file):In%5B22%5D:http.request('GET','/index.htm')To test whether this was successful, use the getresponse method:In%5B23%5D:resp%3Dhttp.getresponse()The returned object provides status information. Fortunately, our request wassuccessful:In%5B24%5D:resp.status,resp.reasonOut%5B24%5D: (200, 'OK')Equipped with the response object, we can now read the content as follows:In%5B25%5D:content%3Dresp.read()content%5B:100%5D# first 100 characters of the fileOut%5B25%5D: '%3C!doctype html%3E%5Cn%3Chtml lang%3D%22en%22%3E%5Cn%5Cn%5Ct%3Chead%3E%5Cn%5Ct%5Ct%3Cmeta charset%3D%22utf-         8%22%3E%5Cn%5Cn%5Ct%5Ct%3Ctitle%3EDr. Yves J. Hilpisch %5Cxe2%5Cx80'Once you have the content of a particular web page, there are many potential use cases.You might want to look up certain information, for example. You might know that youcan find the email address on the page by looking for E(in this very particular case).Since content is a string object, you can apply the find method to look for E:3In%5B26%5D:index%3Dcontent.find(' E ')indexOut%5B26%5D: 2071Equipped with the index value for the information you are looking for, you can inspectthe subsequent characters of the object:","4.There are alternatives to these libraries, like Requests, that come with a more modern API.In%5B27%5D:content%5Bindex:index%2B29%5DOut%5B27%5D: ' E contact %5Bat%5D dyjh %5Bdot%5D de'Once you are finished, you should again close the connection to the server:In%5B28%5D:http.close()","There is another Python library that supports the use of differentweb protocols. It iscalled urllib. There is also a related library called urllib2. Both libraries are designedto work with arbitrary web resources, in the spirit of the %E2%80%9Cuniform%E2%80%9D in URL(uniformresource locator).4 A standard use case, for example, is to retrieve files, like CSVdatafiles, via the Web. Begin by importing urllib:In%5B29%5D:importurllibThe application of the library's functions resembles that of both ftplib and httplib.Of course, we need a URLrepresenting the web resource of interest (HTTP or FTP server,in general). For this example, we use the URLof Yahoo! Finance to retrieve stock priceinformation in CSV format:In%5B30%5D:url%3D'http://ichart.finance.yahoo.com/table.csv%3Fg%3Dd%26ignore%3D.csv'url%2B%3D'%26s%3DYHOO%26a%3D01%26b%3D1%26c%3D2014%26d%3D02%26e%3D6%26f%3D2014'Next, one has to establish a connection to the resource:In%5B31%5D:connect%3Durllib.urlopen(url)With the connection established, read out the content by calling the readmethod onthe connection object:In%5B32%5D:data%3Dconnect.read()The result in this case is historical stock price information for Yahoo! itself:In%5B33%5D:printdataOut%5B33%5D: Date,Open,High,Low,Close,Volume,Adj Close         2014-03-06,39.60,39.98,39.50,39.66,10626700,39.66         2014-03-05,39.83,40.15,39.19,39.50,12536800,39.50         2014-03-04,38.76,39.79,38.68,39.63,16139400,39.63         2014-03-03,37.65,38.66,37.43,38.25,14714700,38.25         2014-02-28,38.55,39.38,38.22,38.67,16957100,38.67         2014-02-27,37.80,38.48,37.74,38.47,15489400,38.47         2014-02-26,37.35,38.10,37.34,37.62,15778900,37.62         2014-02-25,37.48,37.58,37.02,37.26,9756900,37.26         2014-02-24,37.23,37.71,36.82,37.42,15738900,37.42         2014-02-21,37.90,37.96,37.22,37.29,12351900,37.29","         2014-02-20,37.83,38.04,37.30,37.79,11155900,37.79         2014-02-19,38.06,38.33,37.68,37.81,15851900,37.81         2014-02-18,38.31,38.59,38.09,38.31,12096400,38.31         2014-02-14,38.43,38.45,38.11,38.23,9975800,38.23         2014-02-13,37.92,38.69,37.79,38.52,12088100,38.52         2014-02-12,38.60,38.91,38.03,38.11,14088500,38.11         2014-02-11,38.15,38.86,38.09,38.50,18348000,38.50         2014-02-10,38.00,38.13,37.25,37.76,17642900,37.76         2014-02-07,36.65,37.27,36.24,37.23,16178500,37.23         2014-02-06,35.65,36.75,35.61,36.24,14250000,36.24         2014-02-05,35.60,35.94,34.99,35.49,14022900,35.49         2014-02-04,35.11,35.86,34.86,35.66,21082500,35.66         2014-02-03,35.94,36.01,34.66,34.90,22195200,34.90The library also provides convenience functions to customize URL strings. For example,you might want to be able to parameterize the symbol to look up and the starting date.To this end, define a new URL string with a string replacement part where you can insertthe parameters:In%5B34%5D:url%3D'http://ichart.finance.yahoo.com/table.csv%3Fg%3Dd%26ignore%3D.csv'url%2B%3D'%26%25s'# for replacement with parametersurl%2B%3D'%26d%3D06%26e%3D30%26f%3D2014'The function urlencode takes as an argument a Python dictionary with the parameternames and the values to associate:In%5B35%5D:params%3Durllib.urlencode(%7B's':'MSFT','a':'05','b':1,'c':2014%7D)As result, there is a stringobject that can be inserted into the preceding URL string tocomplete it:In%5B36%5D:paramsOut%5B36%5D: 'a%3D05%26s%3DMSFT%26b%3D1%26c%3D2014'In%5B37%5D:url%25paramsOut%5B37%5D: 'http://ichart.finance.yahoo.com/table.csv%3Fg%3Dd%26ignore%3D.csv%26a%3D05%26s%3DMSFT%26         b%3D1%26c%3D2014%26d%3D06%26e%3D30%26f%3D2014'Equipped with this new URLstring, establish a connection and read the data from theconnection:In%5B38%5D:connect%3Durllib.urlopen(url%25params)In%5B39%5D:data%3Dconnect.read()The result again is stock price data, this time for more dates and for Microsoft:","In%5B40%5D:printdataOut%5B40%5D: Date,Open,High,Low,Close,Volume,Adj Close         2014-07-30,44.07,44.10,43.29,43.58,31921400,43.31         2014-07-29,43.91,44.09,43.64,43.89,27763100,43.62         2014-07-28,44.36,44.51,43.93,43.97,29684200,43.70         2014-07-25,44.30,44.66,44.30,44.50,26737700,44.22         2014-07-24,44.93,45.00,44.32,44.40,30725300,44.12         2014-07-23,45.45,45.45,44.62,44.87,52362900,44.59         2014-07-22,45.00,45.15,44.59,44.83,43095800,44.55         2014-07-21,44.56,45.16,44.22,44.84,37604400,44.56         2014-07-18,44.65,44.84,44.25,44.69,43407500,44.41         2014-07-17,45.45,45.71,44.25,44.53,82180300,44.25         2014-07-16,42.51,44.31,42.48,44.08,63318000,43.81         2014-07-15,42.33,42.47,42.03,42.45,28748700,42.19         2014-07-14,42.22,42.45,42.04,42.14,21881100,41.88         2014-07-11,41.70,42.09,41.48,42.09,24083000,41.83         2014-07-10,41.37,42.00,41.05,41.69,21854700,41.43         2014-07-09,41.98,41.99,41.53,41.67,18445900,41.41         2014-07-08,41.87,42.00,41.61,41.78,31218200,41.52         2014-07-07,41.75,42.12,41.71,41.99,21952400,41.73         2014-07-03,41.91,41.99,41.56,41.80,15969300,41.54         2014-07-02,41.73,41.90,41.53,41.90,20208100,41.64         2014-07-01,41.86,42.15,41.69,41.87,26917000,41.61         2014-06-30,42.17,42.21,41.70,41.70,30805500,41.44         2014-06-27,41.61,42.29,41.51,42.25,74640000,41.99         2014-06-26,41.93,41.94,41.43,41.72,23604400,41.46         2014-06-25,41.70,42.05,41.46,42.03,20049100,41.77         2014-06-24,41.83,41.94,41.56,41.75,26509100,41.49         2014-06-23,41.73,42.00,41.69,41.99,18743900,41.73         2014-06-20,41.45,41.83,41.38,41.68,47764900,41.42         2014-06-19,41.57,41.77,41.33,41.51,19828200,41.25         2014-06-18,41.61,41.74,41.18,41.65,27097000,41.39         2014-06-17,41.29,41.91,40.34,41.68,22518600,41.42         2014-06-16,41.04,41.61,41.04,41.50,24205300,41.24         2014-06-13,41.10,41.57,40.86,41.23,26310000,40.97         2014-06-12,40.81,40.88,40.29,40.58,29818900,40.33         2014-06-11,40.93,41.07,40.77,40.86,18040000,40.61         2014-06-10,41.03,41.16,40.86,41.11,15117700,40.85         2014-06-09,41.39,41.48,41.02,41.27,15019200,41.01         2014-06-06,41.48,41.66,41.24,41.48,24060500,41.22         2014-06-05,40.59,41.25,40.40,41.21,31865200,40.95         2014-06-04,40.21,40.37,39.86,40.32,23209000,40.07         2014-06-03,40.60,40.68,40.25,40.29,18068900,40.04         2014-06-02,40.95,41.09,40.68,40.79,18504300,40.54The function urlretrieve allows us to retrieve content and save it to disk in a singlestep, which is quite convenient in many circumstances:In%5B41%5D:urllib.urlretrieve(url%25params,'./data/msft.csv')Out%5B41%5D: ('./data/msft.csv', %3Chttplib.HTTPMessage instance at 0x7f92ca59afc8%3E)","5.For more information on interactive plots with matplotlib, refer to the library's home page.A brief inspection of the content of the saved file shows that we have indeed retrievedand saved the same content as before:In%5B42%5D:csv%3Dopen('./data/msft.csv','r')csv.readlines()%5B:5%5DOut%5B42%5D: %5B'Date,Open,High,Low,Close,Volume,Adj Close%5Cn',          '2014-07-30,44.07,44.10,43.29,43.58,31921400,43.31%5Cn',          '2014-07-29,43.91,44.09,43.64,43.89,27763100,43.62%5Cn',          '2014-07-28,44.36,44.51,43.93,43.97,29684200,43.70%5Cn',          '2014-07-25,44.30,44.66,44.30,44.50,26737700,44.22%5Cn'%5DIn%5B43%5D:!rm-f./data/*","Chapter 5 introduces matplotlib, the most popular plotting library for Python. How%E2%80%90ever, as powerful as it might be for 2D and 3D plotting, its strength lies in static plotting.In fact, matplotlibis also able to generate interactive plots, e.g., with sliders for vari%E2%80%90ables. But it is safe to say that this is not one of its strengths.5This section starts with generating static plots, then proceeds to interactiveplots tofinally arrive at real-time plotting.","First, a brief benchmark example using the pandas library based on a financial timeseries from the Yahoo! Finance API, as used in the previous section:In%5B44%5D:importnumpyasnpimportpandasaspd%25matplotlibinlineAs shown in Chapter 6, using pandas makes data retrieval from the Web in general quiteconvenient. We do not even have to use additional libraries, such as urllib-almosteverything happens under the hood. The following retrieves historical stock pricequotes for Microsoft Inc. and stores the data in a DataFrame object:In%5B45%5D:url%3D'http://ichart.yahoo.com/table.csv%3Fs%3DMSFT%26a%3D0%26b%3D1%26c%3D2009'data%3Dpd.read_csv(url,parse_dates%3D%5B'Date'%5D)pandas accepts column names as parameter values for the xand ycoordinates. Theresult is shown in Figure 14-1:In%5B46%5D:data.plot(x%3D'Date',y%3D'Close')","Figure 14-1. Historical stock prices for Microsoft since January 2009 (matplotlib)Graphics and plots like Figure 14-1 can of course also be used in a web context. Forexample, it is straightforward to save plots generated with matplotlibas files in the PNG(Portable Network Graphics) format and to include such files in a website. However,recent web technologies typically also provide interactivity, like panning or zooming.Bokehis a library that explicitly aims at providing modern, interactive web-based plotsto Python. According to its website:Bokeh is a Python interactive visualization library for large data sets that natively usesthe latest web technologies. Its goal is to provide elegant, concise construction of novelgraphics in the style of Protovis/D3, while delivering high-performance interactivity overlarge data to thin clients.Three elements of this description are noteworthy:Large data setsIt is a %E2%80%9Cplotting problem%E2%80%9D in itself to plot large data sets. Just imagine a scatter plotwith 1,000,000 points-in general, large parts of the information get lost%3B Bokehprovides built-in help in this regard.Latest web technologiesIn general, JavaScriptis the language of choice as of today when it comes to webdevelopment and visualization%3B it underlies libraries such as D3(Data-Driven Documents) and also Bokeh.High-performance interactivityOn the Web, people are used to real-time interactivity (think modern browsergames), which can become an issue when visualizing and interacting with large datasets%3B Bokeh also provides built-in capabilities to reach this goal.","On a fundamental level, working with Bokeh is not that different from working withmatplotlib. However, the default output generally is not a standard window or, forexample, an IPython Notebook (which is also an option). It is a separate HTML file:In%5B47%5D:importbokeh.plottingasbpIn%5B48%5D:bp.output_file(%22../images/msft_1.html%22,title%3D%22Bokeh Example (Static)%22)# use: bp.output_notebook(%22default%22)# for output within an IPython NotebookIn terms of plotting, Bokeh provides a wealth of different plotting styles that are con%E2%80%90tinuously enhanced. To start with the simplest one, consider the following code thatgenerates a line plot similar to our pandas/matplotlibbenchmark plot. The result isshown as Figure 14-2. Apart from the x and y coordinates, all other parameters areoptional:In%5B49%5D:bp.line(data%5B'Date'%5D,# x coordinatesdata%5B'Close'%5D,# y coordinatescolor%3D'#0066cc',# set a color for the linelegend%3D'MSFT',# attach a legend labeltitle%3D'Historical Stock Quotes',# plot titlex_axis_type%3D'datetime',# datetime information on x-axistools%3D'')bp.show()In the tradition of matplotlib, Bokeh also has a gallery showcasing different plot styles.","Figure 14-2. Screenshot of HTML-based Bokeh plot","The next step is to add interactivity to the web-based plot. Available interactivity ele%E2%80%90ments (%E2%80%9Ctools%E2%80%9D) include:panSupports panning of the plot (like panning with a movie camera)%3B i.e., moving theplot (including x and y coordinates) relative to the fixed plotting frame","6.The majority of graphics formats matplotlibcan export to are static by nature (i.e., bitmaps). A counter%E2%80%90example is graphics in SVG(Scalable Vector Graphics) format, which can be programmed in JavaScript/ECMAScript. The library's website provides some examples of how to do this.wheel_zoomEnables zooming into the plot by using the mouse wheelbox_zoomEnables zooming into the plot by marking a box with the mouseresetResets the original/default view of the plotpreviewsaveGenerates a static (bitmap) version of the plot that can be saved in PNG formatThe following code demonstrates adding these tools:In%5B50%5D:bp.output_file(%22../images/msft_2.html%22,title%3D%22Bokeh Example (Interactive)%22)bp.line(data%5B'Date'%5D,data%5B'Close'%5D,color%3D'#0066cc',legend%3D'MSFT',title%3D'Historical Stock Quotes',x_axis_type%3D%22datetime%22,tools%3D'pan, wheel_zoom, box_zoom, reset, previewsave'# adding a list of interactive tools)bp.show()The output of this code is shown as Figure 14-3, where the panning function is used tomove the plot within the plotting frame (compare this with Figure 14-2).In principle, all the features shown so far can also be implemented by using matplotlib. In fact, the interactive tools shown for Bokehare available by default with matplotlib when you plot into a separate window. Figure 14-4 shows a zoomed and pannedversion of the pandas plot in Figure 14-1 in a separate (Python-controlled) window.However, in contrast to Bokeh, matplotlib cannot %E2%80%9Cexport%E2%80%9D this functionality to beincluded in a separate, standalone graphics file.6","Figure 14-3. Screenshot of HTML-based Bokeh plot with interactive elements","Figure 14-4. Screenshot of pandas/matplotlib-based plot with interactive elements","The previous subsection shows how easy it is to generate interactive, web-based plotswith Bokeh. However, Bokeh shines when it comes to real-time visualization of, forexample, high-frequency financial data. Therefore, this subsection contains examplesfor two different real-time APIs, one for FX (foreign exchange) data in JSON(JavaScriptObject Notation) format and one for intraday tick data for stock prices delivered in","CSV text file format. Apart from the visualization aspect, how to read out data from suchAPIs is also of interest.","Our first example is based on a JSON API for, among others, FX rates. Some importsfirst:In%5B51%5D:importtimeimportpandasaspdimportdatetimeasdtimportrequestsThe API we use is from OANDA, an FX online broker. This broker offers an API sandboxthat provides random/dummy data that resembles real exchange rates. Our example isbased on the EUR%E2%80%93USD exchange rate (cf. the API guide):In%5B52%5D:url%3D'http://api-sandbox.oanda.com/v1/prices%3Finstruments%3D%25s'# real-time FX (dummy!) data from JSON APITo connect to the API we use the requestslibrary whose aim is to improve the interfacefor %E2%80%9Chumans%E2%80%9D when interacting with web resources:In%5B53%5D:instrument%3D'EUR_USD'api%3Drequests.get(url%25instrument)With the open connection, data in JSON format is simply read by calling the methodjson on the connection object:In%5B54%5D:data%3Dapi.json()dataOut%5B54%5D: %7Bu'prices': %5B%7Bu'ask': 1.25829,            u'bid': 1.2582,            u'instrument': u'EUR_USD',            u'time': u'2014-09-29T06:14:34.749878Z'%7D%5D%7DUnfortunately, the data is not yet completely in the format we would like it to have.Therefore, we transform it a bit. The following code takes only the first element of thelist object stored under the key %E2%80%9Cprices.%E2%80%9D The resulting object is a standard dictobject:In%5B55%5D:data%3Ddata%5B'prices'%5D%5B0%5DdataOut%5B55%5D: %7Bu'ask': 1.25829,          u'bid': 1.2582,          u'instrument': u'EUR_USD',          u'time': u'2014-09-29T06:14:34.749878Z'%7DSince we collect such small data sets at a high frequency, we use a DataFrame object tostore all the data. The following code initializes an appropriate DataFrame object:In%5B56%5D:ticks%3Dpd.DataFrame(%7B'bid':data%5B'bid'%5D,'ask':data%5B'ask'%5D,","'instrument':data%5B'instrument'%5D,'time':pd.Timestamp(data%5B'time'%5D)%7D,index%3D%5Bpd.Timestamp(data%5B'time'%5D),%5D)# initialization of ticks DataFrameIn%5B57%5D:ticks%5B%5B'ask','bid','instrument'%5D%5DOut%5B57%5D:                                       ask     bid instrument         2014-09-29 06:14:34.749878%2B00:00  1.25829  1.2582    EUR_USDImplementing a real-time plot requires two things: real-time data collection and real-time updates of the plot. With Bokeh, this is accomplished by using the Bokehserver,which handles real-time updates of a plot given new data. It has to be started via theshell or command-line interface as follows:%24 bokeh-serverWith the server running in the background, let us implement the real-time data updateroutine:In%5B58%5D:importbokeh.plottingasbpfrombokeh.objectsimportGlyphBefore any updatingtakes place, there needs to be an object to be updated. This againis a line plot-if only with very little data at first. The output is directed to the IPythonNotebook the code is executed in. However, in fact it is redirected again to the server,which in this case can be accessed locally via http://localhost:5006/:In%5B59%5D:bp.output_notebook(%22default%22)bp.line(ticks%5B'time'%5D,ticks%5B'bid'%5D,x_axis_type%3D'datetime',legend%3Dinstrument)Out%5B59%5D: Using saved session configuration for http://localhost:5006/         To override, pass 'load_from_config%3DFalse' to Session         %3Cbokeh.objects.Plot at 0x7fdb7e1b2e10%3EWe need to get access to our current plot (i.e., the most recently generated plot). Callingthe function curplot returns the object we are looking for:In%5B60%5D:bp.curplot()Out%5B60%5D: %3Cbokeh.objects.Plot at 0x7fdb7e1b2e10%3ESuch a Plotobject consists of a number of rendering objects that accomplish differentplotting tasks, like plotting a Grid or plotting the line (%3D Glyph) representing the fi%E2%80%90nancial data. All rendering objects are stored in a list attribute called renderers:In%5B61%5D:bp.curplot().renderersOut%5B61%5D: %5B%3Cbokeh.objects.DatetimeAxis at 0x7fdbaece6b50%3E,          %3Cbokeh.objects.Grid at 0x7fdb7e161190%3E,          %3Cbokeh.objects.LinearAxis at 0x7fdb7e161090%3E,          %3Cbokeh.objects.Grid at 0x7fdb7e1614d0%3E,          %3Cbokeh.objects.BoxSelectionOverlay at 0x7fdb7e161490%3E,","          %3Cbokeh.objects.BoxSelectionOverlay at 0x7fdb7e161550%3E,          %3Cbokeh.objects.Legend at 0x7fdb7e161650%3E,          %3Cbokeh.objects.Glyph at 0x7fdb7e161610%3E%5DThe following list comprehension returns the first rendering object of type Glyph:In%5B62%5D:renderer%3D%5Brforrinbp.curplot().renderersifisinstance(r,Glyph)%5D%5B0%5DThe glyphattribute of the object contains the type of the Glyph object-in this case, asexpected, a Line object:In%5B63%5D:renderer.glyphOut%5B63%5D: %3Cbokeh.glyphs.Line at 0x7fdb7e161590%3EWith the rendering object, we can access its data source directly:In%5B64%5D:renderer.data_sourceOut%5B64%5D: %3Cbokeh.objects.ColumnDataSource at 0x7fdb7e1b2ed0%3EIn%5B65%5D:renderer.data_source.dataOut%5B65%5D: %7B'x': 2014-09-29 06:14:34.749878%2B00:00    2014-09-29 06:14:34.749878%2B00         :00          Name: time, dtype: object, 'y': 2014-09-29 06:14:34.749878%2B00:00    1.         2582          Name: bid, dtype: float64%7DIn%5B66%5D:ds%3Drenderer.data_sourceThis is the object that we will work with and that is to be updated whenever new dataarrives. The following while loop runs for a predetermined period of time only. Duringthe loop, a new request object is generated and the JSON data is read. The new data isappended to the existing DataFrame object. The x and y coordinates of the renderingobject are updated and then stored to the current session:In%5B67%5D:start%3Dtime.time()# run for 60 secondswhile(time.time()-start)%3C60:data%3Drequests.get(url%25instrument).json()# connect and read datadata%3Ddict(data%5B'prices'%5D%5B0%5D)# transform data to dict objectticks%3Dticks.append(pd.DataFrame(%7B'bid':data%5B'bid'%5D,'ask':data%5B'ask'%5D,'instrument':data%5B'instrument'%5D,'time':pd.Timestamp(data%5B'time'%5D)%7D,index%3D%5Bpd.Timestamp(data%5B'time'%5D),%5D))# append DataFrame object with new data to existing objectds.data%5B'x'%5D%3Dticks%5B'time'%5D# update x coordinates in rendering objectds.data%5B'y'%5D%3Dticks%5B'bid'%5D# update y coordinates in rendering object","bp.cursession().store_objects(ds)# store data objectstime.sleep(0.1)# wait for a bitFigure 14-5shows the output of the plotting exercise-i.e., a static snapshot of a real-time plot. This approach and the underlying technology of course have many interestingapplication areas, both in finance, with its focus today on real-time, high-frequencydata, and far beyond.Figure 14-5. Screenshot of real-time Bokeh plot via Bokeh Server (exchange rate)","The second example uses real-time, high-frequency stock price data. First, make sureto correctly direct the output (i.e., in this case to the Bokeh server for the real-time plot):","In%5B68%5D:bp.output_notebook(%22default%22)Out%5B68%5D: Using saved session configuration for http://localhost:5006/         To override, pass 'load_from_config%3DFalse' to SessionChapter 6provides an example based on the data source and API that we use in whatfollows. It is the stock price API for intraday real-time data provided by Netfonds, aNorwegian online broker. The API and web service, respectively, have the followingbasic URL format:In%5B69%5D:url1%3D'http://hopey.netfonds.no/posdump.php%3F'url2%3D'date%3D%25s%25s%25s%26paper%3D%25s.O%26csv_format%3Dcsv'url%3Durl1%2Burl2This URL is to be customized by providing date information and the symbol one isinterested in:In%5B70%5D:today%3Ddt.datetime.now()y%3D'%25d'%25today.year# current yearm%3D'%2502d'%25today.month# current month, add leading zero if neededd%3D'%2502d'%25(today.day)# current day, add leading zero if neededsym%3D'AAPL'# Apple Inc. stocksIn%5B71%5D:y,m,d,symOut%5B71%5D: ('2014', '09', '29', 'AAPL')In%5B72%5D:urlreq%3Durl%25(y,m,d,sym)urlreqOut%5B72%5D: 'http://hopey.netfonds.no/posdump.php%3Fdate%3D20140929%26paper%3DAAPL.O%26csv_fo         rmat%3Dcsv'","Equipped with the right URL string, retrieving data is only one line of code away:In%5B73%5D:data%3Dpd.read_csv(urlreq,parse_dates%3D%5B'time'%5D)# initialize DataFrame objectThe details of what follows are known from the previous example. First, the initial plot:In%5B74%5D:bp.line(data%5B'time'%5D,data%5B'bid'%5D,x_axis_type%3D'datetime',legend%3Dsym)# intial plotOut%5B74%5D: %3Cbokeh.objects.Plot at 0x7f92bedc8dd0%3ESecond, selection of the rendering object:In%5B75%5D:renderer%3D%5Brforrinbp.curplot().renderersifisinstance(r,Glyph)%5D%5B0%5Dds%3Drenderer.data_sourceThird, the while loop updating the financial data and the plot per loop:In%5B76%5D:start%3Dtime.time()while(time.time()-start)%3C60:data%3Dpd.read_csv(urlreq,parse_dates%3D%5B'time'%5D)data%3Ddata%5Bdata%5B'time'%5D%3Edt.datetime(int(y),int(m),int(d),10,0,0)%5D# only data from trading start at 10amds.data%5B'x'%5D%3Ddata%5B'time'%5Dds.data%5B'y'%5D%3Ddata%5B'bid'%5Dds._dirty%3DTruebp.cursession().store_objects(ds)time.sleep(0.5)Figure 14-6shows the resulting output-again, unfortunately, only a static snapshot ofa real-time plot.","Figure 14-6. Screenshot of real-time Bokeh plot via Bokeh Server (stock quotes)","If the Pythonworld were to be divided into continents, there might be, among others,the science and financecontent, the system administrationcontinent, and for sure theweb developmentcontinent. Although not really transparent, it is highly probable thatthe web development continent, to stay with this concept, might be one of the largestwhen it comes to people (developers) populating it and houses (applications) builton it.","7.See http://wiki.python.org/moin/WebFrameworks for further information on Python web frameworks. Seehttps://wiki.python.org/moin/ContentManagementSystems for an overview of content management systems(CMSs) for Python.8.Although the framework is still quite recent (it all started in 2010), there are already books about Flaskavailable. Cf. Grinberg (2014).One of the major reasons for Python being strong in web development is the availabilityof different high-level, full-stack frameworks. As the Python web page states:A web application may use a combination of a base HTTP application server, a storagemechanism such as a database, a template engine, a request dispatcher, an authenticationmodule and an AJAX toolkit. These can be individual components or be provided to%E2%80%90gether in a high-level framework.Among the most popular frameworks are:%E2%80%A2Django%E2%80%A2Flask%E2%80%A2Pyramid/Pylons%E2%80%A2TurboGears%E2%80%A2ZopeIt is safe to say that there is not a single framework that is best suited for everybody andevery different application type.7All have their strengths (and sometimes weaknesses),and often it is more a matter of taste (regarding architecture, style, syntax, APIs, etc.)what framework is chosen.One framework that has recently gained popularity quite rapidly is Flask. It is theframework we use here, mainly for the following reasons:PythonicApplication development with Flask is really Pythonic, with a lot of the web-relateddetails being taken care of behind the scenes.CompactnessIt is not too complex and can therefore be learned quite rapidly%3B it is based mainlyon standard components and libraries widely used elsewhere.DocumentationIt is well documented, with both an online HTML version and a PDFwith around 300pages available at the time of this writing.8The two main libraries that Flask relies on are:%E2%80%A2Jinja2, a web templating language/engine for Python","9.The example application is called Flaskrand represents a microblog application. Our example is, more orless, a mixture between Flaskr and Minitwit, another Flask example application resembling a simpleTwitter clone.%E2%80%A2Werkzeug, a WSGI (Web Server Gateway Interface) toolkit for Python","We will now dive into the example application called Tradechat for a traders' chat room,which basically relies on the example used in the tutorial of the Flask documentationbut includes a couple of changes and adds some further functionality.9The basic idea is to build a web-based application for which traders can register thatprovides one central chat room to exchange ideas and talk markets. The main screenshall allow a user who is logged in to type in text that is, after pushing a button, addedto the timeline, indicating who added the comment and when this happened. The mainscreen also shows all the historical entries in descending order (from newest to oldest).","We start by generating the needed directories. tradechatshall be the main directory.In addition, at a minimum, we need the two subdirectories static and templates (byFlask convention):%24 mkdir tradechat%24 mkdir tradechat/static%24 mkdir tradechat/templatesTo store data-both for registered users and for comments made in the chat room-weuse SQLite3 (cf. http://www.sqlite.org and http://docs.python.org/2/library/sqlite3.html) as a database. Two different tables are needed that can be generated by theSQL schema presented in Example 14-1, the details of which we do not discuss here. Youshould store this under the filename tables.sqlin the main directory of the applica%E2%80%90tion, tradechat.Example 14-1. SQL schema to generate tables in SQLite3droptableifexistscomments%3Bcreatetablecomments(idintegerprimarykeyautoincrement,commenttextnotnull,usertextnotnull,timetextnotnull)%3Bdroptableifexistsusers%3B","createtableusers(idintegerprimarykeyautoincrement,nametextnotnull,passwordtextnotnull)%3B","The SQLschema is a main input for the Python/Flaskapplication to follow. We will gothrough the single elements step by step to finally arrive at the complete Python scriptto be stored under tradechat.py in the main directory, tradechat.","At the beginning we need to import a couple of libraries and also some main functionsfrom Flask. We import the functions directly to shorten the code throughout and in%E2%80%90crease readability somewhat:# Tradechat## A simple example for a web-based chat room# based on Flask and SQLite3.#importosimportdatetimeasdtfromsqlite3importdbapi2assqlite3fromflaskimportFlask,request,session,g,redirect,url_for,abort, %5Crender_template,flashThe whole application hinges on a Flask object, an instance of the main class of theframework. Instantiating the class with name lets the object inherit the application name(i.e., main) when the script is executed, for example, from a shell:# the application object from the main Flask classapp%3DFlask(__name__)The next step is to do some configuration for the new application object. In particular,we need to provide a database filename:# override config from environment variableapp.config.update(dict(DATABASE%3Dos.path.join(app.root_path,'tradechat.db'),# the SQLite3 database file (%22TC database%22)DEBUG%3DTrue,SECRET_KEY%3D'secret_key',# use secure key here for real applications))app.config.from_envvar('TC_SETTINGS',silent%3DTrue)# do not complain if no config file exists","Having provided the path and filename of the database, the function connect_db con%E2%80%90nects to the database and returns the connection object:defconnect_db():''' Connects to the TC database.'''rv%3Dsqlite3.connect(app.config%5B'DATABASE'%5D)rv.row_factory%3Dsqlite3.RowreturnrvFlaskuses an object called gto store global data and other objects. For example, webapplications serving large numbers of users make it necessary to connect regularly todatabases. It would be inefficient to instantiate a connection object every time a databaseoperation has to be executed. One can rather store such a connection object in theattribute sqlite_dbof the gobject. The function get_dbmakes use of this approach inthat a new database connection is opened only when there is no connection object storedin the g object already:defget_db():''' Opens a new connection to the TC database. '''ifnothasattr(g,'sqlite_db'):# open only if none exists yetg.sqlite_db%3Dconnect_db()returng.sqlite_dbAt least once, we need to create the tables in the database. Calling the function init_dbfor a second time will delete all information previously stored in the database (accordingto the SQL schema used):definit_db():''' Creates the TC database tables.'''withapp.app_context():db%3Dget_db()withapp.open_resource('tables.sql',mode%3D'r')asf:db.cursor().executescript(f.read())# creates entries and users tablesdb.commit()The function close_dbcloses the database connection if one exists in the g object. Forthe first time (and for sure not the last time), we encounter a Flask function decorator,i.e., @app.teardown_appcontext. This decorator ensures that the respective function iscalled whenever the application context tears down-that is, roughly speaking, whenthe execution of the application is terminated by the user or by an error/exception:@app.teardown_appcontextdefclose_db(error):''' Closes the TC database at the end of the request. '''ifhasattr(g,'sqlite_db'):g.sqlite_db.close()","Building on the database infrastructure, we can now proceed and implement the corefunctionality for the application. First, we have to define what happens when we connectto the main/home page of the application. To this end, we use the Flask function dec%E2%80%90orator @app.route(%22/%22). The function decorated in that way will be called whenever aconnection is established to the main page. The function show_entriesbasically es%E2%80%90tablishes a database connection, retrieves all comments posted so far (maybe none,maybe many), and sends them to a template-based rendering engine to return an HTMLdocument based on the template and the data provided (more on the templatingpart soon):@app.route('/')defshow_entries():''' Renders all entries of the TC database. '''db%3Dget_db()query%3D'select comment, user, time from comments order by id desc'cursor%3Ddb.execute(query)comments%3Dcursor.fetchall()returnrender_template('show_entries.html',comments%3Dcomments)We only want to allow registeredusers to post comments in the chat room. Therefore,we must provide functionality for a user to register. To this end, technically, we mustallow use of the POST method for the respective HTML to be rendered by the applicationand to be accessed by the user. To register, a user must provide a usernameand apassword. Otherwise, an error is reported. The function registershould be considereda simple illustration only. It is missing a number of ingredients important for real-worldapplications, like checking whether a username already exists and encryption of thepasswords (they are stored as plain text). Once users have successfully registered, theirstatus is automatically changed to logged_in and they are redirected to the main pagevia redirect(url_for(%22show_entries%22)):@app.route('/register',methods%3D%5B'GET','POST'%5D)defregister():''' Registers a new user in the TC database. '''error%3DNoneifrequest.method%3D%3D'POST':db%3Dget_db()ifrequest.form%5B'username'%5D%3D%3D''orrequest.form%5B'password'%5D%3D%3D'':error%3D'Provide both a username and a password.'# both fields have to be nonemptyelse:db.execute('insert into users (name, password) values (%3F, %3F)',%5Brequest.form%5B'username'%5D,request.form%5B'password'%5D%5D)db.commit()session%5B'logged_in'%5D%3DTrue# directly log in new userflash('You were sucessfully registered.')app.config.update(dict(USERNAME%3Drequest.form%5B'username'%5D))","returnredirect(url_for('show_entries'))returnrender_template('register.html',error%3Derror)For such a web application, there are probably returning users that do not need or wantto reregister anew. We therefore need to provide a form to log in with an existing account.This is what the function login does. The functionality is similar to that provided byregister:@app.route('/login',methods%3D%5B'GET','POST'%5D)deflogin():''' Logs in a user. '''error%3DNoneifrequest.method%3D%3D'POST':db%3Dget_db()try:query%3D'select id from users where name %3D %3F and password %3D %3F'id%3Ddb.execute(query,(request.form%5B'username'%5D,request.form%5B'password'%5D)).fetchone()%5B0%5D# fails if record with provided username and password# is not foundsession%5B'logged_in'%5D%3DTrueflash('You are now logged in.')app.config.update(dict(USERNAME%3Drequest.form%5B'username'%5D))returnredirect(url_for('show_entries'))except:error%3D'User not found or wrong password.'returnrender_template('login.html',error%3Derror)Once users have registered or logged in again, they should be able to add comments inthe chat room. The function add_entrystores the comment text, the username of theuser who commented, and the exact time (to the second) of the posting. The functionalso checks whether the user is logged in or not:@app.route('/add',methods%3D%5B'POST'%5D)defadd_entry():''' Adds entry to the TC database. '''ifnotsession.get('logged_in'):abort(401)db%3Dget_db()now%3Ddt.datetime.now()db.execute('insert into comments (comment, user, time) values (%3F, %3F, %3F)',%5Brequest.form%5B'text'%5D,app.config%5B'USERNAME'%5D,str(now)%5B:-7%5D%5D)db.commit()flash('Your comment was successfully added.')returnredirect(url_for('show_entries'))Finally, to end the session, the user must log out. This is what the function logoutsupports:@app.route('/logout')deflogout():''' Logs out the current user. '''","session.pop('logged_in',None)flash('You were logged out')returnredirect(url_for('show_entries'))If we want to run the Python script as a standalone application we should add thefollowing lines, which make sure that a server is fired up and that the application isserved:# main routineif__name__%3D%3D'__main__':init_db()# comment out if data in current# TC database is to be keptapp.run()Putting all these pieces together, we end up with the Python script shown asExample 14-2.Example 14-2. Python script embodying the core of the Tradechat application# Tradechat## A simple example for a web-based chat room# based on Flask and SQLite3.#importosimportdatetimeasdtfromsqlite3importdbapi2assqlite3fromflaskimportFlask,request,session,g,redirect,url_for,abort, %5Crender_template,flash# the application object from the main Flask classapp%3DFlask(__name__)# override config from environment variableapp.config.update(dict(DATABASE%3Dos.path.join(app.root_path,'tradechat.db'),# the SQLite3 database file (%22TC database%22)DEBUG%3DTrue,SECRET_KEY%3D'secret_key',# use secure key here for real applications))app.config.from_envvar('TC_SETTINGS',silent%3DTrue)# do not complain if no config file existsdefconnect_db():''' Connects to the TC database.'''rv%3Dsqlite3.connect(app.config%5B'DATABASE'%5D)rv.row_factory%3Dsqlite3.Rowreturnrv","defget_db():''' Opens a new connection to the TC database. '''ifnothasattr(g,'sqlite_db'):# open only if none exists yetg.sqlite_db%3Dconnect_db()returng.sqlite_dbdefinit_db():''' Creates the TC database tables.'''withapp.app_context():db%3Dget_db()withapp.open_resource('tables.sql',mode%3D'r')asf:db.cursor().executescript(f.read())# creates entries and users tablesdb.commit()@app.teardown_appcontextdefclose_db(error):''' Closes the TC database at the end of the request. '''ifhasattr(g,'sqlite_db'):g.sqlite_db.close()@app.route('/')defshow_entries():''' Renders all entries of the TC database. '''db%3Dget_db()query%3D'select comment, user, time from comments order by id desc'cursor%3Ddb.execute(query)comments%3Dcursor.fetchall()returnrender_template('show_entries.html',comments%3Dcomments)@app.route('/register',methods%3D%5B'GET','POST'%5D)defregister():''' Registers a new user in the TC database. '''error%3DNoneifrequest.method%3D%3D'POST':db%3Dget_db()ifrequest.form%5B'username'%5D%3D%3D''orrequest.form%5B'password'%5D%3D%3D'':error%3D'Provide both a username and a password.'# both fields have to be nonemptyelse:db.execute('insert into users (name, password) values (%3F, %3F)',%5Brequest.form%5B'username'%5D,request.form%5B'password'%5D%5D)db.commit()session%5B'logged_in'%5D%3DTrue# directly log in new userflash('You were sucessfully registered.')app.config.update(dict(USERNAME%3Drequest.form%5B'username'%5D))","returnredirect(url_for('show_entries'))returnrender_template('register.html',error%3Derror)@app.route('/login',methods%3D%5B'GET','POST'%5D)deflogin():''' Logs in a user. '''error%3DNoneifrequest.method%3D%3D'POST':db%3Dget_db()try:query%3D'select id from users where name %3D %3F and password %3D %3F'id%3Ddb.execute(query,(request.form%5B'username'%5D,request.form%5B'password'%5D)).fetchone()%5B0%5D# fails if record with provided username and password# is not foundsession%5B'logged_in'%5D%3DTrueflash('You are now logged in.')app.config.update(dict(USERNAME%3Drequest.form%5B'username'%5D))returnredirect(url_for('show_entries'))except:error%3D'User not found or wrong password.'returnrender_template('login.html',error%3Derror)@app.route('/add',methods%3D%5B'POST'%5D)defadd_entry():''' Adds entry to the TC database. '''ifnotsession.get('logged_in'):abort(401)db%3Dget_db()now%3Ddt.datetime.now()db.execute('insert into comments (comment, user, time) values (%3F, %3F, %3F)',%5Brequest.form%5B'text'%5D,app.config%5B'USERNAME'%5D,str(now)%5B:-7%5D%5D)db.commit()flash('Your comment was successfully added.')returnredirect(url_for('show_entries'))@app.route('/logout')deflogout():''' Logs out the current user. '''session.pop('logged_in',None)flash('You were logged out')returnredirect(url_for('show_entries'))# main routineif__name__%3D%3D'__main__':init_db()# comment out if data in current# TC database is to be keptapp.run()","Although the example in this section illustrates the basic design of awebapplicationinPythonwithFlask,itbarelyaddressessecurityissues,whichareofparamountimportancewhenitcomestowebapplications.However,Flaskandotherwebframeworksprovidecomplete tool sets to tackle typical security issues (e.g., encryption)with due diligence.","Basically, templating with Flask(Jinja2) works similarly to simple string replacementsin Python: you have a basic stringindicating where to replace what and some data tobe inserted into the string object. Consider the following examples:In%5B77%5D:'%25d, %25d, %25d'%25(1,2,3)Out%5B77%5D: '1, 2, 3'In%5B78%5D:'%7B%7D, %7B%7D, %7B%7D'.format(1,2,3)Out%5B78%5D: '1, 2, 3'In%5B79%5D:'%7B%7D, %7B%7D, %7B%7D'.format(*'123')Out%5B79%5D: '1, 2, 3'Templating to generate HTMLpages works pretty similarly. The major difference is thatthe string object %E2%80%9Cresembles%E2%80%9D an HTML document (or a part thereof) and has commandsfor replacements and also, for example, ways of controlling the flow when renderingthe template (e.g., the for loop). Missing information is added during the renderingprocedure, as we added the integers to the string object in the previous examples.Consider now the following stringobject, containing partly standard HTML code andsome template-specific code:In%5B80%5D:templ%3D'''%3C!doctype html%3E           Just print out %3Cb%3Enumbers%3C/b%3E provided to the template.           %3Cbr%3E%3Cbr%3E           %7B%25 for number in numbers %25%7D             %7B%7B number %7D%7D           %7B%25 endfor %25%7D         '''So far, this is a stringobject only. We have to generate a Jinja2Template object outof it before proceeding:In%5B81%5D:fromjinja2importTemplateIn%5B82%5D:t%3DTemplate(templ)This Template object has a method called render to make valid HTML code out of thetemplate and some input values-in this case, some numbers via the parameter numbers:","In%5B83%5D:html%3Dt.render(numbers%3Drange(5))The code is again a string object:In%5B84%5D:htmlOut%5B84%5D: u'%3C!doctype html%3E%5Cn  Just print out %3Cb%3Enumbers%3C/b%3E provided to the temp         late.%5Cn  %3Cbr%3E%3Cbr%3E%5Cn  %5Cn    0%5Cn  %5Cn    1%5Cn  %5Cn    2%5Cn  %5Cn    3%5Cn  %5Cn         4%5Cn  'Such an object containing HTML code can be rendered in IPythonNotebook as follows:In%5B85%5D:fromIPython.displayimportHTMLHTML(html)Out%5B85%5D: %3CIPython.core.display.HTML at 0x7fdb7e1eb890%3EOf course, templating involves much more than this simple example can illustrate (e.g.,inheritance). More details can be found at http://jinja.pocoo.org. However, the templatesfor the Tradechat application already include a number of important aspects. Specifi%E2%80%90cally, we need the following templates:layout.htmlDefines the basic layout from which the other templates inheritregister.htmlThe template for the user registration pagelogin.htmlThe corresponding template for the user loginshow_entries.htmlThe main page showing the comments in the chat room and, if the user is loggedin, the text field for writing and posting commentsThese files have to be stored in templates, the default (sub)directory for templates whenusing Flask.Example 14-3 shows the template containing the basic layout and some meta-information (like the site title). This is the template all other templates inherit from.Example 14-3. Template for basic layout of Tradechat application%3C!doctype html%3E%3Ctitle%3ETradechat%3C/title%3E%3Clinkrel%3Dstylesheettype%3Dtext/csshref%3D%22%7B%7B url_for('static', filename%3D'style.css') %7D%7D%22%3E%3Cdivclass%3Dpage%3E%3Ch1%3ETradechat%3C/h1%3E%3Cdivclass%3Dmetanav%3E  %7B%25 if not session.logged_in %25%7D%3Cahref%3D%22%7B%7B url_for('login') %7D%7D%22%3Elog in%3C/a%3E%3Cbr%3E%3Cahref%3D%22%7B%7B url_for('register') %7D%7D%22%3Eregister%3C/a%3E","  %7B%25 else %25%7D%3Cahref%3D%22%7B%7B url_for('logout') %7D%7D%22%3Elog out%3C/a%3E  %7B%25 endif %25%7D%3C/div%3E  %7B%25 for message in get_flashed_messages() %25%7D%3Cdivclass%3Dflash%3E%7B%7B message %7D%7D%3C/div%3E  %7B%25 endfor %25%7D  %7B%25 block body %25%7D%7B%25 endblock %25%7D%3C/div%3EFigure 14-7 shows a screenshot of the main page after starting the application for thefirst time. No users are registered (or logged in, of course). No comments have beenposted yet.Figure 14-7. Screenshot of %E2%80%9Cempty%E2%80%9D home page of TradechatExample 14-4 provides the templating code for the user registration page. Here, formsare used to allow users to provide information to the page via the POST method.Example 14-4. Template for Tradechat user registration%7B%25 extends %22layout.html%22 %25%7D%7B%25 block body %25%7D%3Ch2%3ERegister%3C/h2%3E  %7B%25 if error %25%7D%3Cpclass%3Derror%3E%3Cstrong%3EError:%3C/strong%3E %7B%7B error %7D%7D%7B%25 endif %25%7D%3Cformaction%3D%22%7B%7B url_for('register') %7D%7D%22method%3Dpost%3E%3Cdl%3E","%3Cdd%3E%3Cfontsize%3D%22-1%22%3EUsername%3C/font%3E%3Cdd%3E%3Cinputtype%3Dtextname%3Dusername%3E%3Cdd%3E%3Cfontsize%3D%22-1%22%3EPassword%3C/font%3E%3Cdd%3E%3Cinputtype%3Dpasswordname%3Dpassword%3E%3Cdd%3E%3Cinputtype%3Dsubmitvalue%3DRegister%3E%3C/dl%3E%3C/form%3E%7B%25 endblock %25%7DFigure 14-8 shows a screenshot of the registration page.Figure 14-8. Screenshot of Tradechat registration pageThe templating code for the login page, as shown in Example 14-5, is pretty similar tothe code for the registration page. Again, the user can provide login information viaa form.Example 14-5. Template for Tradechat user login%7B%25 extends %22layout.html%22 %25%7D%7B%25 block body %25%7D%3Ch2%3ELogin%3C/h2%3E  %7B%25 if error %25%7D%3Cpclass%3Derror%3E%3Cstrong%3EError:%3C/strong%3E %7B%7B error %7D%7D%7B%25 endif %25%7D%3Cformaction%3D%22%7B%7B url_for('login') %7D%7D%22method%3Dpost%3E%3Cdl%3E%3Cdd%3E%3Cfontsize%3D%22-1%22%3EUsername%3C/font%3E%3Cdd%3E%3Cinputtype%3Dtextname%3Dusername%3E%3Cdd%3E%3Cfontsize%3D%22-1%22%3EPassword%3C/font%3E","%3Cdd%3E%3Cinputtype%3Dpasswordname%3Dpassword%3E%3Cdd%3E%3Cinputtype%3Dsubmitvalue%3DLogin%3E%3C/dl%3E%3C/form%3E%7B%25 endblock %25%7DThe login page, as shown in Figure 14-9, not only looks pretty similar to the registrationpage but also provides mainly the same functionality.Figure 14-9. Screenshot of Tradechat login pageFinally, Example 14-6 provides the templating code for the main page. This templatedoes mainly two things:","Enables commentingIf the user is logged in, a text field and a Postbutton are shown to allow the userto post comments.Displays commentsAll comments found in the database are displayed in reverse chronological order(newest first, oldest last).Example 14-6. Template for Tradechat main page with chat room comments%7B%25 extends %22layout.html%22 %25%7D%7B%25 block body %25%7D  %7B%25 if session.logged_in %25%7D%3Cformaction%3D%22%7B%7B url_for('add_entry') %7D%7D%22method%3Dpostclass%3Dadd-comment%3E%3Cdl%3E%3Cdd%3EWhat's up%3F%3Cdd%3E%3Ctextareaname%3Dtextrows%3D3cols%3D40%3E%3C/textarea%3E%3Cdd%3E%3Cinputtype%3Dsubmitvalue%3DPost%3E%3C/dl%3E%3C/form%3E  %7B%25 endif %25%7D%3Culclass%3Dcomments%3E  %7B%25 for comment in comments %25%7D%3Cli%3E%7B%7B comment.comment%7Csafe %7D%7D%3Cfontsize%3D%22-2%22%3E(%7B%7B comment.user %7D%7D @ %7B%7B comment.time %7D%7D)%3C/font%3E  %7B%25 else %25%7D%3Cli%3E%3Cem%3ENo comments so far.%3C/em%3E  %7B%25 endfor %25%7D%3C/ul%3E%7B%25 endblock %25%7DOnce a user is logged in and has posted some comments, the main page shows the textfield and the Post button as well as all comments stored in the database (cf.Figure 14-10).Just showing the screenshots in combination with the templates is cheating, in a sense.What is missing in the mix is the styling information.","Figure 14-10. Screenshot of Tradechat main page","Today's standard when it comes to the styling of web pages and web-based applicationsis CSS (Cascading Style Sheets). If you take a closer look at the single templates, youwill find in many places parameterizations like class%3Dcommentsor class%3Dadd-comment. Without a corresponding CSS file, these parameterizations are essentiallymeaningless.Therefore, let us have a look at the file style.css, stored in the (sub)directory staticand shown in Example 14-7. Here you find the aforementioned parameters (comments, add-comment) again. You also find references to standard HTMLtags, like h1 forthe highest-ranking header. All information provided after a custom class name, likecomments, or a standard tag, like h1, defines or changes certain style elements (e.g., fonttype and/or size) of the relevant object.This style information is the final ingredient defining the look of the Tradechat appli%E2%80%90cation and explaining why, for example, the %E2%80%9CTradechat%E2%80%9D heading is displayed in blue(namely, due to the line a, h1, h2 %7B color: #0066cc%3B %7D).","Example 14-7. CSS stylesheet for Tradechat applicationbody%7Bfont-family:sans-serif%3Bbackground:#eee%3B%7Da,h1,h2%7Bcolor:#0066cc%3B%7Dh1,h2%7Bfont-family:'Helvetica',sans-serif%3Bmargin:0%3B%7Dh1%7Bfont-size:1.4em%3Bborder-bottom:2pxsolid#eee%3B%7Dh2%7Bfont-size:1.0em%3B%7D.page%7Bmargin:2emauto%3Bwidth:35em%3Bborder:1pxsolid#ccc%3Bpadding:0.8em%3Bbackground:white%3B%7D.comments%7Blist-style:none%3Bmargin:0%3Bpadding:0%3B%7D.commentsli%7Bmargin:0.8em1.2em%3B%7D.commentslih2%7Bmargin-left:-1em%3B%7D.add-comment%7Bcolor:#0066cc%3Bfont-size:0.7em%3Bborder-bottom:1pxsolid#ccc%3B%7D.add-commentdl%7Bfont-weight:bold%3B%7D.metanav%7Btext-align:right%3Bfont-size:0.8em%3Bpadding:0.3em%3Bmargin-bottom:1em%3Bbackground:#fafafa%3B%7D.flash%7Bcolor:#b9b9b9%3Bfont-size:0.7em%3B%7D.error%7Bcolor:#ff4629%3Bfont-size:0.7em%3Bpadding:0.5em%3B%7DIf you have followed every step, your tradechat directory should now contain the samefiles listed here:In%5B86%5D:importosforpath,dirs,filesinos.walk('../python/tradechat'):printpathforfinfiles:printfOut%5B86%5D: ../python/tradechat         tables.sql         tradechat.db         tradechat.py         ../python/tradechat/static         style.css         ../python/tradechat/templates         layout.html         login.html         register.html         show_entries.html","You can now run the main script from the shell as follows and start the application:%24 python tradechat.pyYou can then access the application via your web browser at http://127.0.0.1:5000. Clickon register to register as a user, and after having provided a username and a passwordyou will be able to post your comments.","The last topic in this chapter-and a very interesting and important one-is web serv%E2%80%90ices. Web services provide a simple and efficient means to access server-based func%E2%80%90tionality via web protocols. For example, one of the web services with the highest trafficis the Google search functionality. We are used to visiting http://www.google.comandtyping some words of interest into the search/text input field provided on the website.However, what happens after you press the Return key or push the Search button is thatthe page translates all the information it has (from the search field and maybe yourpersonal preferences) into a more or less complex URL.Such a URL could, for example, take on the form http://www.google.de/search%3Fnum%3D5%26q%3Dyves%2Bpython. When you click this link or copy it into your web browser,Google Search returns those five search results (num%3D5) that the engine considers thebest matches given the words provided (q%3DYves%2BPython). Your web browser then dis%E2%80%90plays something similar to Figure 14-11.Using web services, any kind of data- and transaction-oriented financial service can beprovided via web technologies. For instance, Yahoo! Finance and Google Finance offerhistorical stock price information via such a web service approach. More complex serv%E2%80%90ices such as derivatives pricing and risk analytics are also available via such services (forexample, the web-based analytics solution DEXISION%3B cf. http://derivatives-analytics.com). The following example illustrates the implementation of such a servicein the context of option pricing.","10.See also the larger case study about volatility options presented in Chapter 19.Figure 14-11. Screenshot of Google search results via web service","In this section, we are going to implement a web service that allows us to value volatilityoptions (e.g., on a volatility index). The model we use is the one of Gruenbichler andLongstaff (1996). They model the volatility process (e.g., the process of a volatility index)in direct fashion by a square-root diffusion, provided in Equation 14-1. This process isknown to exhibit convenient features for volatility modeling, like positivity and meanreversion.10","Equation 14-1. Square-root diffusion for volatility modelingdVt%3D%CE%BAV%CE%B8V%E2%88%92Vtdt%2B%CF%83VVtdZThe variables and parameters in Equation 14-1 have the following meanings:VtThe time t value of the volatility index (for example, the VSTOXX)%CE%B8VThe long-run mean of the volatility index%CE%BAVThe rate at which Vt reverts to %ED%9C%83%CE%A3VThe volatility of the volatility (%E2%80%9Cvol-vol%E2%80%9D)%CE%B8V, %CE%BAV, and %CE%A3VAssumed to be constant and positiveZtA standard Brownian motionBased on this model, Gruenbichler and Longstaff (1996) derive the formula providedin Equation 14-2 for the value of a European call option. In the formula, D(T) is theappropriate discount factor. The parameter %ED%9C%81 denotes the expected premium for vola%E2%80%90tility risk, while Q%C2%B7 is the complementary noncentral %ED%9C%922 distribution.Equation 14-2. Call option formula of Gruenbichler and Longstaff (1996) C V0,K,T%3DDT%C2%B7e%E2%88%92%CE%B2T%C2%B7V0%C2%B7Q%CE%B3%C2%B7K%CE%BD%2B4,%CE%BB%2BDT%C2%B7%CE%B1%CE%B2%C2%B71%E2%88%92e%E2%88%92%CE%B2T%C2%B7Q%CE%B3%C2%B7K%CE%BD%2B2,%CE%BB%E2%88%92DT%C2%B7K%C2%B7Q%CE%B3%C2%B7K%CE%BD,%CE%BB%CE%B1%3D%CE%BA%CE%B8%CE%B2%3D%CE%BA%2B%CE%B6%CE%B3%3D4%CE%B2%CF%8321%E2%88%92e%E2%88%92%CE%B2T%CE%BD%3D4%CE%B1%CF%832%CE%BB%3D%CE%B3%C2%B7e%E2%88%92%CE%B2T%C2%B7V","The translation of the formula as presented in Equation 14-2 to Python is, as usual, quitestraightforward. Example 14-8 shows the code of a Python module with such a valuationfunction. We call the script vol_pricing_formula.py and store it in a sub-directory,volservice.Example 14-8. Python script for volatility option valuation## Valuation of European volatility call options# in Gruenbichler-Longstaff (1996) model# square-root diffusion framework# -- semianalytical formula#fromscipy.statsimportncx2importnumpyasnp# Semianalytical option pricing formula of GL96defcalculate_option_value(V0,kappa,theta,sigma,zeta,T,r,K):''' Calculation of European call option price in GL96 model.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    V0 : float        current volatility level    kappa : float        mean reversion factor    theta : float        long-run mean of volatility    sigma : float        volatility of volatility    zeta :        volatility risk premium    T : float        time-to-maturity    r : float        risk-free short rate    K : float        strike price of the option    Returns    %3D%3D%3D%3D%3D%3D%3D    value : float        net present value of volatility call option    '''D%3Dnp.exp(-r*T)# discount factor# variablesalpha%3Dkappa*theta","beta%3Dkappa%2Bzetagamma%3D4*beta/(sigma**2*(1-np.exp(-beta*T)))nu%3D4*alpha/sigma**2lamb%3Dgamma*np.exp(-beta*T)*V0cx1%3D1-ncx2.cdf(gamma*K,nu%2B4,lamb)cx2%3D1-ncx2.cdf(gamma*K,nu%2B2,lamb)cx3%3D1-ncx2.cdf(gamma*K,nu,lamb)# formula for European call pricevalue%3D(D*np.exp(-beta*T)*V0*cx1%2BD*(alpha/beta)*(1-np.exp(-beta*T))*cx2-D*K*cx3)returnvalueTo simplify the implementation of the web service we write a convenience function,get_option_value, which will check for the provision of all needed parameters to cal%E2%80%90culate a call option value. The function is stored in a Pythonmodule called vol_pricing_service.py, the code of which is shown in Example 14-9. This script also containsa dictionary with all the necessary parameters and brief descriptions of these parame%E2%80%90ters. The function will return an error message detailing what is missing whenever oneor more parameters are missing. If all necessary parameters are provided during theweb service call, the function calls the pricing function calculate_option_valuefromthe vol_pricing_formula.py script.Example 14-9. Python script for volatility option valuation and web service helperfunction## Valuation of European volatility options# in Gruenbichler-Longstaff (1996) model# square-root diffusion framework# -- parameter dictionary %26 web service function#fromvol_pricing_formulaimportcalculate_option_value# model parametersPARAMS%3D%7B'V0':'current volatility level','kappa':'mean reversion factor','theta':'long-run mean of volatility','sigma':'volatility of volatility','zeta':'factor of the expected volatility risk premium','T':'time horizon in years','r':'risk-free interest rate','K':'strike'%7D# function for web service","defget_option_value(data):''' A helper function for web service. '''errorline%3D'Missing parameter %25s (%25s)%5Cn'errormsg%3D''forparainPARAMS:ifnotdata.has_key(para):# check if all parameters are providederrormsg%2B%3Derrorline%25(para,PARAMS%5Bpara%5D)iferrormsg!%3D'':returnerrormsgelse:result%3Dcalculate_option_value(float(data%5B'V0'%5D),float(data%5B'kappa'%5D),float(data%5B'theta'%5D),float(data%5B'sigma'%5D),float(data%5B'zeta'%5D),float(data%5B'T'%5D),float(data%5B'r'%5D),float(data%5B'K'%5D))returnstr(result)To begin with, we add the path of the aforementioned Python scripts:In%5B87%5D:importsyssys.path.append(%22../python/volservice%22)# adjust if necessary to your pathWe use the library Werkzeugto handle our WSGIapplication-based web service (recallthat Werkzeug is an integral part of Flask). To this end, we need to import some functionsfrom Werkzeug sublibraries:In%5B88%5D:fromwerkzeug.wrappersimportRequest,ResponseFurthermore, for our core WSGI application to follow, we need the function get_option_value that we defined earlier:In%5B89%5D:fromvol_pricing_serviceimportget_option_valueThe only thing that remains is to implement the WSGIapplication (function) itself. Thisfunction might in our case look as follows:In%5B90%5D:defapplication(environ,start_response):request%3DRequest(environ)# wrap environ in new objecttext%3Dget_option_value(request.args)# provide all parameters of call to function# get back either error message or option valueresponse%3DResponse(text,mimetype%3D'text/html')# generate response object based on the returned textreturnresponse(environ,start_response)","Here, environis a dictionary containing all incoming information. The Request func%E2%80%90tion wraps all information in a manner that makes accessing the environ informationa bit more convenient. start_response is usually used to indicate the start of a response.However, with Werkzeug you have the Response function, which takes care of theresponse.All parameters provided to the web service are found in the request.argsattribute,and this is what we provide to the get_option_valuefunction. This function returnseither an error message in text form or the calculated option value in text form.To be better able to serve this function (e.g., via a local web server), we put the functioninto a separate WSGI script and add the serving functionality to it. Example 14-10showsthe code of this script, called vol_pricing.py.Example 14-10. Python script for volatility option valuation and web service helperfunction## Valuation of European volatility options# in Gruenbichler-Longstaff (1996) model# square-root diffusion framework# -- WSGI application for web service#fromvol_pricing_serviceimportget_option_valuefromwerkzeug.wrappersimportRequest,Responsefromwerkzeug.servingimportrun_simpledefapplication(environ,start_response):request%3DRequest(environ)# wrap environ in new objecttext%3Dget_option_value(request.args)# provide all parameters of call to function# get back either error message or option valueresponse%3DResponse(text,mimetype%3D'text/html')# generate response object based on the returned textreturnresponse(environ,start_response)if__name__%3D%3D'__main__':run_simple('localhost',4000,application)Being in the right subdirectory (volservice), you can now start the application byexecuting the following command via the shell or command-line interface:%24 python vol_pricing.py * Running on http://localhost:4000/This fires up a separate Python process that serves the WSGI application. Using urllib,we can now access the %E2%80%9Cfull power%E2%80%9D of the web service. Copying the URL in your webbrowser and pressing the Return key yields something like the result shown inFigure 14-12.","Figure 14-12. Screenshot of the error message of the web serviceHowever, usually you want to use a web service quite a bit differently-for example,from a scripting environment like IPython. To this end, we can use the functionalitythe urllib library provides:In%5B91%5D:importnumpyasnpimporturlliburl%3D'http://localhost:4000/'A simple call to the web service without providing any parameters returns the followingerror message, which (apart from formatting issues) is the same as in the screenshot inFigure 14-12:In%5B92%5D:printurllib.urlopen(url).read()Out%5B92%5D: Missing parameter V0 (current volatility level)         Missing parameter r (risk-free interest rate)         Missing parameter kappa (mean-reversion factor)         Missing parameter T (time horizon in years)         Missing parameter theta (long-run mean of volatility)         Missing parameter zeta (factor of the expected volatility risk premium)         Missing parameter sigma (volatility of volatility)         Missing parameter K (strike)Of course, we need to provide a number of parameters. Therefore, we first build a URLstring object in which we can replace specific parameter values during later calls:In%5B93%5D:urlpara%3Durl%2B'application%3FV0%3D%25s%26kappa%3D%25s%26theta%3D%25s%26sigma%3D%25s%26zeta%3D%25s'urlpara%2B%3D'%26T%3D%25s%26r%3D%25s%26K%3D%25s'","A possible parameterization might be the following one:In%5B94%5D:urlval%3Durlpara%25(25,2.0,20,1.0,0.0,1.5,0.02,22.5)urlvalOut%5B94%5D: 'http://localhost:4000/application%3FV0%3D25%26kappa%3D2.0%26theta%3D20%26sigma%3D1.0%26z         eta%3D0.0%26T%3D1.5%26r%3D0.02%26K%3D22.5'Using this particular URL string returns an option value, as desired:In%5B95%5D:printurllib.urlopen(urlval).read()Out%5B95%5D: 0.202937705934With such a web service, you can of course do multiple calls to calculate multiple optionvalues quite easily:In%5B96%5D:%25%25timeurlpara%3D'http://localhost:4000/application%3FV0%3D25%26kappa%3D2.0'urlpara%2B%3D'%26theta%3D25%26sigma%3D1.0%26zeta%3D0.0%26T%3D1%26r%3D0.02%26K%3D%25s'strikes%3Dnp.linspace(20,30,50)results%3D%5B%5DforKinstrikes:results.append(float(urllib.urlopen(urlpara%25K).read()))results%3Dnp.array(results)Out%5B96%5D: CPU times: user 64 ms, sys: 20 ms, total: 84 ms         Wall time: 196 msIn%5B97%5D:resultsOut%5B97%5D: array(%5B 4.91296701,  4.71661296,  4.52120153,  4.32692516,  4.1339945 ,                 3.94264561,  3.75313813,  3.56575972,  3.38079846,  3.19858765,                 3.01946028,  2.8437621 ,  2.67184576,  2.50406508,  2.34078693,                 2.18230495,  2.02898213,  1.88111287,  1.738968  ,  1.60280064,                 1.47281111,  1.34917004,  1.23204859,  1.12141092,  1.01739405,                 0.9199686 ,  0.82907686,  0.74462353,  0.66647327,  0.59445387,                 0.52843174,  0.46798166,  0.41300694,  0.36319553,  0.31824647,                 0.27785656,  0.24171678,  0.20951651,  0.18094732,  0.1557064 ,                 0.1334996 ,  0.11414975,  0.09710449,  0.08234678,  0.06958767,                 0.05859317,  0.04915788,  0.04109348,  0.03422854,  0.02840802%5D)One advantage of this approach is that you do not use your local resources to get theresults, but rather the resources of a web server-which might also use, for example,parallelization techniques. Of course, in our example all is local and the web service usesthe local computing resources. Figure 14-13 shows the valuation results graphically,concluding this section:In%5B98%5D:importmatplotlib.pyplotasplt%25matplotlibinlineplt.plot(strikes,results,'b')plt.plot(strikes,results,'ro')plt.grid(True)plt.xlabel('strike')plt.ylabel('European call option value')","Figure 14-13. Value of European volatility call option for different strikes","The web services architecture is often a powerful and efficient alter%E2%80%90nativetotheprovisionofPython-basedanalyticalfunctionality,orevenwholeapplications.ThisholdstruefortheInternetaswellasformodels where private networks are used. This architecture also sim%E2%80%90plifiesupdatesandmaintenance,sincesuchservicesaregenerallyprovided in a centralized fashion.","Nowadays, web technologies are an integral part of almost any application architecture.They are not only beneficial for communicating with the outside world and providingsimple to sophisticated web services to external entities, but also within (financial)organizations.This chapter first illustrates some basic techniques with regard to the most commoncommunication protocols (mainly FTP and HTTP). It also shows how to implement in%E2%80%90teractive web plotting, how to interface in real time with web-based financial data APIs(e.g., JSON-based) and how to visualize such high frequency data in real time withBokeh. These basic tools and techniques are helpful in almost any context.However, the Pythonecosystem also provides a number of powerful, high level frame%E2%80%90works to develop even complex web applications in rapid fashion. We use Flask, aframework which has gained some popularity recently, to implement a simple chat roomfor traders with simple user administration (registration and login). All elements of atypical web application-core functionality in Python, templating with Jinja2, andstyling with CSS-are illustrated.","Finally, the last section in this chapter addresses the important topic of web services.Using the Werkzeug library for a somewhat simplified handling of WSGIapplications,we implement a web-based pricing service for volatility options based on the model andformula of Gruenbichler and Longstaff (1996).","The following web resources are helpful with regard to the topics covered in this chapter:%E2%80%A2The Pythondocumentation should be a starting point for the basic tools and tech%E2%80%90niques shown in this chapter: http://docs.python.org%3B see also this overview page:http://docs.python.org/2/howto/webservers.html.%E2%80%A2You should consult the home page of Bokehfor more on this webfocused plottinglibrary: http://bokeh.pydata.org.%E2%80%A2For more on Flask, start with the home page of the framework: http://flask.pocoo.org%3B also, download the PDF documentation: https://media.readthedocs.org/pdf/flask/latest/flask.pdf.%E2%80%A2Apart from the Python documentation itself, consult the home page of the Werkzeuglibrary for more on web services: http://werkzeug.pocoo.org.For a Flask reference in book form, see the following:%E2%80%A2Grinberg, Miguel (2014): Flask Web Development-Developing Web Applicationswith Python. O'Reilly, Sebastopol, CA.Finally, here is the research paper about the valuation of volatility options:%E2%80%A2Gruenbichler, Andreas and Francis Longstaff (1996): %E2%80%9CValuing Futures and Optionson Volatility.%E2%80%9D Journal of Banking and Finance, Vol. 20, pp. 985%E2%80%931001.","1Cf. Bittman, James (2009): Trading Options as a Professional(McGraw Hill, New York) for an introductionto and a comprehensive overview of options trading and related topics like market fundamentals and the roleof the so-called Greeks in options risk management.","This part of the book is concerned with the development of a smaller, but neverthelessstill powerful, real-world application for the pricing of options and derivatives by MonteCarlo simulation.1The goal is to have, in the end, a set of Python classes-a library wecall DX, for Derivatives AnalytiX-that allows us to do the following:ModelingTo model short rates for discounting purposes%3B to model European and Americanoptions, including their underlying risk factors, as well as their relevant marketenvironments%3B to model even complex portfolios consisting of multiple optionswith multiple, possibly correlated, underlying risk factorsSimulationTo simulate risk factors based on geometric Brownian motions and jump diffusionsas well as on square-root diffusions%3B to simulate a number of such risk factors si%E2%80%90multaneously and consistently, whether they are correlated or notValuationTo value, by the risk-neutral valuation approach, European and American optionswith arbitrary payoffs%3B to value portfolios composed of such options in a consistent,integrated fashionRisk managementTo estimate numerically the most important Greeks-i.e., the Delta and the Vegaof an option/derivative-independently of the underlying risk factor or the exercisetypeApplicationTo use the library to value and manage a VSTOXX volatility options portfolio in amarket-based manner (i.e., with a calibrated model for the VSTOXX)The material presented in this part of the book relies on the DX Analytics library, whichis developed and offered by the author and The Python Quants GmbH (in combinationwith the Python Quant Platform). The full-fledged version allows, for instance, themodeling, pricing, and risk management of complex, multi-risk derivatives and tradingbooks composed thereof.The part is divided into the following chapters:%E2%80%A2Chapter 15 presents the valuation framework in both theoretical and technicalform. Theoretically, the Fundamental Theorem of Asset Pricing and the risk-neutral valuation approach are central. Technically, the chapter presents Pythonclasses for risk-neutral discounting and for market environments.%E2%80%A2Chapter 16 is concerned with the simulation of risk factors based on geometricBrownian motions, jump diffusions, and square-root diffusion processes%3B a genericclass and three specialized classes are discussed.%E2%80%A2Chapter 17 addresses the valuation of single derivatives with European or Americanexercise based on a single underlying risk factor%3B again, a generic and two specializedclasses represent the major building blocks. The generic class allows the estimationof the Delta and the Vega independent of the option type.%E2%80%A2Chapter 18 is about the valuation of possibly complex derivatives portfolios withmultiple derivatives based on multiple, possibly correlated underlyings%3B a simpleclass for the modeling of a derivatives position is presented as well as a more com%E2%80%90plex class for a consistent portfolio valuation.%E2%80%A2Chapter 19 uses the DX library developed in the other chapters to value and managea portfolio of options on the VSTOXX volatility index.1.Cf. the book by Delbaen and Schachermayer (2004) for a comprehensive review and details of the mathe%E2%80%90matical machinery involved. See also Chapter 4 of Hilpisch (2015) for a shorter introduction, in particularfor the discrete time version.","Compound interest is the greatest mathematical discovery of all time.- Albert EinsteinThis chapter provides the framework for the development of the DX library by intro%E2%80%90ducing the most fundamental concepts needed for such an undertaking. It briefly re%E2%80%90views the Fundamental Theorem of Asset Pricing, which provides the theoretical back%E2%80%90ground for the simulation and valuation. It then proceeds by addressing the funda%E2%80%90mental concepts of date handling and risk-neutral discounting. We take only the simplestcase of constant short rates for the discounting, but more complex and realistic modelscan be added to the library quite easily. This chapter also introduces the concept of a_market environment_-i.e., a collection of constants, lists, and curves needed for theinstantiation of almost any other class to come in subsequent chapters.","The Fundamental Theorem of Asset Pricing is one of the cornerstones and success storiesof modern financial theory and mathematics.1The central notion underlying the Fun%E2%80%90damental Theorem of Asset Pricing is the concept of a martingalemeasure%3B i.e., a prob%E2%80%90ability measure that removes the drift from a discounted risk factor (stochastic process).In other words, under a martingale measure, all risk factors drift with the risk-free shortrate-and not with any other market rate involving some kind of risk premium overthe risk-free short rate.","2.The strategy would involve selling an option at a price of 2.5 USD and buying 0.25 stocks for 2.5 USD. Thepayoff of such a portfolio is 0 no matter what scenario plays out in the simple economy.","Consider a simple economy at the dates today and tomorrow with a risky asset, a %E2%80%9Cstock,%E2%80%9Dand a riskless asset, a %E2%80%9Cbond.%E2%80%9D The bond costs 10 USD today and pays off 10 USD to%E2%80%90morrow (zero interest rates). The stock costs 10 USD today and, with a probability of60%25 and 40%25, respectively, pays off 20 USD and 0 USD tomorrow. The riskless returnof the bond is 0. The expected return of the stock is 0.6%C2%B720%2B0.4%C2%B7010%E2%88%921%3D0.2, or 20%25. This isthe risk premium the stock pays for its riskiness.Consider now a call option with strike price of 15 USD. What is the fair value of sucha contingent claim that pays 5 USD with 60%25 probability and 0 USD otherwise%3F We cantake the expectation, for example, and discount the resulting value back (here with zerointerest rates). This approach yields a value of 0.6 %C2%B7 5 %3D 3 USD, since the option pays 5USD in the case where the stock price moves up to 20 USD and 0 USD otherwise.However, there is another approach that has been successfully applied to option pricingproblems like this: replication of the option's payoff through a portfolio of traded se%E2%80%90curities. It is easily verified that buying 0.25 of the stock perfectly replicates the option'spayoff (in the 60%25 case we then have 0.25 %C2%B7 20 %3D 5 USD). A quarter of the stock onlycosts 2.5 USD and not 3 USD. Taking expectations under the real-world probabilitymeasure overvalues the option.Why is this case%3F The real-world measure implies a risk premium of 20%25 for the stocksince the risk involved in the stock (gaining 100%25 or losing 100%25) is %E2%80%9Creal%E2%80%9D in the sensethat it cannot be diversified or hedged away. On the other hand, there is a portfolioavailable that replicates the option's payoff without any risk. This also implies thatsomeone writing (selling) such an option can completely hedge away any risk.2 Such aperfectly hedged portfolio of an option and a hedge position must yield the riskless ratein order to avoid arbitrage opportunities (i.e., the opportunity to make some moneyout of no money with a positive probability).Can we save the approach of taking expectations to value the call option%3F Yes, we can.We %E2%80%9Conly%E2%80%9D have to change the probability in such a way that the risky asset, the stock,drifts with the riskless short rate of zero. Obviously, a (martingale) measure giving equalmass of 50%25 to both scenarios accomplishes this%3B the calculation is 0.5%C2%B720%2B0.5%C2%B7010%E2%88%921%3D0.Now, taking expectations of the option's payoff under the new martingale measure yieldsthe correct (arbitrage-free) fair value: 0.5 %C2%B7 5 %2B 0.5 %C2%B7 0 %3D 2.5 USD.","3.Cf. Williams (1991) on the probabilistic concepts.4.Cf. Delbaen and Schachermayer (2004).","The beauty of this approach is that it carries over to even the most complex economieswith, for example, continuous time modeling (i.e., a continuum of points in time toconsider), large numbers of risky assets, complex derivative payoffs, etc.Therefore, consider a general market model in discrete time:3A general market model%E2%84%B3 in discrete time is a collection of:%E2%80%A2 A finite state space %ED%9B%BA%E2%80%A2 A filtration %ED%90%80%E2%80%A2 A strictly positive probability measure P defined on %E2%84%98(%ED%9B%BA)%E2%80%A2 A terminal date T%E2%88%88%E2%84%95, T %3C %E2%88%9E%E2%80%A2 A set %ED%90%80%E2%89%A1Stkt%E2%88%880,...,T:k%E2%88%880,...,K of K %2B 1 strictly positive security price processesWe write %E2%84%B3 %3D %7B(%ED%9B%BA,%E2%84%98(%ED%9B%BA),%ED%90%80,P),T,%ED%90%80%7D.Based on such a general market model, we can formulate the Fundamental Theorem ofAsset Pricing as follows:4Consider the general market model %E2%84%B3. According to the Fundamental Theorem of AssetPricing, the following three statements are equivalent:%E2%80%A2 There are no arbitrage opportunities in the market model %E2%84%B3.%E2%80%A2 The set %E2%84%9A of P-equivalent martingale measures is nonempty.%E2%80%A2 The set %E2%84%99 of consistent linear price systems is nonempty.When it comes to valuation and pricing of contingent claims (i.e., options, derivatives,futures, forwards, swaps, etc.), the importance of the theorem is illustrated by the fol%E2%80%90lowing corollary:If the market model %E2%84%B3 is arbitrage-free, then there exists a unique priceV0 associatedwith any attainable (i.e., replicable) contingent claim (option, derivative, etc.) VT. It sat%E2%80%90isfies %E2%88%80Q%E2%88%88%E2%84%9A:V0%3D%ED%90%800Qe%E2%88%92rTVT, where e%E2%80%93rTis the relevant risk-neutral discount factor fora constant short rate r.This result illustrates the importance of the theorem, and shows that our simple rea%E2%80%90soning from the introductory above indeed carries over to the general market model.Due to the role of the martingale measure, this approach to valuation is also often calledthe martingale approach, or-since under the martingale measure all risky assets driftwith the riskless short rate-the risk-neutral valuation approach. The second term","5.Adding a time component is actually a straightforward undertaking, which is nevertheless not done here forthe ease of the exposition.might, for our purposes, be the better one because in numerical applications, we %E2%80%9Csimply%E2%80%9Dlet the risk factors (stochastic processes) drift by the risk-neutral short rate. One doesnot have to deal with the probability measures directly for our applications-they are,however, what theoretically justifies the central theoretical results we apply and thetechnical approach we implement.Finally, consider market completeness in the general market model:The market model %E2%84%B3 is complete if it is arbitrage-free and if every contingent claim(option, derivative, etc.) is attainable (i.e., replicable).Suppose that the market model %E2%84%B3 is arbitrage-free. The market model is complete if andonly if %E2%84%9A is a singleton%3B i.e., if there is a unique P-equivalent martingale measure.This mainly completes the discussion of the theoretical background for what follows.For a detailed exposition of the concepts, notions, definitions, and results, refer toChapter 4 of Hilpisch (2015).","Obviously, risk-neutral discounting is central to the risk-neutral valuation approach.We therefore start by developing a Pythonclass for risk-neutral discounting. However,it pays to first have a closer look at the modeling and handling of relevant datesfor avaluation.","A necessary prerequisite for discounting is the modeling of dates (see also Appen%E2%80%90dix C). For valuation purposes, one typically divides the time interval between todayand the final date of the general market model T into discrete time intervals. These timeintervals can be homogenous (i.e., of equal length), or they can be heterogenous (i.e.,of varying length). A valuation library should be able to handle the more general caseof heterogeneous time intervals, since the simpler case is then automatically included.Therefore, we work with lists of dates, assuming that the smallest relevant time intervalis one day. This implies that we do not care about intraday events, for which we wouldhave to model time (in addition to dates).5To compile a list of relevant dates, one can basically take one of two approaches: con%E2%80%90structing a list of concrete dates (e.g., as datetime.datetimeobjects in Python) or ofyear fractions (as decimal numbers, as is often done in theoretical works).For example, the following two definitions of dates and fractionsare (roughly)equivalent:","In%5B1%5D:importdatetimeasdtIn%5B2%5D:dates%3D%5Bdt.datetime(2015,1,1),dt.datetime(2015,7,1),dt.datetime(2016,1,1)%5DIn%5B3%5D:(dates%5B1%5D-dates%5B0%5D).days/365.Out%5B3%5D:0.4958904109589041In%5B4%5D:(dates%5B2%5D-dates%5B1%5D).days/365.Out%5B4%5D:0.5041095890410959In%5B5%5D:fractions%3D%5B0.0,0.5,1.0%5DThey are only roughly equivalent since year fractions seldom lie on the beginning (0a.m.) of a certain day. Just consider the result of dividing a year by 50.Sometimes it is necessary to get year fractions out of a list of dates. The functionget_year_deltas presented in Example 15-1 does the job.Example 15-1. Function to get year fractions from a list or array of datetime objects## DX Library Frame# get_year_deltas.py#importnumpyasnpdefget_year_deltas(date_list,day_count%3D365.):''' Return vector of floats with day deltas in years.    Initial value normalized to zero.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    date_list : list or array        collection of datetime objects    day_count : float        number of days for a year        (to account for different conventions)    Results    %3D%3D%3D%3D%3D%3D%3D    delta_list : array        year fractions    '''start%3Ddate_list%5B0%5Ddelta_list%3D%5B(date-start).days/day_countfordateindate_list%5Dreturnnp.array(delta_list)This function can then be applied as follows:","6.For the pricing of, for example, short-dated options, this assumption seems satisfied in many circumstances.7.A unit zero-coupon bondpays exactly one currency unit at its maturity and no coupons between today andmaturity.8.See Chapter 13for the basics of object-oriented development in Python. Here, and for the rest of this part,we deviate from the standard PEP 8 naming conventions with regard to Python class names. PEP 8 recom%E2%80%90mends using %E2%80%9CCapWords%E2%80%9D or %E2%80%9CCamelCase%E2%80%9D convention in general for Python class names. We rather use thefunction name convention as mentioned in PEP 8 as a valid alternative %E2%80%9Cin cases where the interface is doc%E2%80%90umented and used primarily as a callable.%E2%80%9DIn%5B1%5D:importdatetimeasdtIn%5B2%5D:dates%3D%5Bdt.datetime(2015,1,1),dt.datetime(2015,7,1),dt.datetime(2016,1,1)%5DIn%5B3%5D:get_year_deltas(dates)Out%5B4%5D:array(%5B0.,0.49589041,1.%5D)When modeling the short rate, it becomes clear what the benefit of this is.","We focus on the simplest case for discounting by the short rate%3B namely, the case wherethe short rate is constant through time. Many option pricing models, like the ones ofBlack-Scholes-Merton (1973), Merton (1976), and Cox-Ross-Rubinstein (1979), makethis assumption.6 We assume continuous discounting, as is usual for option pricingapplications. In such a case, the general discount factor as of today, given a future datetand a constant short rate of r, is then given by D0(t) %3D e%E2%80%93rt. Of course, for the end of theeconomy we have the special case D0(T) %3D e%E2%80%93rT. Note that here both t and T are in yearfractions.The discount factors can also be interpreted as the value of a unit zero-coupon bond(ZCB) as of today, maturing at tand T, respectively.7Given two dates t%E2%89%A5s%E2%89%A5 0, thediscount factor relevant for discounting from t to sis then given by the equation Ds(t)%3D D0(t) / D0(s) %3D e%E2%80%93rt / e%E2%80%93rs %3D e%E2%80%93rt %C2%B7 ers %3D e%E2%80%93r(t%E2%80%93s).Example 15-2 presents a Python class that translates all these considerations into Pythoncode.8Example 15-2. Class for risk-neutral discounting with constant short rate## DX Library Frame# constant_short_rate.py#fromget_year_deltasimport*classconstant_short_rate(object):''' Class for constant short rate discounting.","    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    name : string        name of the object    short_rate : float (positive)        constant rate for discounting    Methods    %3D%3D%3D%3D%3D%3D%3D    get_discount_factors :        get discount factors given a list/array of datetime objects        or year fractions    '''def__init__(self,name,short_rate):self.name%3Dnameself.short_rate%3Dshort_rateifshort_rate%3C0:raiseValueError('Short rate negative.')defget_discount_factors(self,date_list,dtobjects%3DTrue):ifdtobjectsisTrue:dlist%3Dget_year_deltas(date_list)else:dlist%3Dnp.array(date_list)dflist%3Dnp.exp(self.short_rate*np.sort(-dlist))returnnp.array((date_list,dflist)).TThe application of the class constant_short_rateis best illustrated by a simple, con%E2%80%90crete example. We stick to the same list of datetime objects as before:In%5B1%5D:importdatetimeasdtIn%5B2%5D:dates%3D%5Bdt.datetime(2015,1,1),dt.datetime(2015,7,1),...:dt.datetime(2016,1,1)%5DIn%5B3%5D:fromconstant_short_rateimport*In%5B4%5D:csr%3Dconstant_short_rate('csr',0.05)In%5B5%5D:csr.get_discount_factors(dates)Out%5B5%5D:array(%5B%5Bdatetime.datetime(2015,1,1,0,0),0.95122942450071402%5D,%5Bdatetime.datetime(2015,7,1,0,0),0.9755103387657228%5D,%5Bdatetime.datetime(2016,1,1,0,0),1.0%5D%5D,dtype%3Dobject)The main result is a two-dimensional ndarray object containing pairs of a datetimeobject and the relevant discount factor. The class in general and the object csr in par%E2%80%90ticular work with year fractions as well:","9.On this concept see also Fletcher and Gardner (2009), who use market environments extensively.In%5B7%5D:deltas%3Dget_year_deltas(dates)In%5B8%5D:csr.get_discount_factors(deltas,dtobjects%3DFalse)Out%5B8%5D:array(%5B%5B0.,0.95122942%5D,%5B0.49589041,0.97551034%5D,%5B1.,1.%5D%5D)This class will take care of all discounting operations needed in other classes.","Market environmentis %E2%80%9Cjust%E2%80%9D a name for a collection of other data and Python objects.However, it is rather convenient to work with this abstraction since it simplifies a num%E2%80%90ber of operations and also allows for a consistent modeling of recurring aspects.9Amarket environment mainly consists of three dictionaries to store the following typesof data and Python objects:ConstantsThese can be, for example, model parameters or option maturity dates.ListsThese are sequences of objects in general, like a list object of objects modeling(risky) securities.CurvesThese are objects for discounting%3B for example, like an instance of the constant_short_rate class.Example 15-3 presents the market_environment class. Refer to Chapter 4 for a refresheron the handling of dict objects.Example 15-3. Class for modeling a market environment with constants, lists, andcurves## DX Library Frame# market_environment.py#classmarket_environment(object):''' Class to model a market environment relevant for valuation.    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    name: string","        name of the market environment    pricing_date : datetime object        date of the market environment    Methods    %3D%3D%3D%3D%3D%3D%3D    add_constant :        adds a constant (e.g. model parameter)    get_constant :        gets a constant    add_list :        adds a list (e.g. underlyings)    get_list :        gets a list    add_curve :        adds a market curve (e.g. yield curve)    get_curve :        gets a market curve    add_environment :        adds and overwrites whole market environments        with constants, lists, and curves    '''def__init__(self,name,pricing_date):self.name%3Dnameself.pricing_date%3Dpricing_dateself.constants%3D%7B%7Dself.lists%3D%7B%7Dself.curves%3D%7B%7Ddefadd_constant(self,key,constant):self.constants%5Bkey%5D%3Dconstantdefget_constant(self,key):returnself.constants%5Bkey%5Ddefadd_list(self,key,list_object):self.lists%5Bkey%5D%3Dlist_objectdefget_list(self,key):returnself.lists%5Bkey%5Ddefadd_curve(self,key,curve):self.curves%5Bkey%5D%3Dcurvedefget_curve(self,key):returnself.curves%5Bkey%5Ddefadd_environment(self,env):# overwrites existing values, if they existforkeyinenv.constants:self.constants%5Bkey%5D%3Denv.constants%5Bkey%5D","forkeyinenv.lists:self.lists%5Bkey%5D%3Denv.lists%5Bkey%5Dforkeyinenv.curves:self.curves%5Bkey%5D%3Denv.curves%5Bkey%5DAlthough there is nothing special in the market_environmentclass, a simple exampleshall illustrate how convenient it is to work with instances of the class:In%5B1%5D:frommarket_environmentimport*In%5B2%5D:importdatetimeasdtIn%5B3%5D:dates%3D%5Bdt.datetime(2015,1,1),dt.datetime(2015,7,1),dt.datetime(2016,1,1)%5DIn%5B4%5D:csr%3Dconstant_short_rate('csr',0.05)In%5B5%5D:me_1%3Dmarket_environment('me_1',dt.datetime(2015,1,1))In%5B6%5D:me_1.add_list('symbols',%5B'AAPL','MSFT','FB'%5D)In%5B7%5D:me_1.get_list('symbols')Out%5B7%5D:%5B'AAPL','MSFT','FB'%5DIn%5B8%5D:me_2%3Dmarket_environment('me_2',dt.datetime(2015,1,1))In%5B9%5D:me_2.add_constant('volatility',0.2)In%5B10%5D:me_2.add_curve('short_rate',csr)# add instance of discounting classIn%5B11%5D:me_2.get_curve('short_rate')Out%5B11%5D:%3Cconstant_short_rate.constant_short_rateat0x104ac3c90%3EIn%5B12%5D:me_1.add_environment(me_2)# add complete environmentIn%5B13%5D:me_1.get_curve('short_rate')Out%5B13%5D:%3Cconstant_short_rate.constant_short_rateat0x104ac3c90%3EIn%5B14%5D:me_1.constantsOut%5B14%5D:%7B'volatility':0.2%7DIn%5B15%5D:me_1.listsOut%5B15%5D:%7B'symbols':%5B'AAPL','MSFT','FB'%5D%7DIn%5B16%5D:me_1.curvesOut%5B16%5D:%7B'short_rate':%3Cconstant_short_rate.constant_short_rateat0x104ac3c90%3E%7DIn%5B17%5D:me_1.get_curve('short_rate').short_rateOut%5B17%5D:0.05This illustrates the basic handling of this rather generic %E2%80%9Cstorage%E2%80%9D class. For practicalapplications, market data and other data as well as Python objects are first collected,","then a market_environment object is instantiated and filled with the relevant data andobjects. This is then delivered in a single step to other classes that need the data andobjects stored in the respective market_environment object.A major advantage of this object-oriented modeling approach is, for example, that in%E2%80%90stances of the constant_short_rateclass can live in multiple environments. Once theinstance is updated-for example, when a new constant short rate is set-all the in%E2%80%90stances of the market_environmentclass containing that particular instance of the dis%E2%80%90counting class will be updated automatically.","This chapter provides the framework for the larger project of building a Python libraryto value options and other derivatives by Monte Carlo simulation. The chapter intro%E2%80%90duces the Fundamental Theorem of Asset Pricing, illustrating it by a rather simple nu%E2%80%90merical example. Important results in this regard are provided for a general marketmodel in discrete time.The chapter also develops a Python class for risk-neutral discounting purposes to makenumerical use of the machinery of the Fundamental Theorem of Asset Pricing. Basedon a list of either Pythondatetime objects or floats representing year fractions, in%E2%80%90stances of the class constant_short_rate provide the respective discount factors(present values of unit zero-coupon bonds).The chapter concludes with the rather generic market_environmentclass, which allowsfor the collection of relevant data and Pythonobjects for modeling, simulation, valua%E2%80%90tion, and other purposes.To simplify future imports we will use a wrapper module called dx_frame.py, as pre%E2%80%90sented in Example 15-4.Example 15-4. Wrapper module for framework components## DX Library Frame# dx_frame.py#importdatetimeasdtfromget_year_deltasimportget_year_deltasfromconstant_short_rateimportconstant_short_ratefrommarket_environmentimportmarket_environmentA single import statement like the following then makes all framework componentsavailable in a single step:fromdx_frameimport*","Thinking of a Pythonlibrary and a package of modules, there is also the option to storeall relevant Python modules in a (sub)directory and to put in that directory a specialinit file that does all the imports. For example, when storing all modules in a directorycalled dx, say, the file presented in Example 15-5 does the job. However, notice thenaming convention for this particular file.Example 15-5. Python packaging file## DX Library# packaging file# __init__.py#importdatetimeasdtfromget_year_deltasimportget_year_deltasfromconstant_short_rateimportconstant_short_ratefrommarket_environmentimportmarket_environmentIn that case you can just use the directory name to accomplish all the imports at once:fromdximport*Or via the alternative approach:importdx","Useful references in book form for the topics covered in this chapter are:%E2%80%A2Delbaen, Freddy and Walter Schachermayer (2004): The Mathematics of Arbi%E2%80%90trage. Springer Verlag, Berlin, Heidelberg.%E2%80%A2Fletcher, Shayne and Christopher Gardner (2009): Financial Modelling in Python.John Wiley %26 Sons, Chichester, England.%E2%80%A2Hilpisch, Yves (2015): Derivatives Analytics with Python. Wiley Finance, Chiches%E2%80%90ter, England. http://derivatives-analytics-with-python.com.%E2%80%A2Williams, David (1991): Probability with Martingales. Cambridge University Press,Cambridge, England.For the original research papers defining the models cited in this chapter, refer to the%E2%80%9CFurther Reading%E2%80%9D sections in subsequent chapters.","The purpose of science is not to analyze ordescribe but to make useful models of the world.- Edward de BonoChapter 10 introduces in some detail the Monte Carlo simulation of stochastic processesusing Python and NumPy. This chapter applies the basic techniques presented there toimplement simulation classes as a central component of the DX library. We restrict ourattention to three widely used stochastic processes:Geometric Brownian motionThis is the process that was introduced to the option pricing literature by the seminalwork of Black and Scholes (1973)%3B it is used several times throughout this book andstill represents-despite its known shortcomings and given the mounting empiricalevidence from financial reality-a benchmark process for option and derivativevaluation purposes.Jump diffusionThe jump diffusion, as introduced by Merton (1976), adds a log-normally dis%E2%80%90tributed jump component to the geometric Brownian motion (GBM)%3B this allowsus to take into account that, for example, short-term out-of-the-money (OTM)options often seem to have priced in the possibility of large jumps. In other words,relying on GBM as a financial model often cannot explain the market values of suchOTM options satisfactorily, while a jump diffusion may be able to do so.Square-root diffusionThe square-root diffusion, popularized for finance by Cox, Ingersoll, and Ross(1985), is used to model mean-reverting quantities like interest rates and volatility%3Bin addition to being mean-reverting, the process stays positive, which is generallya desirable characteristic for those quantities.","1.We speak of %E2%80%9Crandom%E2%80%9D numbers knowing that they are in general %E2%80%9Cpseudorandom%E2%80%9D only.2.Cf. Glasserman (2004), Chapter 2, on generating random numbers and random variables.The chapter proceeds in the first section with developing a function to generate standardnormally distributed random numbers using variance reduction techniques.1Subse%E2%80%90quent sections then develop a generic simulation class and three specific simulationclasses, one for each of the aforementioned stochastic processes of interest.For further details on the simulation of the models presented in this chapter, refer alsoto Hilpisch (2015). In particular, that book also contains a complete case study basedon the jump diffusion model of Merton (1976).","Random number generation is a central task of Monte Carlo simulation.2Chapter 10shows how to use Python and libraries such as numpy.randomto generate random num%E2%80%90bers with different distributions. For our project at hand, standard normally distributedrandom numbers are the most important ones. That is why it pays off to have a conve%E2%80%90nience function available for generating this particular type of random numbers.Example 16-1 presents such a function.Example 16-1. Function to generate standard normally distributed random numbersimportnumpyasnpdefsn_random_numbers(shape,antithetic%3DTrue,moment_matching%3DTrue,fixed_seed%3DFalse):''' Returns an array of shape shape with (pseudo)random numbers    that are standard normally distributed.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    shape : tuple (o, n, m)        generation of array with shape (o, n, m)    antithetic : Boolean        generation of antithetic variates    moment_matching : Boolean        matching of first and second moments    fixed_seed : Boolean        flag to fix the seed    Results    %3D%3D%3D%3D%3D%3D%3D    ran : (o, n, m) array of (pseudo)random numbers    '''iffixed_seed:np.random.seed(1000)ifantithetic:","3.Glasserman (2004) presents in Chapter 4 an overview and theoretical details of different variance reductiontechniques.ran%3Dnp.random.standard_normal((shape%5B0%5D,shape%5B1%5D,shape%5B2%5D/2))ran%3Dnp.concatenate((ran,-ran),axis%3D2)else:ran%3Dnp.random.standard_normal(shape)ifmoment_matching:ran%3Dran-np.mean(ran)ran%3Dran/np.std(ran)ifshape%5B0%5D%3D%3D1:returnran%5B0%5Delse:returnranThe variance reduction techniques used in this function, namely antithetic pathsandmoment matching, are also illustrated in Chapter 10.3The application of the function is straightforward:In%5B1%5D:fromsn_random_numbersimport*In%5B2%5D:snrn%3Dsn_random_numbers((2,2,2),antithetic%3DFalse,...:moment_matching%3DFalse,...:fixed_seed%3DTrue)In%5B3%5D:snrnOut%5B3%5D:array(%5B%5B%5B-0.8044583,0.32093155%5D,%5B-0.02548288,0.64432383%5D%5D,%5B%5B-0.30079667,0.38947455%5D,%5B-0.1074373,-0.47998308%5D%5D%5D)In%5B4%5D:snrn_mm%3Dsn_random_numbers((2,3,2),antithetic%3DFalse,...:moment_matching%3DTrue,...:fixed_seed%3DTrue)In%5B5%5D:snrn_mmOut%5B5%5D:array(%5B%5B%5B-1.47414161,0.67072537%5D,%5B0.01049828,1.28707482%5D,%5B-0.51421897,0.80136066%5D%5D,%5B%5B-0.14569767,-0.85572818%5D,%5B1.19313679,-0.82653845%5D,%5B1.3308292,-1.47730025%5D%5D%5D)In%5B6%5D:snrn_mm.mean()Out%5B6%5D:1.8503717077085941e-17","In%5B7%5D:snrn_mm.std()Out%5B7%5D:1.0This function will prove a workhorse for the simulation classes to follow.","Object-oriented modeling-as introduced in Chapter 13-allows inheritance of at%E2%80%90tributes and methods. This is what we want to make use of when building our simulationclasses: we start with a generic simulation class containing those attributes and methodsthat all other simulation classes share.To begin with, it is noteworthy that we instantiate an object of any simulation class by%E2%80%9Conly%E2%80%9D providing three attributes:nameA string object as a name for the model simulation objectmar_envAn instance of the market_environment classcorrA flag (bool) indicating whether the object is correlated or notThis again illustrates the role of a market environment: to provide in a single step alldata and objects required for simulation and valuation. The methods of the genericclass are:generate_time_gridThis method generates the time grid of relevant dates used for the simulation%3B thistask is the same for every simulation class.get_instrument_valuesEvery simulation class has to return the ndarray object with the simulated instru%E2%80%90ment values (e.g., simulated stock prices, commodities prices, volatilities).Example 16-2presents such a generic model simulation class. The methods make useof other methods that the model-tailored classes will provide, like self.generate_paths. All details in this regard will become clear when we have the full picture ofa specialized, nongeneric simulation class.Example 16-2. Generic financial model simulation class## DX Library Simulation# simulation_class.py#importnumpyasnpimportpandasaspd","classsimulation_class(object):''' Providing base methods for simulation classes.    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    name : string        name of the object    mar_env : instance of market_environment        market environment data for simulation    corr : Boolean        True if correlated with other model object    Methods    %3D%3D%3D%3D%3D%3D%3D    generate_time_grid :        returns time grid for simulation    get_instrument_values :        returns the current instrument values (array)    '''def__init__(self,name,mar_env,corr):try:self.name%3Dnameself.pricing_date%3Dmar_env.pricing_dateself.initial_value%3Dmar_env.get_constant('initial_value')self.volatility%3Dmar_env.get_constant('volatility')self.final_date%3Dmar_env.get_constant('final_date')self.currency%3Dmar_env.get_constant('currency')self.frequency%3Dmar_env.get_constant('frequency')self.paths%3Dmar_env.get_constant('paths')self.discount_curve%3Dmar_env.get_curve('discount_curve')try:# if time_grid in mar_env take this# (for portfolio valuation)self.time_grid%3Dmar_env.get_list('time_grid')except:self.time_grid%3DNonetry:# if there are special dates, then add theseself.special_dates%3Dmar_env.get_list('special_dates')except:self.special_dates%3D%5B%5Dself.instrument_values%3DNoneself.correlated%3DcorrifcorrisTrue:# only needed in a portfolio context when# risk factors are correlatedself.cholesky_matrix%3Dmar_env.get_list('cholesky_matrix')self.rn_set%3Dmar_env.get_list('rn_set')%5Bself.name%5Dself.random_numbers%3Dmar_env.get_list('random_numbers')","except:print%22Error parsing market environment.%22defgenerate_time_grid(self):start%3Dself.pricing_dateend%3Dself.final_date# pandas date_range function# freq %3D e.g. 'B' for Business Day,# 'W' for Weekly, 'M' for Monthlytime_grid%3Dpd.date_range(start%3Dstart,end%3Dend,freq%3Dself.frequency).to_pydatetime()time_grid%3Dlist(time_grid)# enhance time_grid by start, end, and special_datesifstartnotintime_grid:time_grid.insert(0,start)# insert start date if not in listifendnotintime_grid:time_grid.append(end)# insert end date if not in listiflen(self.special_dates)%3E0:# add all special datestime_grid.extend(self.special_dates)# delete duplicatestime_grid%3Dlist(set(time_grid))# sort listtime_grid.sort()self.time_grid%3Dnp.array(time_grid)defget_instrument_values(self,fixed_seed%3DTrue):ifself.instrument_valuesisNone:# only initiate simulation if there are no instrument valuesself.generate_paths(fixed_seed%3Dfixed_seed,day_count%3D365.)eliffixed_seedisFalse:# also initiate resimulation when fixed_seed is Falseself.generate_paths(fixed_seed%3Dfixed_seed,day_count%3D365.)returnself.instrument_valuesParsing of the market environment is embedded in a singletry-except clause, whichraises an exception whenever the parsing fails. To keep the code concise, there are nosanity checks implemented. For example, the following line of code is considered a%E2%80%9Csuccess,%E2%80%9D no matter if the content is indeed an instance of a discounting class or not.Therefore, one has to be rather careful when compiling and passing market_environment objects to any simulation class:self.discount_curve%3Dmar_env.get_curve('discount_curve')Table 16-1shows all components that a market_environmentobject must contain forthe generic and therefore for all other simulation classes.","Table 16-1. Elements of market environment for all simulation classes","initial_valueConstantYesInitial value of process at pricing_datevolatilityConstantYesVolatility coefficient of processfinal_dateConstantYesSimulation horizoncurrencyConstantYesCurrency of the financial entityfrequencyConstantYesDate frequency, as pandasfreq parameterpathsConstantYesNumber of paths to be simulateddiscount_curveCurveYesInstance of constant_short_ratetime_gridListNoTime grid of relevant dates (in portfolio context)random_numbersListNoRandom number array (for correlated objects)cholesky_matrixListNoCholesky matrix (for correlated objects)rn_setListNodict object with pointer to relevant random number setEverything that has to do with the correlation of model simulation objects is explainedin subsequent chapters. In this chapter, we focus on the simulation of single, uncorre%E2%80%90lated processes. Similarly, the option to pass a time_gridis only relevant in a portfoliocontext, something also explained later.","Geometric Brownian motion is a stochastic process as described in Equation 16-1 (seealso Equation 10-2 in Chapter 10, in particular for the meaning of the parameters andvariables). The drift of the process is already set equal to the riskless, constant short rater, implying that we operate under the equivalent martingale measure (see Chapter 15).Equation 16-1. Stochastic differential equation of geometric Brownian motiondSt %3D rStdt %2B %ED%9C%8EStdZtEquation 16-2 presents an Euler discretization of the stochastic differential equation forsimulation purposes (see also Equation 10-3 in Chapter 10 for further details). We workin a discrete time market model, such as the general market model %E2%84%B3 from Chap%E2%80%90ter 15, with a finite set of relevant dates 0 %3C t1 %3C t2 %3C %E2%80%A6 %3C T.","Equation 16-2. Difference equation to simulate the geometric Brownian motionStm%2B1%3DStmexpr%E2%88%9212%CF%832tm%2B1%E2%88%92tm%2B%CF%83tm%2B1%E2%88%92tmzt0%E2%89%A4tm%3Ctm%2B1%E2%89%A4T","Example 16-3now presents the specialized class for the GBM model. We present it inits entirety first and highlight selected aspects afterward.Example 16-3. Simulation class for geometric Brownian motion## DX Library Simulation# geometric_brownian_motion.py#importnumpyasnpfromsn_random_numbersimportsn_random_numbersfromsimulation_classimportsimulation_classclassgeometric_brownian_motion(simulation_class):''' Class to generate simulated paths based on    the Black-Scholes-Merton geometric Brownian motion model.    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    name : string        name of the object    mar_env : instance of market_environment        market environment data for simulation    corr : Boolean        True if correlated with other model simulation object    Methods    %3D%3D%3D%3D%3D%3D%3D    update :        updates parameters    generate_paths :        returns Monte Carlo paths given the market environment    '''def__init__(self,name,mar_env,corr%3DFalse):super(geometric_brownian_motion,self).__init__(name,mar_env,corr)defupdate(self,initial_value%3DNone,volatility%3DNone,final_date%3DNone):ifinitial_valueisnotNone:self.initial_value%3Dinitial_valueifvolatilityisnotNone:","self.volatility%3Dvolatilityiffinal_dateisnotNone:self.final_date%3Dfinal_dateself.instrument_values%3DNonedefgenerate_paths(self,fixed_seed%3DFalse,day_count%3D365.):ifself.time_gridisNone:self.generate_time_grid()# method from generic simulation class# number of dates for time gridM%3Dlen(self.time_grid)# number of pathsI%3Dself.paths# array initialization for path simulationpaths%3Dnp.zeros((M,I))# initialize first date with initial_valuepaths%5B0%5D%3Dself.initial_valueifnotself.correlated:# if not correlated, generate random numbersrand%3Dsn_random_numbers((1,M,I),fixed_seed%3Dfixed_seed)else:# if correlated, use random number object as provided# in market environmentrand%3Dself.random_numbersshort_rate%3Dself.discount_curve.short_rate# get short rate for drift of processfortinrange(1,len(self.time_grid)):# select the right time slice from the relevant# random number setifnotself.correlated:ran%3Drand%5Bt%5Delse:ran%3Dnp.dot(self.cholesky_matrix,rand%5B:,t,:%5D)ran%3Dran%5Bself.rn_set%5Ddt%3D(self.time_grid%5Bt%5D-self.time_grid%5Bt-1%5D).days/day_count# difference between two dates as year fractionpaths%5Bt%5D%3Dpaths%5Bt-1%5D*np.exp((short_rate-0.5*self.volatility**2)*dt%2Bself.volatility*np.sqrt(dt)*ran)# generate simulated values for the respective dateself.instrument_values%3DpathsIn this particular case, the market_environment object has to contain only the data andobjects shown in Table 16-1-i.e., the minimum set of components.The method update does what its name suggests: it allows the updating of selectedimportant parameters of the model. The method generate_paths is, of course, a bitmore involved. However, it has a number of inline comments that should make clearthe most important aspects. Some complexity is brought into this method by, in","principle, allowing for the correlation between different model simulation objects. Thiswill become clearer, especially in Example 18-2.","The following interactive IPythonsession illustrates the use of the geometric_brownian_motion class. First, we have to generate a market_environmentobject with allmandatory elements:In%5B1%5D:fromdximport*In%5B2%5D:me_gbm%3Dmarket_environment('me_gbm',dt.datetime(2015,1,1))In%5B3%5D:me_gbm.add_constant('initial_value',36.)me_gbm.add_constant('volatility',0.2)me_gbm.add_constant('final_date',dt.datetime(2015,12,31))me_gbm.add_constant('currency','EUR')me_gbm.add_constant('frequency','M')# monthly frequency (respective month end)me_gbm.add_constant('paths',10000)In%5B4%5D:csr%3Dconstant_short_rate('csr',0.05)In%5B5%5D:me_gbm.add_curve('discount_curve',csr)Second, we instantiate a model simulation object:In%5B6%5D:fromdx_simulationimport*In%5B7%5D:gbm%3Dgeometric_brownian_motion('gbm',me_gbm)Third, we can work with the object. For example, let us generate and inspect thetime_grid. You will notice that we have 13 datetime objects in the time_gridarrayobject (all the month ends in the relevant year, plus the pricing_date):In%5B8%5D:gbm.generate_time_grid()In%5B9%5D:gbm.time_gridOut%5B9%5D: array(%5Bdatetime.datetime(2015, 1, 1, 0, 0),               datetime.datetime(2015, 1, 31, 0, 0),               datetime.datetime(2015, 2, 28, 0, 0),               datetime.datetime(2015, 3, 31, 0, 0),               datetime.datetime(2015, 4, 30, 0, 0),               datetime.datetime(2015, 5, 31, 0, 0),               datetime.datetime(2015, 6, 30, 0, 0),               datetime.datetime(2015, 7, 31, 0, 0),               datetime.datetime(2015, 8, 31, 0, 0),               datetime.datetime(2015, 9, 30, 0, 0),               datetime.datetime(2015, 10, 31, 0, 0),               datetime.datetime(2015, 11, 30, 0, 0),               datetime.datetime(2015, 12, 31, 0, 0)%5D, dtype%3Dobject)Next, we might ask for the simulated instrument values:","In%5B10%5D:%25timepaths_1%3Dgbm.get_instrument_values()Out%5B10%5D: CPU times: user 10.7 ms, sys: 2.91 ms, total: 13.6 ms         Wall time: 12.8 msIn%5B11%5D:paths_1Out%5B11%5D: array(%5B%5B 36.        ,  36.        ,  36.        , ...,  36.        ,                  36.        ,  36.        %5D,                %5B 37.37221481,  38.08890977,  34.37156575, ...,  36.22258915,                  35.05503522,  39.63544014%5D,                %5B 39.45866146,  42.18817025,  32.38579992, ...,  34.80319951,                  33.60600939,  37.62733874%5D,                ...,                %5B 40.15717404,  33.16701733,  23.32556112, ...,  37.5619937 ,                  29.89282508,  30.2202427 %5D,                %5B 42.0974104 ,  36.59006321,  21.70771374, ...,  35.70950512,                  30.64670854,  30.45901309%5D,                %5B 43.33170027,  37.42993532,  23.8840177 , ...,  35.92624556,                  27.87720187,  28.77424561%5D%5D)Let us generate instrument values for a higher volatility as well:In%5B12%5D:gbm.update(volatility%3D0.5)In%5B13%5D:%25timepaths_2%3Dgbm.get_instrument_values()Out%5B13%5D: CPU times: user 9.78 ms, sys: 1.36 ms, total: 11.1 ms         Wall time: 10.2 msThe difference in the two sets of paths is illustrated in Figure 16-1:In%5B14%5D:importmatplotlib.pyplotasplt%25matplotlibinlineplt.figure(figsize%3D(8,4))p1%3Dplt.plot(gbm.time_grid,paths_1%5B:,:10%5D,'b')p2%3Dplt.plot(gbm.time_grid,paths_2%5B:,:10%5D,'r-.')plt.grid(True)l1%3Dplt.legend(%5Bp1%5B0%5D,p2%5B0%5D%5D,%5B'low volatility','high volatility'%5D,loc%3D2)plt.gca().add_artist(l1)plt.xticks(rotation%3D30)","Figure 16-1. Simulated paths from GBM simulation class","Equipped with the background knowledge from the geometric_brownian_motionclass, it is now straightforward to implement a class for the jump diffusion model de%E2%80%90scribed by Merton (1976). Recall the stochastic differential equation of the jump diffu%E2%80%90sion, as shown in Equation 16-3 (see also Equation 10-8 in Chapter 10, in particular forthe meaning of the parameters and variables).Equation 16-3. Stochastic differential equation for Merton jump diffusion modeldSt %3D (r %E2%80%93 rJ)Stdt %2B %ED%9C%8EStdZt %2B JtStdNtAn Euler discretization for simulation purposes is presented in Equation 16-4 (see alsoEquation 10-9 in Chapter 10 and the more detailed explanations given there).Equation 16-4. Euler discretization for Merton jump diffusion modelStm%2B1%3DStmexpr%E2%88%92rJ%E2%88%9212%CF%832tm%2B1%E2%88%92tm%2B%CF%83tm%2B1%E2%88%92tmzt1%2Be%CE%BCJ%2B%CE%B4zt2%E2%88%921yt0%E2%89%A4tm%3Ctm%2B1%E2%89%A4T","Example 16-4 presents the Python code for the jump_diffusion simulation class. Thisclass should by now contain no surprises. Of course, the model is different, but thedesign and the methods are essentially the same.","Example 16-4. Simulation class for jump diffusion## DX Library Simulation# jump_diffusion.py#importnumpyasnpfromsn_random_numbersimportsn_random_numbersfromsimulation_classimportsimulation_classclassjump_diffusion(simulation_class):''' Class to generate simulated paths based on    the Merton (1976) jump diffusion model.    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    name : string        name of the object    mar_env : instance of market_environment        market environment data for simulation    corr : Boolean        True if correlated with other model object    Methods    %3D%3D%3D%3D%3D%3D%3D    update :        updates parameters    generate_paths :        returns Monte Carlo paths given the market environment    '''def__init__(self,name,mar_env,corr%3DFalse):super(jump_diffusion,self).__init__(name,mar_env,corr)try:# additional parameters neededself.lamb%3Dmar_env.get_constant('lambda')self.mu%3Dmar_env.get_constant('mu')self.delt%3Dmar_env.get_constant('delta')except:print%22Error parsing market environment.%22defupdate(self,initial_value%3DNone,volatility%3DNone,lamb%3DNone,mu%3DNone,delta%3DNone,final_date%3DNone):ifinitial_valueisnotNone:self.initial_value%3Dinitial_valueifvolatilityisnotNone:self.volatility%3DvolatilityiflambisnotNone:self.lamb%3DlambifmuisnotNone:self.mu%3DmuifdeltaisnotNone:","self.delt%3Ddeltaiffinal_dateisnotNone:self.final_date%3Dfinal_dateself.instrument_values%3DNonedefgenerate_paths(self,fixed_seed%3DFalse,day_count%3D365.):ifself.time_gridisNone:self.generate_time_grid()# method from generic simulation class# number of dates for time gridM%3Dlen(self.time_grid)# number of pathsI%3Dself.paths# array initialization for path simulationpaths%3Dnp.zeros((M,I))# initialize first date with initial_valuepaths%5B0%5D%3Dself.initial_valueifself.correlatedisFalse:# if not correlated, generate random numberssn1%3Dsn_random_numbers((1,M,I),fixed_seed%3Dfixed_seed)else:# if correlated, use random number object as provided# in market environmentsn1%3Dself.random_numbers# standard normally distributed pseudorandom numbers# for the jump componentsn2%3Dsn_random_numbers((1,M,I),fixed_seed%3Dfixed_seed)rj%3Dself.lamb*(np.exp(self.mu%2B0.5*self.delt**2)-1)short_rate%3Dself.discount_curve.short_ratefortinrange(1,len(self.time_grid)):# select the right time slice from the relevant# random number setifself.correlatedisFalse:ran%3Dsn1%5Bt%5Delse:# only with correlation in portfolio contextran%3Dnp.dot(self.cholesky_matrix,sn1%5B:,t,:%5D)ran%3Dran%5Bself.rn_set%5Ddt%3D(self.time_grid%5Bt%5D-self.time_grid%5Bt-1%5D).days/day_count# difference between two dates as year fractionpoi%3Dnp.random.poisson(self.lamb*dt,I)# Poisson-distributed pseudorandom numbers for jump componentpaths%5Bt%5D%3Dpaths%5Bt-1%5D*(np.exp((short_rate-rj-0.5*self.volatility**2)*dt%2Bself.volatility*np.sqrt(dt)*ran)%2B(np.exp(self.mu%2Bself.delt*","sn2%5Bt%5D)-1)*poi)self.instrument_values%3DpathsOf course, since we are dealing now with a different model, we need a different set ofelements in the market_environment object. In addition to those for the geometric_brownian_motion class (see Table 16-1), there are three additions, as outlined inTable 16-2: namely, the parameters of the log-normal jump component, lambda, mu, anddelta.Table 16-2. Specific elements of market environment for jump_diffusion class","lambdaConstantYesJump intensity (probability p.a.)muConstantYesExpected jump sizedeltaConstantYesStandard deviation of jump sizeFor the generation of the paths, this class of course needs further random numbersbecause of the jump component. Inline comments in the method generate_pathshighlight the two spots where these additional random numbers are generated. For thegeneration of Poisson-distributed random numbers, see also Chapter 10.","In what follows, we again illustrate the use of the simulation class jump_diffusioninteractively. We make use of the market_environment object defined for the GBMobject in the previous section:In%5B15%5D:me_jd%3Dmarket_environment('me_jd',dt.datetime(2015,1,1))In%5B16%5D:# add jump diffusion specific parametersme_jd.add_constant('lambda',0.3)me_jd.add_constant('mu',-0.75)me_jd.add_constant('delta',0.1)To this environment, we add the complete environment of the GBM simulation class,which completes the input needed:In%5B17%5D:me_jd.add_environment(me_gbm)Based on this market_environment object, we can instantiate the simulation class forthe jump diffusion:In%5B18%5D:fromjump_diffusionimportjump_diffusionIn%5B19%5D:jd%3Djump_diffusion('jd',me_jd)Due to the modeling approach we have implemented, the generation of instrumentvalues is now formally the same. The method call in this case is a bit slower, however,since we need to simulate more numerical values due to the jump component:","In%5B20%5D:%25timepaths_3%3Djd.get_instrument_values()Out%5B20%5D: CPU times: user 19.7 ms, sys: 2.92 ms, total: 22.6 ms         Wall time: 21.9 msWith the aim of again comparing two different sets of paths, change, for example, thejump probability:In%5B21%5D:jd.update(lamb%3D0.9)In%5B22%5D:%25timepaths_4%3Djd.get_instrument_values()Out%5B22%5D: CPU times: user 26.3 ms, sys: 2.07 ms, total: 28.4 ms         Wall time: 27.7 msFigure 16-2compares a couple of simulated paths from the two sets with low and highintensity (jump probability), respectively. You can spot a few jumps for the low intensitycase and multiple jumps for the high intensity case in the figure:In%5B23%5D:plt.figure(figsize%3D(8,4))p1%3Dplt.plot(gbm.time_grid,paths_3%5B:,:10%5D,'b')p2%3Dplt.plot(gbm.time_grid,paths_4%5B:,:10%5D,'r-.')plt.grid(True)l1%3Dplt.legend(%5Bp1%5B0%5D,p2%5B0%5D%5D,%5B'low intensity','high intensity'%5D,loc%3D3)plt.gca().add_artist(l1)plt.xticks(rotation%3D30)Figure 16-2. Simulated paths from jump diffusion simulation class","The third stochastic process to be simulated is the square-root diffusion as used by Cox,Ingersoll, and Ross (1985) to model stochastic short rates. Equation 16-5shows thestochastic differential equation of the process (see also Equation 10-4 in Chapter 10 forfurther details).","Equation 16-5. Stochastic differential equation of square-root diffusiondxt%3D%CE%BA%CE%B8%E2%88%92xtdt%2B%CF%83xtdZtWe use the discretization scheme as presented in Equation 16-6 (see also Equation 10-5in Chapter 10, as well as Equation 10-6, for an alternative, exact scheme).Equation 16-6. Euler discretization for square-root diffusion (full truncation scheme)x%CB%9Ctm%2B1%3Dx%CB%9Ctm%2B%CE%BA%CE%B8%E2%88%92x%CB%9Cs%2Btm%2B1%E2%88%92tm%2B%CF%83x%CB%9Cs%2Btm%2B1%E2%88%92tmztxtm%2B1%3Dx%CB%9Ctm%2B1%2B","Example 16-5 presents the Python code for the square_root_diffusionsimulationclass. Apart from, of course, a different model and discretization scheme, the class doesnot contain anything new compared to the other two specialized classes.Example 16-5. Simulation class for square-root diffusion## DX Library Simulation# square_root_diffusion.py#importnumpyasnpfromsn_random_numbersimportsn_random_numbersfromsimulation_classimportsimulation_classclasssquare_root_diffusion(simulation_class):''' Class to generate simulated paths based on    the Cox-Ingersoll-Ross (1985) square-root diffusion model.    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    name : string        name of the object    mar_env : instance of market_environment        market environment data for simulation    corr : Boolean        True if correlated with other model object    Methods    %3D%3D%3D%3D%3D%3D%3D","    update :        updates parameters    generate_paths :        returns Monte Carlo paths given the market environment    '''def__init__(self,name,mar_env,corr%3DFalse):super(square_root_diffusion,self).__init__(name,mar_env,corr)try:self.kappa%3Dmar_env.get_constant('kappa')self.theta%3Dmar_env.get_constant('theta')except:print%22Error parsing market environment.%22defupdate(self,initial_value%3DNone,volatility%3DNone,kappa%3DNone,theta%3DNone,final_date%3DNone):ifinitial_valueisnotNone:self.initial_value%3Dinitial_valueifvolatilityisnotNone:self.volatility%3DvolatilityifkappaisnotNone:self.kappa%3DkappaifthetaisnotNone:self.theta%3Dthetaiffinal_dateisnotNone:self.final_date%3Dfinal_dateself.instrument_values%3DNonedefgenerate_paths(self,fixed_seed%3DTrue,day_count%3D365.):ifself.time_gridisNone:self.generate_time_grid()M%3Dlen(self.time_grid)I%3Dself.pathspaths%3Dnp.zeros((M,I))paths_%3Dnp.zeros_like(paths)paths%5B0%5D%3Dself.initial_valuepaths_%5B0%5D%3Dself.initial_valueifself.correlatedisFalse:rand%3Dsn_random_numbers((1,M,I),fixed_seed%3Dfixed_seed)else:rand%3Dself.random_numbersfortinrange(1,len(self.time_grid)):dt%3D(self.time_grid%5Bt%5D-self.time_grid%5Bt-1%5D).days/day_countifself.correlatedisFalse:ran%3Drand%5Bt%5Delse:ran%3Dnp.dot(self.cholesky_matrix,rand%5B:,t,:%5D)ran%3Dran%5Bself.rn_set%5D# full truncation Euler discretization","paths_%5Bt%5D%3D(paths_%5Bt-1%5D%2Bself.kappa*(self.theta-np.maximum(0,paths_%5Bt-1,:%5D))*dt%2Bnp.sqrt(np.maximum(0,paths_%5Bt-1,:%5D))*self.volatility*np.sqrt(dt)*ran)paths%5Bt%5D%3Dnp.maximum(0,paths_%5Bt%5D)self.instrument_values%3DpathsTable 16-3 lists the two elements of the market environment that are specific to thisclass.Table 16-3. Specific elements of market environment for square_root_diffusion class","kappaConstantYesMean reversion factorthetaConstantYesLong-term mean of process","A rather brief use case illustrates the use of the simulation class. As usual, we need amarket environment, for example to model a volatility (index) process:In%5B35%5D:me_srd%3Dmarket_environment('me_srd',dt.datetime(2015,1,1))In%5B36%5D:me_srd.add_constant('initial_value',.25)me_srd.add_constant('volatility',0.05)me_srd.add_constant('final_date',dt.datetime(2015,12,31))me_srd.add_constant('currency','EUR')me_srd.add_constant('frequency','W')me_srd.add_constant('paths',10000)Two components of the market environment are specific to the class:In%5B37%5D:# specific to simualation classme_srd.add_constant('kappa',4.0)me_srd.add_constant('theta',0.2)Although we do not need it here to implement the simulation, the generic simulationclass requires a discounting object. This requirement can be justified from a risk-neutralvaluation perspective, which is the overarching goal of the whole DX analytics library:In%5B38%5D:# required but not needed for the classme_srd.add_curve('discount_curve',constant_short_rate('r',0.0))In%5B39%5D:fromsquare_root_diffusionimportsquare_root_diffusionIn%5B40%5D:srd%3Dsquare_root_diffusion('srd',me_srd)As before, we get simulation paths, given the market_environmentobject as input, bycalling the get_instrument_values method:In%5B41%5D:srd_paths%3Dsrd.get_instrument_values()%5B:,:10%5D","Figure 16-3 illustrates the mean-reverting characteristic by showing how the singlesimulated paths on average revert to the long-term mean theta (dashed line):In%5B42%5D:plt.figure(figsize%3D(8,4))plt.plot(srd.time_grid,srd.get_instrument_values()%5B:,:10%5D)plt.axhline(me_srd.get_constant('theta'),color%3D'r',ls%3D'--',lw%3D2.0)plt.grid(True)plt.xticks(rotation%3D30)Figure 16-3. Simulated paths from square-root diffusion simulation class (dashed line%3D long-term mean theta)","This chapter develops all the tools and classes needed for the simulation of the threestochastic processes of interest: geometric Brownian motions, jump diffusions, andsquare-root diffusions. The chapter presents a function to conveniently generate stan%E2%80%90dard normally distributed random numbers. It then proceeds by introducing a genericmodel simulation class. Based on this foundation, the chapter introduces three speci%E2%80%90alized simulation classes and presents use cases for these classes.To simplify future imports, we can again use a wrapper module called dx_simulation.py, as presented in Example 16-6.Example 16-6. Wrapper module for simulation components## DX Library Simulation# dx_simulation.py#importnumpyasnpimportpandasaspdfromdx_frameimport*fromsn_random_numbersimportsn_random_numbers","fromsimulation_classimportsimulation_classfromgeometric_brownian_motionimportgeometric_brownian_motionfromjump_diffusionimportjump_diffusionfromsquare_root_diffusionimportsquare_root_diffusionAs with the first wrapper module, dx_frame.py, the benefit is that a single importstatement makes available all simulation components in a single step:fromdx_simulationimport*Since dx_simulation.py also imports everything from dx_frame.py, this single importin fact exposes all functionalitydeveloped so far. The same holds true for the enhancedinit file in the dx directory, as shown in Example 16-7.Example 16-7. Enhanced Python packaging file## DX Library# packaging file# __init__.py#importnumpyasnpimportpandasaspdimportdatetimeasdt# framefromget_year_deltasimportget_year_deltasfromconstant_short_rateimportconstant_short_ratefrommarket_environmentimportmarket_environment# simulationfromsn_random_numbersimportsn_random_numbersfromsimulation_classimportsimulation_classfromgeometric_brownian_motionimportgeometric_brownian_motionfromjump_diffusionimportjump_diffusionfromsquare_root_diffusionimportsquare_root_diffusion","Useful references in book form for the topics covered in this chapter are:%E2%80%A2Glasserman, Paul (2004): Monte Carlo Methods in Financial Engineering. Springer,New York.%E2%80%A2Hilpisch, Yves (2015): Derivatives Analytics with Python. Wiley Finance, Chiches%E2%80%90ter, England. http://derivatives-analytics-with-python.com.Original papers cited in this chapter are:","%E2%80%A2Black, Fischer and Myron Scholes (1973): %E2%80%9CThe Pricing of Options and CorporateLiabilities.%E2%80%9D Journal of Political Economy, Vol. 81, No. 3, pp. 638%E2%80%93659.%E2%80%A2Cox, John, Jonathan Ingersoll, and Stephen Ross (1985): %E2%80%9CA Theory of the TermStructure of Interest Rates.%E2%80%9D Econometrica, Vol. 53, No. 2, pp. 385%E2%80%93407.%E2%80%A2Merton, Robert (1973): %E2%80%9CTheory of Rational Option Pricing.%E2%80%9D Bell Journal of Eco%E2%80%90nomics and Management Science, Vol. 4, pp. 141%E2%80%93183.%E2%80%A2Merton, Robert (1976): %E2%80%9COption Pricing When the Underlying Stock Returns AreDiscontinuous.%E2%80%9D Journal of Financial Economics, Vol. 3, No. 3, pp. 125%E2%80%93144.","Derivatives are a huge, complex issue.- Judd GreggOptions and derivatives valuation has long been the domain of so-called rocket scientistson Wall Street-i.e., people with a Ph.D. in physics or a similarly demanding disciplinewhen it comes to the mathematics involved. However, the application of the models bythe means of numerical methods like Monte Carlo simulation is generally a little lessinvolved than the theoretical models themselves.This is particularly true for the valuation of options and derivatives with Europeanexercise-i.e., where exercise is only possible at a certain, predetermined date. It is a bitless true for options and derivatives with American exercise, where exercise is allowedat any point over a prespecified period of time. This chapter introduces and uses theLeast-Squares Monte Carlo (LSM) algorithm, which has become a benchmark algorithmwhen it comes to American options valuation based on Monte Carlo simulation.The current chapter is similar in structure to Chapter 16 in that it first introduces ageneric valuation class and then provides two specialized valuation classes, one for Eu%E2%80%90ropean exercise and another one for American exercise.The generic valuation class contains methods to numerically estimate the most impor%E2%80%90tant Greeks of an option: the Delta and the Vega. Therefore, the valuation classes areimportant not only for valuation purposes, but also for risk management purposes.","As with the generic simulation class, we instantiate an object of the valuation class byproviding only a few inputs (in this case, four):","nameA string object as a name for the model simulation objectunderlyingAn instance of a simulation class representing the underlyingmar_envAn instance of the market_environment classpayoff_funcA Python string containing the payoff function for the option/derivativeThe generic class has three methods:updateThis method updates selected valuation parameters (attributes).deltaThis method calculates a numerical value for the Delta of an option/derivative.vegaThis method calculates the Vega of an option/derivative.Equipped with the background knowledge from the previous chapters about the DXlibrary, the generic valuation class as presented in Example 17-1 should be almost self-explanatory%3B where appropriate, inline comments are also provided. We again presentthe class in its entirety first and highlight selected topics immediately afterward and inthe subsequent sections.Example 17-1. Generic valuation class## DX Library Valuation# valuation_class.py#classvaluation_class(object):''' Basic class for single-factor valuation.    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    name : string        name of the object    underlying :        instance of simulation class    mar_env : instance of market_environment        market environment data for valuation    payoff_func : string        derivatives payoff in Python syntax        Example: 'np.maximum(maturity_value - 100, 0)'        where maturity_value is the NumPy vector with","        respective values of the underlying        Example: 'np.maximum(instrument_values - 100, 0)'        where instrument_values is the NumPy matrix with        values of the underlying over the whole time/path grid    Methods    %3D%3D%3D%3D%3D%3D%3D    update:        updates selected valuation parameters    delta :        returns the Delta of the derivative    vega :        returns the Vega of the derivative    '''def__init__(self,name,underlying,mar_env,payoff_func%3D''):try:self.name%3Dnameself.pricing_date%3Dmar_env.pricing_datetry:self.strike%3Dmar_env.get_constant('strike')# strike is optionalexcept:passself.maturity%3Dmar_env.get_constant('maturity')self.currency%3Dmar_env.get_constant('currency')# simulation parameters and discount curve from simulation objectself.frequency%3Dunderlying.frequencyself.paths%3Dunderlying.pathsself.discount_curve%3Dunderlying.discount_curveself.payoff_func%3Dpayoff_funcself.underlying%3Dunderlying# provide pricing_date and maturity to underlyingself.underlying.special_dates.extend(%5Bself.pricing_date,self.maturity%5D)except:print%22Error parsing market environment.%22defupdate(self,initial_value%3DNone,volatility%3DNone,strike%3DNone,maturity%3DNone):ifinitial_valueisnotNone:self.underlying.update(initial_value%3Dinitial_value)ifvolatilityisnotNone:self.underlying.update(volatility%3Dvolatility)ifstrikeisnotNone:self.strike%3DstrikeifmaturityisnotNone:self.maturity%3Dmaturity# add new maturity date if not in time_gridifnotmaturityinself.underlying.time_grid:self.underlying.special_dates.append(maturity)self.underlying.instrument_values%3DNone","defdelta(self,interval%3DNone,accuracy%3D4):ifintervalisNone:interval%3Dself.underlying.initial_value/50.# forward-difference approximation# calculate left value for numerical Deltavalue_left%3Dself.present_value(fixed_seed%3DTrue)# numerical underlying value for right valueinitial_del%3Dself.underlying.initial_value%2Bintervalself.underlying.update(initial_value%3Dinitial_del)# calculate right value for numerical deltavalue_right%3Dself.present_value(fixed_seed%3DTrue)# reset the initial_value of the simulation objectself.underlying.update(initial_value%3Dinitial_del-interval)delta%3D(value_right-value_left)/interval# correct for potential numerical errorsifdelta%3C-1.0:return-1.0elifdelta%3E1.0:return1.0else:returnround(delta,accuracy)defvega(self,interval%3D0.01,accuracy%3D4):ifinterval%3Cself.underlying.volatility/50.:interval%3Dself.underlying.volatility/50.# forward-difference approximation# calculate the left value for numerical Vegavalue_left%3Dself.present_value(fixed_seed%3DTrue)# numerical volatility value for right valuevola_del%3Dself.underlying.volatility%2Binterval# update the simulation objectself.underlying.update(volatility%3Dvola_del)# calculate the right value for numerical Vegavalue_right%3Dself.present_value(fixed_seed%3DTrue)# reset volatility value of simulation objectself.underlying.update(volatility%3Dvola_del-interval)vega%3D(value_right-value_left)/intervalreturnround(vega,accuracy)One topic covered by the generic valuation_class class is the estimation of Greeks.This is something we should take a closer look at. To this end, consider that we have acontinuously differentiable function VS0,%CF%830 available that represents the present valueof an option. The Delta of the option is then defined as the first partial derivative withrespect to the current value of the underlying S0%3B i.e., %CE%94%3D%E2%88%82V%C2%B7%E2%88%82S0.Suppose now that we have from Monte Carlo valuation (see Chapter 10and subsequentsections in this chapter) a numerical Monte Carlo estimator VS0,%CF%830for the optionvalue. A numerical approximation for the Delta of the option is then given in","1.For details on how to estimate Greeks numerically by Monte Carlo simulation, refer to Chapter 7 of Glass%E2%80%90erman (2004). We only use forward-difference schemes here since this leads to only one additional simulationand revaluation of the option. For example, a central-difference approximation would lead to twooptionrevaluations and therefore a higher computational burden.Equation 17-1.1 This is what the delta method of the generic valuation class implements.The method assumes the existence of a present_valuemethod that returns the MonteCarlo estimator given a certain set of parameter values.Equation 17-1. Numerical Delta of an option%CE%94%3DVS0%2B%CE%94S,%CF%830%E2%88%92VS0,%CF%830%CE%94S,%CE%94S%3E0Similarly, the Vega of the instrument is defined as the first partial derivative of thepresent value with respect to the current (instantaneous) volatility %ED%9C%8E0, i.e., %ED%90%80%3D%E2%88%82V%C2%B7%E2%88%82%CF%830.Again assuming the existence of a Monte Carlo estimator for the value of the option,Equation 17-2provides a numerical approximation for the Vega. This is what the vegamethod of the valuation_class class implements.Equation 17-2. Numerical Vega of an option%ED%90%80%3DVS0,%CF%830%2B%CE%94%CF%83%E2%88%92VS0,%CF%830%CE%94%CF%83,%CE%94%CF%83%3E0Note that the discussion of Delta and Vega is based only on the existence of either adifferentiable function or a Monte Carlo estimator for the present value of an option.This is the very reason why we can define methods to numerically estimate these quan%E2%80%90tities without knowledge of the exact definition and numerical implementation of theMonte Carlo estimator.","The first case to which we want to specialize the generic valuation class is Europeanexercise. To this end, consider the following simplified recipe to generate a Monte Carloestimator for an option value:1.Simulate the relevant underlying risk factor S under the risk-neutral measure Itimesto come up with as many simulated values of the underlying at the maturity of theoption T-i.e., STi,i%E2%88%881,2,...,I","2.Calculate the payoff hT of the option at maturity for every simulated value of theunderlying-i.e., hTSTi,i%E2%88%881,2,...,I3.Derive the Monte Carlo estimator for the option's present value asV0%E2%89%A1e%E2%88%92rT1I%E2%88%91i%3D1IhTSTi","Example 17-2 shows the class implementing the present_value method based on thisrecipe. In addition, it contains the method generate_payoffto generate the simulatedpaths and the payoff of the option given the simulated paths. This, of course, builds thevery basis for the Monte Carlo estimator.Example 17-2. Valuation class for European exercise## DX Library Valuation# valuation_mcs_european.py#importnumpyasnpfromvaluation_classimportvaluation_classclassvaluation_mcs_european(valuation_class):''' Class to value European options with arbitrary payoff    by single-factor Monte Carlo simulation.    Methods    %3D%3D%3D%3D%3D%3D%3D    generate_payoff :        returns payoffs given the paths and the payoff function    present_value :        returns present value (Monte Carlo estimator)    '''defgenerate_payoff(self,fixed_seed%3DFalse):'''        Parameters        %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D        fixed_seed : Boolean            use same/fixed seed for valuation        '''try:# strike defined%3Fstrike%3Dself.strikeexcept:passpaths%3Dself.underlying.get_instrument_values(fixed_seed%3Dfixed_seed)time_grid%3Dself.underlying.time_gridtry:","time_index%3Dnp.where(time_grid%3D%3Dself.maturity)%5B0%5Dtime_index%3Dint(time_index)except:print%22Maturity date not in time grid of underlying.%22maturity_value%3Dpaths%5Btime_index%5D# average value over whole pathmean_value%3Dnp.mean(paths%5B:time_index%5D,axis%3D1)# maximum value over whole pathmax_value%3Dnp.amax(paths%5B:time_index%5D,axis%3D1)%5B-1%5D# minimum value over whole pathmin_value%3Dnp.amin(paths%5B:time_index%5D,axis%3D1)%5B-1%5Dtry:payoff%3Deval(self.payoff_func)returnpayoffexcept:print%22Error evaluating payoff function.%22defpresent_value(self,accuracy%3D6,fixed_seed%3DFalse,full%3DFalse):'''        Parameters        %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D        accuracy : int            number of decimals in returned result        fixed_seed : Boolean            use same/fixed seed for valuation        full : Boolean            return also full 1d array of present values        '''cash_flow%3Dself.generate_payoff(fixed_seed%3Dfixed_seed)discount_factor%3Dself.discount_curve.get_discount_factors((self.pricing_date,self.maturity))%5B0,1%5Dresult%3Ddiscount_factor*np.sum(cash_flow)/len(cash_flow)iffull:returnround(result,accuracy),discount_factor*cash_flowelse:returnround(result,accuracy)The generate_payoffmethod provides some special objects to be used for the defini%E2%80%90tion of the payoff of the option:%E2%80%A2strike is the strike of the option.%E2%80%A2maturity_value represents the 1D ndarray object with the simulated values of theunderlying at maturity of the option.%E2%80%A2mean_value is the average of the underlying over a whole path from today untilmaturity.%E2%80%A2max_value is the maximum value of the underlying over a whole path.%E2%80%A2min_value gives the minimum value of the underlying over a whole path.","The last three especially allow for the efficient handling of options with Asian (i.e.,lookback) features.","The application of the valuation class valuation_mcs_european is best illustrated by aspecific use case. However, before a valuation class can be instantiated, we need a sim%E2%80%90ulation object-i.e., an underlying for the option to be valued. From Chapter 16, we usethe geometric_brownian_motion class to model the underlying. We also use the ex%E2%80%90ample parameterization of the respective use case there:In%5B1%5D:fromdximport*In%5B2%5D:me_gbm%3Dmarket_environment('me_gbm',dt.datetime(2015,1,1))In%5B3%5D:me_gbm.add_constant('initial_value',36.)me_gbm.add_constant('volatility',0.2)me_gbm.add_constant('final_date',dt.datetime(2015,12,31))me_gbm.add_constant('currency','EUR')me_gbm.add_constant('frequency','M')me_gbm.add_constant('paths',10000)In%5B4%5D:csr%3Dconstant_short_rate('csr',0.06)In%5B5%5D:me_gbm.add_curve('discount_curve',csr)In%5B6%5D:gbm%3Dgeometric_brownian_motion('gbm',me_gbm)In addition to a simulation object, we need to provide a market environment for theoption itself. It has to contain at least a maturity and a currency. Optionally, we canprovide a strike:In%5B7%5D:me_call%3Dmarket_environment('me_call',me_gbm.pricing_date)In%5B8%5D:me_call.add_constant('strike',40.)me_call.add_constant('maturity',dt.datetime(2015,12,31))me_call.add_constant('currency','EUR')A central element, of course, is the payoff function, provided here as a stringobjectcontaining Python code that the eval function can evaluate. We want to define a Eu%E2%80%90ropean calloption. Such an option has a payoff of hT %3D max(ST %E2%80%93 K,0), with ST being thevalue of the underlying at maturity and K being the strike price of the option. In Pythonand NumPy-i.e., with vectorized storage of all simulated values-this takes on the fol%E2%80%90lowing form:In%5B9%5D:payoff_func%3D'np.maximum(maturity_value - strike, 0)'We can now put all the ingredients together to instantiate the valuation_mcs_europeanclass:In%5B10%5D:fromvaluation_mcs_europeanimportvaluation_mcs_european","In%5B11%5D:eur_call%3Dvaluation_mcs_european('eur_call',underlying%3Dgbm,mar_env%3Dme_call,payoff_func%3Dpayoff_func)With this valuation object available, all quantities of interest are only one method callaway. Let us start with the present value of the option:In%5B12%5D:%25timeeur_call.present_value()Out%5B12%5D: CPU times: user 41.7 ms, sys: 11 ms, total: 52.7 ms         Wall time: 44.6 msOut%5B12%5D: 2.180511The Delta of the option is, as expected for a European call option, positive-i.e., thepresent value of the option increases with increasing initial value of the underlying:In%5B13%5D:%25timeeur_call.delta()Out%5B13%5D: CPU times: user 10.9 ms, sys: 1.09 ms, total: 12 ms         Wall time: 11.1 ms         0.4596The Vega is calculated similarly. It shows the increase in the present value of the optiongiven an increase in the initial volatility of 1%25%3B e.g., from 24%25 to 25%25. The Vega is positivefor both European put and call options:In%5B14%5D:%25timeeur_call.vega()Out%5B14%5D: CPU times: user 15.2 ms, sys: 1.34 ms, total: 16.5 ms         Wall time: 15.6 ms         14.2782Once we have the valuation object, a more comprehensive analysis of the present valueand the Greeks is easily implemented. The following code calculates the present value,Delta, and Vega for initial values of the underlying ranging from 34 to 46 EUR:In%5B15%5D:%25%25times_list%3Dnp.arange(34.,46.1,2.)p_list%3D%5B%5D%3Bd_list%3D%5B%5D%3Bv_list%3D%5B%5Dforsins_list:eur_call.update(initial_value%3Ds)p_list.append(eur_call.present_value(fixed_seed%3DTrue))d_list.append(eur_call.delta())v_list.append(eur_call.vega())Out%5B15%5D: CPU times: user 239 ms, sys: 8.18 ms, total: 248 ms         Wall time: 248 msEquipped with all these values, we can graphically inspect the results. To this end, weuse a helper function as shown in Example 17-3.","Example 17-3. Helper function to plot options statistics## DX Library Valuation# plot_option_stats.py#importmatplotlib.pyplotaspltdefplot_option_stats(s_list,p_list,d_list,v_list):''' Plots option prices, Deltas, and Vegas for a set of    different initial values of the underlying.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    s_list : array or list        set of initial values of the underlying    p_list : array or list        present values    d_list : array or list        results for Deltas    v_list : array or list        results for Vegas    '''plt.figure(figsize%3D(9,7))sub1%3Dplt.subplot(311)plt.plot(s_list,p_list,'ro',label%3D'present value')plt.plot(s_list,p_list,'b')plt.grid(True)%3Bplt.legend(loc%3D0)plt.setp(sub1.get_xticklabels(),visible%3DFalse)sub2%3Dplt.subplot(312)plt.plot(s_list,d_list,'go',label%3D'Delta')plt.plot(s_list,d_list,'b')plt.grid(True)%3Bplt.legend(loc%3D0)plt.ylim(min(d_list)-0.1,max(d_list)%2B0.1)plt.setp(sub2.get_xticklabels(),visible%3DFalse)sub3%3Dplt.subplot(313)plt.plot(s_list,v_list,'yo',label%3D'Vega')plt.plot(s_list,v_list,'b')plt.xlabel('initial value of underlying')plt.grid(True)%3Bplt.legend(loc%3D0)Importing this function and providing the valuation results to it generates a picture likethat shown in Figure 17-1:In%5B16%5D:fromplot_option_statsimportplot_option_stats%25matplotlibinlineIn%5B17%5D:plot_option_stats(s_list,p_list,d_list,v_list)","Figure 17-1. Present value, Delta, and Vega estimates for European call optionThis illustrates that working with the DXlibrary-although heavy numerics areinvolved-boils down to an approach that is comparable to having a closed-form optionpricing formula available. However, this approach does not only apply to such simplepayoffs as the one considered so far. With exactly the same approach, we can handlemuch more complex payoffs. To this end, consider the following payoff, a mixture of aregular and an Asian payoff:In%5B18%5D:payoff_func%3D'np.maximum(0.33 * (maturity_value %2B max_value) - 40, 0)'# payoff dependent on both the simulated maturity value# and the maximum valueEverything else shall remain the same:In%5B19%5D:eur_as_call%3Dvaluation_mcs_european('eur_as_call',underlying%3Dgbm,mar_env%3Dme_call,payoff_func%3Dpayoff_func)All statistics, of course, change in this case:In%5B20%5D:%25%25times_list%3Dnp.arange(34.,46.1,2.)p_list%3D%5B%5D%3Bd_list%3D%5B%5D%3Bv_list%3D%5B%5Dforsins_list:eur_as_call.update(s)p_list.append(eur_as_call.present_value(fixed_seed%3DTrue))d_list.append(eur_as_call.delta())v_list.append(eur_as_call.vega())","2.American exercise refers to a situation where exercise is possible at every instant of time over a fixed timeinterval (at least during trading hours). Bermudan exercise generally refers to a situation where there aremultiple, discrete exercise dates. In numerical applications, American exercise is approximated by Bermudanexercise, and maybe letting the number of exercise dates go to infinity in the limit.Out%5B20%5D: CPU times: user 286 ms, sys: 14.5 ms, total: 300 ms         Wall time: 303 msFigure 17-2 shows that Delta becomes 1 when the initial value of the underlying reachesthe strike price of 40 in this case. Every (marginal) increase of the initial value of theunderlying leads to the same (marginal) increase in the option's value from this partic%E2%80%90ular point on:In%5B21%5D:plot_option_stats(s_list,p_list,d_list,v_list)Figure 17-2. Present value, Delta, and Vega estimates for European%E2%80%93Asian call option","The valuation of options with American exercise-or Bermudan exercise, to this end2-is much more involved than with European exercise. Therefore, we have to introducea bit more valuation theory first before proceeding to the valuation class.","3.That is why their algorithm is generally abbreviated as LSM, for Least-Squares Monte Carlo.4.Kohler (2010) provides a concise overview of the theory of American option valuation in general and the useof regression-based methods in particular.","Although Cox, Ross, and Rubinstein (1979) presented with their binomial model asimple numerical method to value European and American options in the same frame%E2%80%90work, only with the Longstaff-Schwartz (2001) model was the valuation of Americanoptions by Monte Carlo simulation (MCS) satisfactorily solved. The major problem isthat MCS per se is a forward-moving algorithm, while the valuation of American optionsis generally accomplished by backward induction, estimating the continuation value ofthe American option starting at maturity and working back to the present.The major insight of the Longstaff-Schwartz (2001) model is to use an ordinary least-squares regression3 to estimate the continuation value based on the cross section of allavailable simulated values-taking into account, per path:%E2%80%A2The simulated value of the underlying(s)%E2%80%A2The inner value of the option%E2%80%A2The actual continuation value given the specific pathIn discrete time, the value of a Bermudan option (and in the limit of an American option)is given by the optimal stopping problem, as presented in Equation 17-3for a finite setof points in time 0 %3C t1 %3C t2 %3C %E2%80%A6 %3C T.4Equation 17-3. Optimal stopping problem in discrete time for Bermudan optionV0%3Dsup%CF%84%E2%88%880,t1,t2,...,Te%E2%88%92r%CF%84%ED%90%800Qh%CF%84S%CF%84Equation 17-4 presents the continuation value of the American option at date0 %E2%89%A4tm %3C T. It is just the risk-neutral expectation at date tm%2B1 under the martingale measureof the value of the American option Vtm%2B1 at the subsequent date.Equation 17-4. Continuation value for the American option C tms%3De%E2%88%92rtm%2B1%E2%88%92tm%ED%90%80tmQVtm%2B1Stm%2B1Stm%3Ds","The value of the American option Vtm at date tm can be shown to equal the formula inEquation 17-5-i.e., the maximum of the payoff of immediate exercise (inner value)and the expected payoff of not exercising (continuation value).Equation 17-5. Value of American option at any given dateVtm%3Dmaxhtms, C tmsIn Equation 17-5, the inner value is of course easily calculated. The continuation valueis what makes it a bit trickier. The Longstaff-Schwartz (2001) model approximates thisvalue by a regression, as presented in Equation 17-6. There, i stands for the currentsimulated path, D is the number of basis functions for the regression used, %ED%9B%BC* are theoptimal regression parameters, and bd is the regression function numbered d.Equation 17-6. Regression-based approximation of continuation value C tm,i%3D%E2%88%91d%3D1D%CE%B1d,tm*%C2%B7bdStm,iThe optimal regression parameters are the result of the solution of the least-squaresregression problem presented in Equation 17-7. Here, Ytm,i%E2%89%A1e%E2%88%92rtm%2B1%E2%88%92tmVtm%2B1,iis the ac%E2%80%90tual continuation value at date tm for path i (and not a regressed/estimated one).Equation 17-7. Ordinary least-squares regressionmin%CE%B11,tm,...,%CE%B1D,tm1I%E2%88%91i%3D1IYtm,i%E2%88%92%E2%88%91d%3D1D%CE%B1d,tm%C2%B7bdStm,i2This completes the basic (mathematical) tool set to value an American option by MCS.","Example 17-4 presents the class for the valuation of options and derivatives with Amer%E2%80%90ican exercise. There is one noteworthy step in the implementation of the LSM algorithmin the present_value method (which is also commented on inline): the optimal decisionstep. Here, it is important that, based on the decision that is made, the LSM algorithm","5.See also Chapter 6 of Hilpisch (2015).takes either the inner value or the actualcontinuation value-and not the estimatedcontinuation value.5Example 17-4. Valuation class for American exercise## DX Library Valuation# valuation_mcs_american.py#importnumpyasnpfromvaluation_classimportvaluation_classclassvaluation_mcs_american(valuation_class):''' Class to value American options with arbitrary payoff    by single-factor Monte Carlo simulation.    Methods    %3D%3D%3D%3D%3D%3D%3D    generate_payoff :        returns payoffs given the paths and the payoff function    present_value :        returns present value (LSM Monte Carlo estimator)        according to Longstaff-Schwartz (2001)    '''defgenerate_payoff(self,fixed_seed%3DFalse):'''        Parameters        %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D        fixed_seed :            use same/fixed seed for valuation        '''try:strike%3Dself.strikeexcept:passpaths%3Dself.underlying.get_instrument_values(fixed_seed%3Dfixed_seed)time_grid%3Dself.underlying.time_gridtry:time_index_start%3Dint(np.where(time_grid%3D%3Dself.pricing_date)%5B0%5D)time_index_end%3Dint(np.where(time_grid%3D%3Dself.maturity)%5B0%5D)except:print%22Maturity date not in time grid of underlying.%22instrument_values%3Dpaths%5Btime_index_start:time_index_end%2B1%5Dtry:payoff%3Deval(self.payoff_func)returninstrument_values,payoff,time_index_start,time_index_endexcept:","print%22Error evaluating payoff function.%22defpresent_value(self,accuracy%3D6,fixed_seed%3DFalse,bf%3D5,full%3DFalse):'''        Parameters        %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D        accuracy : int            number of decimals in returned result        fixed_seed : boolean            use same/fixed seed for valuation        bf : int            number of basis functions for regression        full : Boolean            return also full 1d array of present values        '''instrument_values,inner_values,time_index_start,time_index_end%3D %5Cself.generate_payoff(fixed_seed%3Dfixed_seed)time_list%3Dself.underlying.time_grid%5Btime_index_start:time_index_end%2B1%5Ddiscount_factors%3Dself.discount_curve.get_discount_factors(time_list,dtobjects%3DTrue)V%3Dinner_values%5B-1%5Dfortinrange(len(time_list)-2,0,-1):# derive relevant discount factor for given time intervaldf%3Ddiscount_factors%5Bt,1%5D/discount_factors%5Bt%2B1,1%5D# regression steprg%3Dnp.polyfit(instrument_values%5Bt%5D,V*df,bf)# calculation of continuation values per path C %3Dnp.polyval(rg,instrument_values%5Bt%5D)# optimal decision step:# if condition is satisfied (inner value %3E regressed cont. value)# then take inner value%3B take actual cont. value otherwiseV%3Dnp.where(inner_values%5Bt%5D%3E C ,inner_values%5Bt%5D,V*df)df%3Ddiscount_factors%5B0,1%5D/discount_factors%5B1,1%5Dresult%3Ddf*np.sum(V)/len(V)iffull:returnround(result,accuracy),df*Velse:returnround(result,accuracy)","As has become by now the means of choice, a use case shall illustrate how to work withthe valuation_mcs_americanclass. The use case replicates all American option valuesas presented in Table 1 of the seminal paper by Longstaff and Schwartz (2001). Theunderlying is the same as before, a geometric_brownian_motionobject. The startingparameterization for the underlying is as follows:In%5B22%5D:fromdx_simulationimport*In%5B23%5D:me_gbm%3Dmarket_environment('me_gbm',dt.datetime(2015,1,1))","6.The main reason is that the %E2%80%9Coptimal exercise policy%E2%80%9D based on the regression estimates of the continuationvalue is only %E2%80%9Csuboptimal.%E2%80%9DIn%5B24%5D:me_gbm.add_constant('initial_value',36.)me_gbm.add_constant('volatility',0.2)me_gbm.add_constant('final_date',dt.datetime(2016,12,31))me_gbm.add_constant('currency','EUR')me_gbm.add_constant('frequency','W')# weekly frequencyme_gbm.add_constant('paths',50000)In%5B25%5D:csr%3Dconstant_short_rate('csr',0.06)In%5B26%5D:me_gbm.add_curve('discount_curve',csr)In%5B27%5D:gbm%3Dgeometric_brownian_motion('gbm',me_gbm)The option type is an American put option with payoff:In%5B28%5D:payoff_func%3D'np.maximum(strike - instrument_values, 0)'The first option in Table 1 of the paper has a maturity of one year, and the strike priceis 40 throughout:In%5B29%5D:me_am_put%3Dmarket_environment('me_am_put',dt.datetime(2015,1,1))In%5B30%5D:me_am_put.add_constant('maturity',dt.datetime(2015,12,31))me_am_put.add_constant('strike',40.)me_am_put.add_constant('currency','EUR')The next step is to instantiate the valuation object based on the numerical assumptions:In%5B31%5D:fromvaluation_mcs_americanimportvaluation_mcs_americanIn%5B32%5D:am_put%3Dvaluation_mcs_american('am_put',underlying%3Dgbm,mar_env%3Dme_am_put,payoff_func%3Dpayoff_func)The valuation of the American put option takes much longer than the same task for theEuropean options. Not only have we increased the number of paths and the frequencyfor the valuation, but the algorithm is much more computationally demanding due tothe backward induction and the regression per induction step. Our numerical value ispretty close to the correct one reported in the original paper of 4.478:In%5B33%5D:%25timeam_put.present_value(fixed_seed%3DTrue,bf%3D5)Out%5B33%5D: CPU times: user 1.36 s, sys: 239 ms, total: 1.6 s         Wall time: 1.6 s         4.470627Due to the very construction of the LSM Monte Carlo estimator, it represents a lowerboundof the mathematically correct American option value.6Therefore, we would ex%E2%80%90pect the numerical estimate to lie under the true value in any numerically realistic case.","7.Cf. Chapter 6 in Hilpisch (2015) for a dual algorithm leading to an upper bound and a Pythonimplementationthereof.Alternative dual estimators can provide upper bounds as well.7Taken together, two suchdifferent estimators then define an interval for the true American option value.The main stated goal of this use case is to replicate all American option values of Table1 in the original paper. To this end, we only need to combine the valuation object witha nested loop. During the innermost loop, the valuation object has to be updated ac%E2%80%90cording to the then-current parameterization:In%5B34%5D:%25%25timels_table%3D%5B%5Dforinitial_valuein(36.,38.,40.,42.,44.):forvolatilityin(0.2,0.4):formaturityin(dt.datetime(2015,12,31),dt.datetime(2016,12,31)):am_put.update(initial_value%3Dinitial_value,volatility%3Dvolatility,maturity%3Dmaturity)ls_table.append(%5Binitial_value,volatility,maturity,am_put.present_value(bf%3D5)%5D)Out%5B34%5D: CPU times: user 31.1 s, sys: 3.22 s, total: 34.3 s         Wall time: 33.9 sFollowing is our simplified version of Table 1 in the paper by Longstaff and Schwartz(2001). Overall, our numerical values come pretty close to those reported in the paper,where some different parameters have been used (they use, for example, double thenumber of paths):In%5B35%5D:print%22S0  %7C Vola %7C T %7C Value%22print22*%22-%22forrinls_table:print%22%25d  %7C %253.1f  %7C %25d %7C %255.3f%22%25 %5C(r%5B0%5D,r%5B1%5D,r%5B2%5D.year-2014,r%5B3%5D)Out%5B35%5D: S0  %7C Vola %7C T %7C Value         ----------------------         36  %7C 0.2  %7C 1 %7C 4.444         36  %7C 0.2  %7C 2 %7C 4.769         36  %7C 0.4  %7C 1 %7C 7.000         36  %7C 0.4  %7C 2 %7C 8.378         38  %7C 0.2  %7C 1 %7C 3.210         38  %7C 0.2  %7C 2 %7C 3.645         38  %7C 0.4  %7C 1 %7C 6.066         38  %7C 0.4  %7C 2 %7C 7.535         40  %7C 0.2  %7C 1 %7C 2.267         40  %7C 0.2  %7C 2 %7C 2.778","         40  %7C 0.4  %7C 1 %7C 5.203         40  %7C 0.4  %7C 2 %7C 6.753         42  %7C 0.2  %7C 1 %7C 1.554         42  %7C 0.2  %7C 2 %7C 2.099         42  %7C 0.4  %7C 1 %7C 4.459         42  %7C 0.4  %7C 2 %7C 6.046         44  %7C 0.2  %7C 1 %7C 1.056         44  %7C 0.2  %7C 2 %7C 1.618         44  %7C 0.4  %7C 1 %7C 3.846         44  %7C 0.4  %7C 2 %7C 5.494To conclude the use case, note that the estimation of Greeks for American options isformally the same as for European options-a major advantage of our approach overalternative numerical methods (like the binomial model):In%5B36%5D:am_put.update(initial_value%3D36.)am_put.delta()Out%5B36%5D: -0.4655In%5B37%5D:am_put.vega()Out%5B37%5D: 17.3411","This chapter is about the numerical valuation of both European and American optionsbased on Monte Carlo simulation. The chapter introduces a generic valuation class,called valuation_class. This class provides methods, for example, to estimate the mostimportant Greeks (Delta, Vega) for both types of options, independent of the simulationobject (risk factor/stochastic process) used for the valuation.Based on the generic valuation class, the chapter presents two specialized classes, valuation_mcs_european and valuation_mcs_american. The class for the valuation of Eu%E2%80%90ropean options is mainly a straightforward implementation of the risk-neutral valuationapproach presented in Chapter 15in combination with the numerical estimation of anexpectation term (i.e., an integral by Monte Carlo simulation, as discussed in Chapter 9).The class for the valuation of American options needs a certain kind of regression-basedvaluation algorithm. This is due to the fact that for American options an optimal exercisepolicy has to be derived for a valuation. This is theoretically and numerically a bit moreinvolved. However, the respective present_value method of the class is still concise.The approach taken with the DX derivatives analytics library proves to be beneficial.Without too much effort we are able to value a pretty large class of options with thefollowing features:%E2%80%A2Single risk factor options%E2%80%A2European or American exercise","%E2%80%A2Arbitrary payoffIn addition, we can estimate the most important Greeks for this class of options. Tosimplify future imports, we will again use a wrapper module, this time called dx_valuation.py, as presented in Example 17-5.Example 17-5. Wrapper module for all components of the library including valuationclasses## DX Library Valuation# dx_valuation.py#importnumpyasnpimportpandasaspdfromdx_simulationimport*fromvaluation_classimportvaluation_classfromvaluation_mcs_europeanimportvaluation_mcs_europeanfromvaluation_mcs_americanimportvaluation_mcs_americanAgain, let us enhance the initfile in the dxdirectory (see Example 17-6) to stay con%E2%80%90sistent here.Example 17-6. Enhanced Python packaging file## DX Library# packaging file# __init__.py#importnumpyasnpimportpandasaspdimportdatetimeasdt# framefromget_year_deltasimportget_year_deltasfromconstant_short_rateimportconstant_short_ratefrommarket_environmentimportmarket_environmentfromplot_option_statsimportplot_option_stats# simulationfromsn_random_numbersimportsn_random_numbersfromsimulation_classimportsimulation_classfromgeometric_brownian_motionimportgeometric_brownian_motionfromjump_diffusionimportjump_diffusionfromsquare_root_diffusionimportsquare_root_diffusion# valuationfromvaluation_classimportvaluation_classfromvaluation_mcs_europeanimportvaluation_mcs_europeanfromvaluation_mcs_americanimportvaluation_mcs_american","References for the topics of this chapter in book form are:%E2%80%A2Glasserman, Paul (2004): Monte Carlo Methods in Financial Engineering. Springer,New York.%E2%80%A2Hilpisch, Yves (2015): Derivatives Analytics with Python. Wiley Finance, Chiches%E2%80%90ter, England. http://derivatives-analytics-with-python.com.Original papers cited in this chapter:%E2%80%A2Cox, John, Stephen Ross, and Mark Rubinstein (1979): %E2%80%9COption Pricing: A Simpli%E2%80%90fied Approach.%E2%80%9D Journal of Financial Economics, Vol. 7, No. 3, pp. 229%E2%80%93263.%E2%80%A2Kohler, Michael (2010): %E2%80%9CA Review on Regression-Based Monte Carlo Methods forPricing American Options.%E2%80%9D In Luc Devroye et al. (eds.): Recent Developments inApplied Probability and Statistics. Physica-Verlag, Heidelberg, pp. 37%E2%80%9358.%E2%80%A2Longstaff, Francis and Eduardo Schwartz (2001): %E2%80%9CValuing American Options bySimulation: A Simple Least Squares Approach.%E2%80%9D Review of Financial Studies, Vol.14, No. 1, pp. 113%E2%80%93147.","Price is what you pay. Value is what you get.- Warren BuffetBy now, the whole approach for building the DX derivatives analytics library-and itsassociated benefits-should be rather clear. By strictly relying on Monte Carlo simula%E2%80%90tion as the only numerical method, we accomplish an almost complete modularizationof the analytics library:DiscountingThe relevant risk-neutral discounting is taken care of by an instance of the constant_short_rate class.Relevant dataRelevant data, parameters, and other input are stored in (several) instances of themarket_environment class.Simulation objectsRelevant risk factors (underlyings) are modeled as instances of one of three simu%E2%80%90lation classes:%E2%80%A2geometric_brownian_motion%E2%80%A2jump_diffusion%E2%80%A2square_root_diffusionValuation objectsOptions and derivatives to be valued are modeled as instances of one of two valu%E2%80%90ation classes:%E2%80%A2valuation_mcs_european%E2%80%A2valuation_mcs_american","One last step is missing: the valuation of possibly complex portfolios of options andderivatives. To this end, we require the following:NonredundancyEvery risk factor (underlying) is modeled only once and potentially used by multiplevaluation objects.CorrelationsCorrelations between risk factors have to be accounted for.PositionsAn options position, for example, can consist of certain multiples of an optionscontract.However, although we have in principle allowed (and even required) providing a cur%E2%80%90rency for both simulation and valuation objects, we assume that we value portfoliosdenominated in a single currency only. This simplifies the aggregation of values withina portfolio significantly, because we can abstract from exchange rates and currency risks.The chapter presents two new classes: a simple one to model a derivatives position, anda more complex one to model and value a derivatives portfolio.","In principle, a derivatives position is nothing more than a combination of a valuationobject and a quantity for the instrument modeled.","Example 18-1 presents the class to model a derivatives position. It is mainly a containerfor other data and objects. In addition, it provides a get_info method, printing the dataand object information stored in an instance of the class.Example 18-1. A simple class to model a derivatives position## DX Library Portfolio# derivatives_position.py#classderivatives_position(object):''' Class to model a derivatives position.    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    name : string        name of the object    quantity : float","        number of assets/derivatives making up the position    underlying : string        name of asset/risk factor for the derivative    mar_env : instance of market_environment        constants, lists, and curves relevant for valuation_class    otype : string        valuation class to use    payoff_func : string        payoff string for the derivative    Methods    %3D%3D%3D%3D%3D%3D%3D    get_info :        prints information about the derivative position    '''def__init__(self,name,quantity,underlying,mar_env,otype,payoff_func):self.name%3Dnameself.quantity%3Dquantityself.underlying%3Dunderlyingself.mar_env%3Dmar_envself.otype%3Dotypeself.payoff_func%3Dpayoff_funcdefget_info(self):print%22NAME%22printself.name,'%5Cn'print%22QUANTITY%22printself.quantity,'%5Cn'print%22UNDERLYING%22printself.underlying,'%5Cn'print%22MARKET ENVIRONMENT%22print%22%5Cn**Constants**%22forkey,valueinself.mar_env.constants.items():printkey,valueprint%22%5Cn**Lists**%22forkey,valueinself.mar_env.lists.items():printkey,valueprint%22%5Cn**Curves**%22forkeyinself.mar_env.curves.items():printkey,valueprint%22%5CnOPTION TYPE%22printself.otype,'%5Cn'print%22PAYOFF FUNCTION%22printself.payoff_funcTo define a derivatives position we need to provide the following information, which isalmost the same as for the instantiation of a valuation class:","nameName of the position as a string objectquantityQuantity of options/derivativesunderlyingInstance of simulation object as a risk factormar_envInstance of market_environmentotypestring, either %E2%80%9CEuropean%E2%80%9D or %E2%80%9CAmerican%E2%80%9Dpayoff_funcPayoff as a Pythonstring object","The following interactive session illustrates the use of the class. However, we need tofirst define a simulation object-but not in full%3B only the most important, object-specificinformation is needed. Here, we basically stick to the numerical examples from theprevious two chapters:In%5B1%5D:fromdximport*For the definition of the derivatives position, we do not need a %E2%80%9Cfull%E2%80%9D market_environment object. Missing information is provided later (during the portfolio valuation),when the simulation object is instantiated:In%5B2%5D:me_gbm%3Dmarket_environment('me_gbm',dt.datetime(2015,1,1))In%5B3%5D:me_gbm.add_constant('initial_value',36.)me_gbm.add_constant('volatility',0.2)me_gbm.add_constant('currency','EUR')However, for the portfolio valuation, one additional constant is needed-namely, forthe model to be used. This will become clear in the subsequent section:In%5B4%5D:me_gbm.add_constant('model','gbm')With the simulation object available, we can proceed to define a derivatives position asfollows:In%5B5%5D:fromderivatives_positionimportderivatives_positionIn%5B6%5D:me_am_put%3Dmarket_environment('me_am_put',dt.datetime(2015,1,1))In%5B7%5D:me_am_put.add_constant('maturity',dt.datetime(2015,12,31))me_am_put.add_constant('strike',40.)me_am_put.add_constant('currency','EUR')","1.In practice, the approach we choose here is sometimes called global valuation instead of instrument-specificvaluation. Cf. the article by Albanese, Gimonet, and White (2010a) in Risk Magazine.In%5B8%5D:payoff_func%3D'np.maximum(strike - instrument_values, 0)'In%5B9%5D:am_put_pos%3Dderivatives_position(name%3D'am_put_pos',quantity%3D3,underlying%3D'gbm',mar_env%3Dme_am_put,otype%3D'American',payoff_func%3Dpayoff_func)Information about such an object is provided by the get_info method:In%5B10%5D:am_put_pos.get_info()Out%5B10%5D: NAME         am_put_pos         QUANTITY         3         UNDERLYING         gbm         MARKET ENVIRONMENT         **Constants**         strike 40.0         maturity 2015-12-31 00:00:00         currency EUR         **Lists**         **Curves**         OPTION TYPE         American         PAYOFF FUNCTION         np.maximum(strike - instrument_values, 0)","From a portfolio perspective, a %E2%80%9Crelevant market%E2%80%9D is mainly composed of the relevantrisk factors (underlyings) and their correlations, as well as the derivatives and derivativespositions, respectively, to be valued. Theoretically, we are now dealing with a generalmarket model %E2%84%B3 as defined in Chapter 15, and applying the Fundamental Theorem ofAsset Pricing (with its corollaries) to it.1","A somewhat complex Python class implementing a portfolio valuation based on theFundamental Theorem of Asset Pricing-taking into account multiple relevant riskfactors and multiple derivatives positions-is presented as Example 18-2. The class israther comprehensively documented inline, especially during passages that implementfunctionality specific to the purpose at hand.Example 18-2. A class to value a derivatives portfolio## DX Library Portfolio# derivatives_portfolio.py#importnumpyasnpimportpandasaspdfromdx_valuationimport*# models available for risk factor modelingmodels%3D%7B'gbm':geometric_brownian_motion,'jd':jump_diffusion,'srd':square_root_diffusion%7D# allowed exercise typesotypes%3D%7B'European':valuation_mcs_european,'American':valuation_mcs_american%7Dclassderivatives_portfolio(object):''' Class for building portfolios of derivatives positions.    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    name : str        name of the object    positions : dict        dictionary of positions (instances of derivatives_position class)    val_env : market_environment        market environment for the valuation    assets : dict        dictionary of market environments for the assets    correlations : list        correlations between assets    fixed_seed : Boolean        flag for fixed rng seed    Methods    %3D%3D%3D%3D%3D%3D%3D    get_positions :        prints information about the single portfolio positions    get_statistics :","        returns a pandas DataFrame object with portfolio statistics    '''def__init__(self,name,positions,val_env,assets,correlations%3DNone,fixed_seed%3DFalse):self.name%3Dnameself.positions%3Dpositionsself.val_env%3Dval_envself.assets%3Dassetsself.underlyings%3Dset()self.correlations%3Dcorrelationsself.time_grid%3DNoneself.underlying_objects%3D%7B%7Dself.valuation_objects%3D%7B%7Dself.fixed_seed%3Dfixed_seedself.special_dates%3D%5B%5Dforposinself.positions:# determine earliest starting_dateself.val_env.constants%5B'starting_date'%5D%3D %5Cmin(self.val_env.constants%5B'starting_date'%5D,positions%5Bpos%5D.mar_env.pricing_date)# determine latest date of relevanceself.val_env.constants%5B'final_date'%5D%3D %5Cmax(self.val_env.constants%5B'final_date'%5D,positions%5Bpos%5D.mar_env.constants%5B'maturity'%5D)# collect all underlyings# add to set%3B avoids redundancyself.underlyings.add(positions%5Bpos%5D.underlying)# generate general time gridstart%3Dself.val_env.constants%5B'starting_date'%5Dend%3Dself.val_env.constants%5B'final_date'%5Dtime_grid%3Dpd.date_range(start%3Dstart,end%3Dend,freq%3Dself.val_env.constants%5B'frequency'%5D).to_pydatetime()time_grid%3Dlist(time_grid)forposinself.positions:maturity_date%3Dpositions%5Bpos%5D.mar_env.constants%5B'maturity'%5Difmaturity_datenotintime_grid:time_grid.insert(0,maturity_date)self.special_dates.append(maturity_date)ifstartnotintime_grid:time_grid.insert(0,start)ifendnotintime_grid:time_grid.append(end)# delete duplicate entriestime_grid%3Dlist(set(time_grid))# sort dates in time_gridtime_grid.sort()self.time_grid%3Dnp.array(time_grid)self.val_env.add_list('time_grid',self.time_grid)","ifcorrelationsisnotNone:# take care of correlationsul_list%3Dsorted(self.underlyings)correlation_matrix%3Dnp.zeros((len(ul_list),len(ul_list)))np.fill_diagonal(correlation_matrix,1.0)correlation_matrix%3Dpd.DataFrame(correlation_matrix,index%3Dul_list,columns%3Dul_list)fori,j,corrincorrelations:corr%3Dmin(corr,0.999999999999)# fill correlation matrixcorrelation_matrix.loc%5Bi,j%5D%3Dcorrcorrelation_matrix.loc%5Bj,i%5D%3Dcorr# determine Cholesky matrixcholesky_matrix%3Dnp.linalg.cholesky(np.array(correlation_matrix))# dictionary with index positions for the# slice of the random number array to be used by# respective underlyingrn_set%3D%7Basset:ul_list.index(asset)forassetinself.underlyings%7D# random numbers array, to be used by# all underlyings (if correlations exist)random_numbers%3Dsn_random_numbers((len(rn_set),len(self.time_grid),self.val_env.constants%5B'paths'%5D),fixed_seed%3Dself.fixed_seed)# add all to valuation environment that is# to be shared with every underlyingself.val_env.add_list('cholesky_matrix',cholesky_matrix)self.val_env.add_list('random_numbers',random_numbers)self.val_env.add_list('rn_set',rn_set)forassetinself.underlyings:# select market environment of assetmar_env%3Dself.assets%5Basset%5D# add valuation environment to market environmentmar_env.add_environment(val_env)# select right simulation classmodel%3Dmodels%5Bmar_env.constants%5B'model'%5D%5D# instantiate simulation objectifcorrelationsisnotNone:self.underlying_objects%5Basset%5D%3Dmodel(asset,mar_env,corr%3DTrue)else:self.underlying_objects%5Basset%5D%3Dmodel(asset,mar_env,corr%3DFalse)forposinpositions:# select right valuation class (European, American)","val_class%3Dotypes%5Bpositions%5Bpos%5D.otype%5D# pick market environment and add valuation environmentmar_env%3Dpositions%5Bpos%5D.mar_envmar_env.add_environment(self.val_env)# instantiate valuation classself.valuation_objects%5Bpos%5D%3D %5Cval_class(name%3Dpositions%5Bpos%5D.name,mar_env%3Dmar_env,underlying%3Dself.underlying_objects%5Bpositions%5Bpos%5D.underlying%5D,payoff_func%3Dpositions%5Bpos%5D.payoff_func)defget_positions(self):''' Convenience method to get information about        all derivatives positions in a portfolio. '''forposinself.positions:bar%3D'%5Cn'%2B50*'-'printbarself.positions%5Bpos%5D.get_info()printbardefget_statistics(self,fixed_seed%3DFalse):''' Provides portfolio statistics. '''res_list%3D%5B%5D# iterate over all positions in portfolioforpos,valueinself.valuation_objects.items():p%3Dself.positions%5Bpos%5Dpv%3Dvalue.present_value(fixed_seed%3Dfixed_seed)res_list.append(%5Bp.name,p.quantity,# calculate all present values for the single instrumentspv,value.currency,# single instrument value times quantitypv*p.quantity,# calculate Delta of positionvalue.delta()*p.quantity,# calculate Vega of positionvalue.vega()*p.quantity,%5D)# generate a pandas DataFrame object with all resultsres_df%3Dpd.DataFrame(res_list,columns%3D%5B'name','quant.','value','curr.','pos_value','pos_delta','pos_vega'%5D)returnres_df","In terms of the DX analytics library, the modeling capabilities are, on a high level, re%E2%80%90stricted to a combination of a simulation and a valuation class. There are a total of sixpossible combinations:models%3D%7B'gbm':geometric_brownian_motion,'jd':jump_diffusion'srd':square_root_diffusion%7Dotypes%3D%7B'European':valuation_mcs_european,'American':valuation_mcs_american%7DIn the interactive use case that follows, we combine selected elements to define twodifferent derivatives positions that we then combine into a portfolio.We build on the use case for the derivatives_position class with the gbmandam_put_pos objects from the previous section. To illustrate the use of the derivatives_portfolioclass, let us define both an additional underlying and an additionaloptions position. First, a jump_diffusion object:In%5B11%5D:me_jd%3Dmarket_environment('me_jd',me_gbm.pricing_date)In%5B12%5D:# add jump diffusion-specific parametersme_jd.add_constant('lambda',0.3)me_jd.add_constant('mu',-0.75)me_jd.add_constant('delta',0.1)# add other parameters from gbmme_jd.add_environment(me_gbm)In%5B13%5D:# needed for portfolio valuationme_jd.add_constant('model','jd')Second, a European call option based on this new simulation object:In%5B14%5D:me_eur_call%3Dmarket_environment('me_eur_call',me_jd.pricing_date)In%5B15%5D:me_eur_call.add_constant('maturity',dt.datetime(2015,6,30))me_eur_call.add_constant('strike',38.)me_eur_call.add_constant('currency','EUR')In%5B16%5D:payoff_func%3D'np.maximum(maturity_value - strike, 0)'In%5B17%5D:eur_call_pos%3Dderivatives_position(name%3D'eur_call_pos',quantity%3D5,underlying%3D'jd',mar_env%3Dme_eur_call,otype%3D'European',payoff_func%3Dpayoff_func)From a portfolio perspective, the relevant market now is:In%5B18%5D:underlyings%3D%7B'gbm':me_gbm,'jd':me_jd%7Dpositions%3D%7B'am_put_pos':am_put_pos,'eur_call_pos':eur_call_pos%7D","For the moment we abstract from correlations between the underlyings. Compiling amarket_environment for the portfolio valuation is the last step before we can instantiatea derivatives_portfolio class:In%5B19%5D:# discounting object for the valuationcsr%3Dconstant_short_rate('csr',0.06)In%5B20%5D:val_env%3Dmarket_environment('general',me_gbm.pricing_date)val_env.add_constant('frequency','W')# monthly frequencyval_env.add_constant('paths',25000)val_env.add_constant('starting_date',val_env.pricing_date)val_env.add_constant('final_date',val_env.pricing_date)# not yet known%3B take pricing_date temporarilyval_env.add_curve('discount_curve',csr)# select single discount_curve for whole portfolioIn%5B21%5D:fromderivatives_portfolioimportderivatives_portfolioIn%5B22%5D:portfolio%3Dderivatives_portfolio(name%3D'portfolio',positions%3Dpositions,val_env%3Dval_env,assets%3Dunderlyings,fixed_seed%3DTrue)Now we can harness the power of the valuation class and get a bunch of different statisticsfor the derivatives_portfolio object just defined:In%5B23%5D:portfolio.get_statistics()Out%5B23%5D:            name    quant.     value   curr.   pos_value   pos_delta   pos_vega0   eur_call_pos        5   2.814638     EUR   14.073190      3.3605    42.79001     am_put_pos        3   4.472021     EUR   13.416063     -2.0895    30.5181The sum of the position values, Deltas, and Vegas is also easily calculated. This portfoliois slightly long Delta (almost neutral) and long Vega:In%5B24%5D:portfolio.get_statistics()%5B%5B'pos_value','pos_delta','pos_vega'%5D%5D.sum()# aggregate over all positionsOut%5B24%5D: pos_value    27.489253         pos_delta     1.271000         pos_vega     73.308100         dtype: float64A complete overview of all positions is conveniently obtained by the get_positionsmethod-such output can, for example, be used for reporting purposes (but is omittedhere due to reasons of space):In%5B25%5D:portfolio.get_positions()Of course, you can also access and use all (simulation, valuation, etc.) objects of thederivatives_portfolio object in direct fashion:","In%5B26%5D:portfolio.valuation_objects%5B'am_put_pos'%5D.present_value()Out%5B26%5D: 4.450573In%5B27%5D:portfolio.valuation_objects%5B'eur_call_pos'%5D.delta()Out%5B27%5D: 0.6498This derivatives portfolio valuation is conducted based on the assumption that the riskfactors are notcorrelated. This is easily verified by inspecting two simulated paths, onefor each simulation object:In%5B28%5D:path_no%3D777path_gbm%3Dportfolio.underlying_objects%5B'gbm'%5D.get_instrument_values()%5B:,path_no%5Dpath_jd%3Dportfolio.underlying_objects%5B'jd'%5D.get_instrument_values()%5B:,path_no%5DFigure 18-1 shows the selected paths in direct comparison-no jump occurs for thejump diffusion:In%5B29%5D:importmatplotlib.pyplotasplt%25matplotlibinlineIn%5B30%5D:plt.figure(figsize%3D(7,4))plt.plot(portfolio.time_grid,path_gbm,'r',label%3D'gbm')plt.plot(portfolio.time_grid,path_jd,'b',label%3D'jd')plt.xticks(rotation%3D30)plt.legend(loc%3D0)%3Bplt.grid(True)Figure 18-1. Noncorrelated risk factorsNow consider the case where the two risk factors are highly positively correlated:In%5B31%5D:correlations%3D%5B%5B'gbm','jd',0.9%5D%5DWith this additional information, a new derivatives_portfolio object is to beinstantiated:","In%5B32%5D:port_corr%3Dderivatives_portfolio(name%3D'portfolio',positions%3Dpositions,val_env%3Dval_env,assets%3Dunderlyings,correlations%3Dcorrelations,fixed_seed%3DTrue)In this case, there is no direct influence on the values of the positions in the portfolio:In%5B33%5D:port_corr.get_statistics()Out%5B33%5D:            name   quant.      value   curr.    pos_value   pos_delta   pos_vega0   eur_call_pos        5   2.804464    EUR     14.022320      3.3760    42.35001     am_put_pos        3   4.458565    EUR     13.375695     -2.0313    30.1416However, the correlation takes place behind the scenes. For the graphical illustration,we take the same two paths as before:In%5B34%5D:path_gbm%3Dport_corr.underlying_objects%5B'gbm'%5D.%5Cget_instrument_values()%5B:,path_no%5Dpath_jd%3Dport_corr.underlying_objects%5B'jd'%5D.%5Cget_instrument_values()%5B:,path_no%5DFigure 18-2 now shows a development almost in perfect parallelism between the tworisk factors:In%5B35%5D:plt.figure(figsize%3D(7,4))plt.plot(portfolio.time_grid,path_gbm,'r',label%3D'gbm')plt.plot(portfolio.time_grid,path_jd,'b',label%3D'jd')plt.xticks(rotation%3D30)plt.legend(loc%3D0)%3Bplt.grid(True)Figure 18-2. Highly correlated risk factorsAs a last numerical and conceptual example, consider the frequency distribution of theportfolio present value. This is something impossible to generate in general with other","approaches, like the application of analytical formulae or the binomial option pricingmodel. We get the complete set of present values per option position by calculating apresent value and passing the parameter flag full%3DTrue:In%5B36%5D:pv1%3D5*port_corr.valuation_objects%5B'eur_call_pos'%5D.%5Cpresent_value(full%3DTrue)%5B1%5Dpv1Out%5B36%5D: array(%5B  0.        ,  22.55857473,   8.2552922 , ...,   0.        ,                  0.        ,   0.        %5D)In%5B37%5D:pv2%3D3*port_corr.valuation_objects%5B'am_put_pos'%5D.%5Cpresent_value(full%3DTrue)%5B1%5Dpv2Out%5B37%5D: array(%5B 22.04450095,  10.90940926,  20.25092898, ...,  21.68232889,                 17.7583897 ,   0.        %5D)First, we compare the frequency distribution of the two positions. The payoff profilesof the two positions, as displayed in Figure 18-3, are quite different. Note that we limitboth the x- and y-axes for better readability:In%5B38%5D:plt.hist(%5Bpv1,pv2%5D,bins%3D25,label%3D%5B'European call','American put'%5D)%3Bplt.axvline(pv1.mean(),color%3D'r',ls%3D'dashed',lw%3D1.5,label%3D'call mean %3D %254.2f'%25pv1.mean())plt.axvline(pv2.mean(),color%3D'r',ls%3D'dotted',lw%3D1.5,label%3D'put mean %3D %254.2f'%25pv2.mean())plt.xlim(0,80)%3Bplt.ylim(0,10000)plt.grid()%3Bplt.legend()Figure 18-3. Portfolio frequency distribution of present valuesThe following figure finally shows the full frequency distribution of the portfolio presentvalues. You can clearly see in Figure 18-4the offsetting diversification effects of com%E2%80%90bining a call with a put option:In%5B39%5D:pvs%3Dpv1%2Bpv2plt.hist(pvs,bins%3D50,label%3D'portfolio')%3B","plt.axvline(pvs.mean(),color%3D'r',ls%3D'dashed',lw%3D1.5,label%3D'mean %3D %254.2f'%25pvs.mean())plt.xlim(0,80)%3Bplt.ylim(0,7000)plt.grid()%3Bplt.legend()Figure 18-4. Portfolio frequency distribution of present valuesWhat impact does the correlation between the two risk factors have on the risk of theportfolio, measured in the standard deviation of the present values%3F The statistics forthe portfolio with correlation are easily calculated as follows:In%5B40%5D:# portfolio with correlationpvs.std()Out%5B40%5D: 16.736290069957963Similarly, for the portfolio without correlation, we have:In%5B41%5D:# portfolio without correlationpv1%3D5*portfolio.valuation_objects%5B'eur_call_pos'%5D.%5Cpresent_value(full%3DTrue)%5B1%5Dpv2%3D3*portfolio.valuation_objects%5B'am_put_pos'%5D.%5Cpresent_value(full%3DTrue)%5B1%5D(pv1%2Bpv2).std()Out%5B41%5D: 21.71542409437863Although the mean value stays constant (ignoring numerical deviations), correlationobviously significantly decreases the portfolio risk when measured in this way. Again,this is an insight that it is not really possible to gain when using alternative numericalmethods or valuation approaches.","This chapter addresses the valuation and risk management of a portfolio of multiplederivatives positions dependent on multiple, possibly correlated, risk factors. To thisend, a new class called derivatives_position is introduced to model an options/","derivatives position. The main focus, however, lies on the derivatives_portfolioclass, which implements some rather complex tasks. For example, the class takescare of:%E2%80%A2Correlations between risk factors (the class generates a single, consistent set of ran%E2%80%90dom numbers for the simulation of all risk factors)%E2%80%A2Instantiation of simulation objects given the single market environments and thegeneral valuation environment, as well as the derivatives positions%E2%80%A2Generation of portfolio statistics based on all the assumptions, the risk factors in%E2%80%90volved, and the terms of the derivatives positionsThe examples presented in this chapter can only show some simple versions of deriv%E2%80%90atives portfolios that can be managed and valued with the DX library and the derivatives_portfolio class. Natural extensions to the DX library would be the addition ofmore sophisticated financial models, like a stochastic volatility model, and the additionof multirisk valuation classes to model and value derivatives dependent on multiple riskfactors, like a European basket option or an American maximum call option, to namejust two. At this stage, the modular modeling and the application of a valuation frame%E2%80%90work as general as the Fundamental Theorem of Asset Pricing (or %E2%80%9CGlobal Valuation%E2%80%9D)plays out its strengths: the nonredundant modeling of the risk factors and the accountingfor the correlations between them will then also have a direct influence on the valuesand Greeks of multirisk derivatives.Example 18-3 is a final, brief wrapper module bringing all components of the DXanalyticslibrary together for a single import statement.Example 18-3. The final wrapper module bringing all DX components together## DX Library Simulation# dx_library.py#fromdx_valuationimport*fromderivatives_positionimportderivatives_positionfromderivatives_portfolioimportderivatives_portfolioAlso, the now-complete init file for the dx directory is in Example 18-4.Example 18-4. Final Python packaging file## DX Library# packaging file# __init__.py#importnumpyasnpimportpandasaspd","importdatetimeasdt# framefromget_year_deltasimportget_year_deltasfromconstant_short_rateimportconstant_short_ratefrommarket_environmentimportmarket_environmentfromplot_option_statsimportplot_option_stats# simulationfromsn_random_numbersimportsn_random_numbersfromsimulation_classimportsimulation_classfromgeometric_brownian_motionimportgeometric_brownian_motionfromjump_diffusionimportjump_diffusionfromsquare_root_diffusionimportsquare_root_diffusion# valuationfromvaluation_classimportvaluation_classfromvaluation_mcs_europeanimportvaluation_mcs_europeanfromvaluation_mcs_americanimportvaluation_mcs_american# portfoliofromderivatives_positionimportderivatives_positionfromderivatives_portfolioimportderivatives_portfolio","As for the preceding chapters on the DX derivatives analytics library, Glasserman (2004)is a comprehensive resource for Monte Carlo simulation in the context of financialengineering and applications. Hilpisch (2015) also provides Python-based implemen%E2%80%90tations of the most important Monte Carlo algorithms:%E2%80%A2Glasserman, Paul (2004): Monte Carlo Methods in Financial Engineering. Springer,New York.%E2%80%A2Hilpisch, Yves (2015): Derivatives Analytics with Python. Wiley Finance, Chiches%E2%80%90ter, England. http://derivatives-analytics-with-python.com.However, there is hardly any research available when it comes to the valuation of (com%E2%80%90plex) portfolios of derivatives in a consistent, nonredundant fashion by Monte Carlosimulation. A notable exception, at least from a conceptual point of view, is the briefarticle by Albanese, Gimonet, and White (2010a). A bit more detailed is the white paperby the same team of authors:%E2%80%A2Albanese, Claudio, Guillaume Gimonet, and Steve White (2010a): %E2%80%9CTowards aGlobal Valuation Model.%E2%80%9D Risk Magazine, May issue. http://bit.ly/risk_may_2010.","%E2%80%A2Albanese, Claudio, Guillaume Gimonet, and Steve White (2010b): %E2%80%9CGlobal Valua%E2%80%90tion and Dynamic Risk Management.%E2%80%9D http://www.albanese.co.uk/Global_Valuation_and_Dynamic_Risk_Management.pdf.","1.For details on how the VSTOXX is calculated and how you can calculate it by yourself-using Pythontocollect the necessary data and to do the calculations-see the Python-based tutorial.","We are facing extreme volatility.- Carlos GhosnVolatility derivatives have become an important risk management and trading tool.While first-generation financial models for option pricing take volatility as just one ofa number of input parameters, second-generation models and products consider vola%E2%80%90tility as an asset class of its own. For example, the VIX volatility index (cf. http://en.wikipedia.org/wiki/CBOE_Volatility_Index), introduced in 1993, has since 2003 been cal%E2%80%90culated as a weighted implied volatility measure of certain out-of-the-money put andcall options with a constant maturity of 30 days on the S%26P 500 index. Generally, thefixed 30-day maturity main index values can only be calculated by interpolating betweena shorter and a longer maturity value for the index-i.e., between two subindices withvarying maturity.The VSTOXX volatility index-introduced in 2005 by Eurex, the derivatives exchangeoperated by Deutsche B%C3%B6rse AG in Germany (cf. http://www.eurexchange.com/advanced-services/)-is calculated similarly%3B however, it is based on implied volatilitiesfrom options on the EURO STOXX 50 index.1This chapter is about the use of the DX derivatives analytics library developed in Chapters15 to 18 to value a portfolio of American put options on the VSTOXX volatility index. Asof today, Eurex only offers futures contracts and Europeancall and put options on theVSTOXX. There are no American options on the VSTOXX available on public markets.This is quite a typical situation for a bank marketing and writing options on indices thatare not offered by the respective exchanges themselves. For simplicity, we assume that2.One of the earlier volatility option pricing models by Gruenbichler and Longstaff (1996) is also based on thesquare-root diffusion. However, they only consider European options, for which they come up with a closed-form solution. For a review of the model and a Python implementation of it, refer to http://www.eurexchange.com/advanced-services/vstoxx/. See also the web service example in Chapter 14, which is based ontheir model and analytical valuation formula.the maturity of the American put options coincides with the maturity of one of thetraded options series.As a model for the VSTOXX volatility index, we take the square_root_diffusionclassfrom the DXlibrary. This model satisfies the major requirements when it comes to themodeling of a quantity like volatility-i.e., mean reversion and positivity (see alsoChapters 10, 14, and 16).2In particular, this chapter implements the following major tasks:Data collectionWe need three types of data, namely for the VSTOXX index itself, the futures onthe index, and options data.Model calibrationTo value the nontraded options in a market-consistent fashion, one generally firstcalibrates the chosen model to quoted option prices in such a way that the modelbased on the optimal parameters replicates the market prices as well as possible.Portfolio valuationEquipped with all the data and a market-calibrated model for the VSTOXX volatilityindex, the final task then is to model and value the nontraded options.","This section collects step by step the necessary data to value the American put optionson the VSTOXX. First, let us import our libraries of choice when it comes to the gath%E2%80%90ering and management of data:In%5B1%5D:importnumpyasnpimportpandasaspd","In Chapter 6, there is a regression example based on the VSTOXX and EURO STOXX50 indices. There, we also use the following public source for VSTOXX daily closingdata:In%5B2%5D:url%3D'http://www.stoxx.com/download/historical_values/h_vstoxx.txt'vstoxx_index%3Dpd.read_csv(url,index_col%3D0,header%3D2,parse_dates%3DTrue,dayfirst%3DTrue)","In%5B3%5D:vstoxx_index.info()Out%5B3%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E        DatetimeIndex: 4010 entries, 1999-01-04 00:00:00 to 2014-09-26 00:00:00        Data columns (total 9 columns):        V2TX    4010 non-null float64        V6I1    3591 non-null float64        V6I2    4010 non-null float64        V6I3    3960 non-null float64        V6I4    4010 non-null float64        V6I5    4010 non-null float64        V6I6    3995 non-null float64        V6I7    4010 non-null float64        V6I8    3999 non-null float64        dtypes: float64(9)For the options analysis to follow, we only need VSTOXX index data for the first quarterof 2014. Therefore, we can delete both older and newer data contained now in theDataFramevstoxx_index:In%5B4%5D:vstoxx_index%3Dvstoxx_index%5B('2013/12/31'%3Cvstoxx_index.index)%26(vstoxx_index.index%3C'2014/4/1')%5DTaking a look at the data reveals that the data set not only contains daily closing valuesfor the main index V2TX, but also for all subindices from V6I1to V6I8, where the lastfigure represents the maturity (1 %3D closest maturity, 8 %3D longest maturity). As pointedout before, the main index generally is an interpolation of two subindices, in particularV6I1 and V6I2, representing in the first case a maturity of under 30 days and in thesecond case of between 30 and 60 days:In%5B5%5D:np.round(vstoxx_index.tail(),2)Out%5B5%5D:              V2TX   V6I1   V6I2   V6I3   V6I4   V6I5   V6I6   V6I7   V6I8        Date        2014-03-25  18.26  18.23  18.31  19.04  19.84  20.31  18.11  20.83  21.20        2014-03-26  17.59  17.48  17.70  18.45  19.42  20.00  20.26  20.45  20.86        2014-03-27  17.64  17.50  17.76  18.62  19.49  20.05  20.11  20.49  20.94        2014-03-28  17.03  16.68  17.29  18.33  19.30  19.83  20.14  20.38  20.82        2014-03-31  17.66  17.61  17.69  18.57  19.43  20.04  19.98  20.44  20.90","The data set we use for the futures and options data is not publicly available in this form.It is a complete data set with daily prices for all instruments traded on the VSTOXXvolatility index provided by Eurex. The data set covers the complete first quarter of 2014:In%5B6%5D:vstoxx_futures%3Dpd.read_excel('./source/vstoxx_march_2014.xlsx','vstoxx_futures')In%5B7%5D:vstoxx_futures.info()Out%5B7%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E        Int64Index: 504 entries, 0 to 503","3.VSTOXX volatility derivatives have their last trading day two days before expiry.        Data columns (total 8 columns):        A_DATE                       504 non-null datetime64%5Bns%5D        A_EXP_YEAR                   504 non-null int64        A_EXP_MONTH                  504 non-null int64        A_CALL_PUT_FLAG              504 non-null object        A_EXERCISE_PRICE             504 non-null int64        A_SETTLEMENT_PRICE_SCALED    504 non-null int64        A_PRODUCT_ID                 504 non-null object        SETTLE                       504 non-null float64        dtypes: datetime64%5Bns%5D(1), float64(1), int64(4), object(2)Several columns are not populated or not needed, such that we can delete them withoutloss of any relevant information:In%5B8%5D:delvstoxx_futures%5B'A_SETTLEMENT_PRICE_SCALED'%5Ddelvstoxx_futures%5B'A_CALL_PUT_FLAG'%5Ddelvstoxx_futures%5B'A_EXERCISE_PRICE'%5Ddelvstoxx_futures%5B'A_PRODUCT_ID'%5DFor brevity, we rename the remaining columns:In%5B9%5D:columns%3D%5B'DATE','EXP_YEAR','EXP_MONTH','PRICE'%5Dvstoxx_futures.columns%3DcolumnsAs is common market practice, exchange-traded options expire on the third Friday ofthe expiry month. To this end, it is helpful to have a helper function third_fridayavailable that gives, for a given year and month, the date of the third Friday:In%5B10%5D:importdatetimeasdtimportcalendardefthird_friday(date):day%3D21-(calendar.weekday(date.year,date.month,1)%2B2)%257returndt.datetime(date.year,date.month,day)For both VSTOXX futures and options, there are at any time eight relevant maturitieswith monthly differences starting either on the third Friday of the current month (beforethis third Friday) or on the third Friday of the nextmonth (one day before, on, or afterthis third Friday).3 In our data set, there are 11 relevant maturities, ranging from January2014 to November 2014:In%5B11%5D:set(vstoxx_futures%5B'EXP_MONTH'%5D)Out%5B11%5D: %7B1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11%7DWe calculate the specific dates of all third Fridays once to reuse them later. Note thatApril 18, 2014 was a public holiday in Germany, although that is irrelevant for thefollowing analysis:","In%5B12%5D:third_fridays%3D%7B%7Dformonthinset(vstoxx_futures%5B'EXP_MONTH'%5D):third_fridays%5Bmonth%5D%3Dthird_friday(dt.datetime(2014,month,1))In%5B13%5D:third_fridaysOut%5B13%5D: %7B1: datetime.datetime(2014, 1, 17, 0, 0),          2: datetime.datetime(2014, 2, 21, 0, 0),          3: datetime.datetime(2014, 3, 21, 0, 0),          4: datetime.datetime(2014, 4, 18, 0, 0),          5: datetime.datetime(2014, 5, 16, 0, 0),          6: datetime.datetime(2014, 6, 20, 0, 0),          7: datetime.datetime(2014, 7, 18, 0, 0),          8: datetime.datetime(2014, 8, 15, 0, 0),          9: datetime.datetime(2014, 9, 19, 0, 0),          10: datetime.datetime(2014, 10, 17, 0, 0),          11: datetime.datetime(2014, 11, 21, 0, 0)%7DWrapping the maturity date dict object in a lambda function allows for easy applicationto the respective EXP_MONTH column of the DataFrame object. For convenience, we storethe maturity dates alongside the other futures data:In%5B14%5D:tf%3Dlambdax:third_fridays%5Bx%5Dvstoxx_futures%5B'MATURITY'%5D%3Dvstoxx_futures%5B'EXP_MONTH'%5D.apply(tf)In%5B15%5D:vstoxx_futures.tail()Out%5B15%5D:           DATE  EXP_YEAR  EXP_MONTH  PRICE   MATURITY         499 2014-03-31      2014          7  20.40 2014-07-18         500 2014-03-31      2014          8  20.70 2014-08-15         501 2014-03-31      2014          9  20.95 2014-09-19         502 2014-03-31      2014         10  21.05 2014-10-17         503 2014-03-31      2014         11  21.25 2014-11-21","At any time, there are eight futures traded on the VSTOXX. In comparison, there areof course many more options, such that we expect a much larger data set for the volatilityoptions. In fact, we have almost 47,000 option quotes for the first quarter of 2014:In%5B16%5D:vstoxx_options%3Dpd.read_excel('./source/vstoxx_march_2014.xlsx','vstoxx_options')In%5B17%5D:vstoxx_options.info()Out%5B17%5D: %3Cclass 'pandas.core.frame.DataFrame'%3E         Int64Index: 46960 entries, 0 to 46959         Data columns (total 8 columns):         A_DATE                       46960 non-null datetime64%5Bns%5D         A_EXP_YEAR                   46960 non-null int64         A_EXP_MONTH                  46960 non-null int64         A_CALL_PUT_FLAG              46960 non-null object         A_EXERCISE_PRICE             46960 non-null int64         A_SETTLEMENT_PRICE_SCALED    46960 non-null int64","         A_PRODUCT_ID                 46960 non-null object         SETTLE                       46960 non-null float64         dtypes: datetime64%5Bns%5D(1), float64(1), int64(4), object(2)As before, not all columns are needed:In%5B18%5D:delvstoxx_options%5B'A_SETTLEMENT_PRICE_SCALED'%5Ddelvstoxx_options%5B'A_PRODUCT_ID'%5DA renaming of the columns simplifies later queries a bit:In%5B19%5D:columns%3D%5B'DATE','EXP_YEAR','EXP_MONTH','TYPE','STRIKE','PRICE'%5Dvstoxx_options.columns%3DcolumnsWe use the tf function to again store the maturity dates alongside the options data:In%5B20%5D:vstoxx_options%5B'MATURITY'%5D%3Dvstoxx_options%5B'EXP_MONTH'%5D.apply(tf)In%5B21%5D:vstoxx_options.head()Out%5B21%5D:         DATE  EXP_YEAR  EXP_MONTH TYPE  STRIKE  PRICE   MATURITY         0 2014-01-02      2014          1    C    1000   7.95 2014-01-17         1 2014-01-02      2014          1    C    1500   3.05 2014-01-17         2 2014-01-02      2014          1    C    1600   2.20 2014-01-17         3 2014-01-02      2014          1    C    1700   1.60 2014-01-17         4 2014-01-02      2014          1    C    1800   1.15 2014-01-17A single options contract is on 100 times the index value. Therefore, the strike price isalso scaled up accordingly. To have a view of a single unit, we rescale the strike price bydividing it by 100:In%5B22%5D:vstoxx_options%5B'STRIKE'%5D%3Dvstoxx_options%5B'STRIKE'%5D/100.All data from the external resources has now been collected and prepared. If needed,one can save the three DataFrame objects for later reuse:In%5B23%5D:save%3DFalseifsaveisTrue:importwarningswarnings.simplefilter('ignore')h5%3Dpd.HDFStore('./source/vstoxx_march_2014.h5',complevel%3D9,complib%3D'blosc')h5%5B'vstoxx_index'%5D%3Dvstoxx_indexh5%5B'vstoxx_futures'%5D%3Dvstoxx_futuresh5%5B'vstoxx_options'%5D%3Dvstoxx_optionsh5.close()","The next important step is the calibration of the financial model used to value theVSTOXX options to available market data. For an in-depth discussion of this topic andexample code in Python see Hilpisch (2015), in particular Chapter 11.","The first step when calibrating a model is to decide on the relevant market data to beused. For the example, let us assume the following:%E2%80%A2Pricing date shall be 31 March 2014.%E2%80%A2Option maturity shall be October 2014.The following Python code defines the pricing_date and maturity, reads the initial_valuefor the VSTOXX from the respective DataFrameobject, and also reads thecorresponding value forward for the VSTOXX future with the appropriate maturity.In%5B24%5D:pricing_date%3Ddt.datetime(2014,3,31)# last trading day in March 2014maturity%3Dthird_fridays%5B10%5D# October maturityinitial_value%3Dvstoxx_index%5B'V2TX'%5D%5Bpricing_date%5D# VSTOXX on pricing_dateforward%3Dvstoxx_futures%5B(vstoxx_futures.DATE%3D%3Dpricing_date)%26(vstoxx_futures.MATURITY%3D%3Dmaturity)%5D%5B'PRICE'%5D.values%5B0%5DOut of the many options quotes in the data set, we take only those that are:%E2%80%A2From the pricing date%E2%80%A2For the right maturity date%E2%80%A2For call options that are less than 20%25 out-of-the-money or in-the-moneyWe therefore have:In%5B25%5D:tol%3D0.20option_selection%3D %5Cvstoxx_options%5B(vstoxx_options.DATE%3D%3Dpricing_date)%26(vstoxx_options.MATURITY%3D%3Dmaturity)%26(vstoxx_options.TYPE%3D%3D'C')%26(vstoxx_options.STRIKE%3E(1-tol)*forward)%26(vstoxx_options.STRIKE%3C(1%2Btol)*forward)%5DThis leaves the following option quotes for the calibration procedure:In%5B26%5D:option_selectionOut%5B26%5D:             DATE  EXP_YEAR  EXP_MONTH TYPE  STRIKE  PRICE   MATURITY         46482 2014-03-31      2014         10    C      17   4.85 2014-10-17         46483 2014-03-31      2014         10    C      18   4.30 2014-10-17         46484 2014-03-31      2014         10    C      19   3.80 2014-10-17         46485 2014-03-31      2014         10    C      20   3.40 2014-10-17         46486 2014-03-31      2014         10    C      21   3.05 2014-10-17         46487 2014-03-31      2014         10    C      22   2.75 2014-10-17         46488 2014-03-31      2014         10    C      23   2.50 2014-10-17","         46489 2014-03-31      2014         10    C      24   2.25 2014-10-17         46490 2014-03-31      2014         10    C      25   2.10 2014-10-17","For the calibration of the square_root_diffusionmodel, the options selected beforehave to be modeled. This is the first time that the DXanalytics library comes into play%3Beverything else so far was %E2%80%9Cjust%E2%80%9D preparation for the following derivatives analytics tasks.We begin by importing the library:In%5B27%5D:fromdximport*The first task is then the definition of a market_environmentobject for the VSTOXXindex, in which we mainly store the previously collected and/or defined data:In%5B28%5D:me_vstoxx%3Dmarket_environment('me_vstoxx',pricing_date)In%5B29%5D:me_vstoxx.add_constant('initial_value',initial_value)me_vstoxx.add_constant('final_date',maturity)me_vstoxx.add_constant('currency','EUR')In%5B30%5D:me_vstoxx.add_constant('frequency','B')me_vstoxx.add_constant('paths',10000)In%5B31%5D:csr%3Dconstant_short_rate('csr',0.01)# somewhat arbitrarily chosen hereIn%5B32%5D:me_vstoxx.add_curve('discount_curve',csr)The major goal of the calibration procedure is to derive optimal parameters for thesquare_root_diffusion simulation class, namely kappa, theta, and volatility.These are the, so to say, degrees of freedom that this class offers. All other parametersare in general dictated by the market or the task at hand.Although the three (optimal) parameters are to be numerically derived, we need toprovide some dummy values to instantiate the simulation class. For the volatilityparameter, we take the historical volatility given our data set:In%5B33%5D:# parameters to be calibrated laterme_vstoxx.add_constant('kappa',1.0)me_vstoxx.add_constant('theta',1.2*initial_value)vol_est%3Dvstoxx_index%5B'V2TX'%5D.std() %5C*np.sqrt(len(vstoxx_index%5B'V2TX'%5D)/252.)me_vstoxx.add_constant('volatility',vol_est)In%5B34%5D:vol_estOut%5B34%5D: 1.0384283035169406Then we provide the market_environment object to the simulation class:In%5B35%5D:vstoxx_model%3Dsquare_root_diffusion('vstoxx_model',me_vstoxx)","Although the DX library is designed to be completely modular, to model risk factorsindependently (and nonredundantly) from the derivatives to be valued, this does notnecessarily have to be the case when it comes to a market_environment object. A singlesuch object can be used for both the underlying risk factor and the option to be valued.To complete the market environment for use with a valuation class, just add values forthe strike and the option maturity:In%5B36%5D:me_vstoxx.add_constant('strike',forward)me_vstoxx.add_constant('maturity',maturity)Of course, a payoff function is also needed to instantiate the valuation class:In%5B37%5D:payoff_func%3D'np.maximum(maturity_value - strike, 0)'In%5B38%5D:vstoxx_eur_call%3Dvaluation_mcs_european('vstoxx_eur_call',vstoxx_model,me_vstoxx,payoff_func)A brief sanity check to see if the modeling so far works %E2%80%9Cin principle%E2%80%9D:In%5B39%5D:vstoxx_eur_call.present_value()Out%5B39%5D: 0.379032To calibrate the model to the previously selected option quotes, we need to model allrelevant European call options. They only differentiate themselves by the relevant strikeprice%3B everything else in the market environment is the same. We store the single valu%E2%80%90ation objects in a dictobject. As keys for the dictobject, we take the index values ofthe option quotes in the DataFrame object option_selection for unique identification:In%5B40%5D:option_models%3D%7B%7Dforoptioninoption_selection.index:strike%3Doption_selection%5B'STRIKE'%5D.ix%5Boption%5Dme_vstoxx.add_constant('strike',strike)option_models%5Boption%5D%3D %5Cvaluation_mcs_european('eur_call_%25d'%25strike,vstoxx_model,me_vstoxx,payoff_func)A single step in the calibration routine makes the updating of all valuation objects anda revaluation of all options necessary. For convenience, we put this functionality into aseparate function:In%5B41%5D:defcalculate_model_values(p0):''' Returns all relevant option values.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             p0 : tuple/list                 tuple of kappa, theta, volatility             Returns","             %3D%3D%3D%3D%3D%3D%3D             model_values : dict                 dictionary with model values             '''kappa,theta,volatility%3Dp0vstoxx_model.update(kappa%3Dkappa,theta%3Dtheta,volatility%3Dvolatility)model_values%3D%7B%7Dforoptioninoption_models:model_values%5Boption%5D%3D %5Coption_models%5Boption%5D.present_value(fixed_seed%3DTrue)returnmodel_valuesProviding a parameter tuple of kappa, theta, and volatility to the function calculate_model_values gives back, ceteris paribus, model option values for all relevantoptions:In%5B42%5D:calculate_model_values((0.5,27.5,vol_est))Out%5B42%5D: %7B46482: 3.206401,          46483: 2.412354,          46484: 1.731028,          46485: 1.178823,          46486: 0.760421,          46487: 0.46249,          46488: 0.263662,          46489: 0.142177,          46490: 0.07219%7D","Calibration of an option pricing model is, in general, a convex optimization problem.The most widely used function used for the calibration-i.e., the minimization-is themean-squared error(MSE) for the model option values given the market quotes of theoptions. Assume there are N relevant options, and also model and market quotes. Theproblem of calibrating a financial model to the market quotes based on the MSE is thengiven in Equation 19-1. There,  C n* and  C nmodare the market price and the model priceof the nth option, respectively. p is the parameter set provided as input to the optionpricing model.Equation 19-1. Model calibration based on mean-squared errorminp1N%E2%88%91n%3D1N C n*%E2%88%92 C nmodp2","The Python function mean_squared_error implements this approach to model cali%E2%80%90bration technically. A global variable is used to control the output of intermediate pa%E2%80%90rameter tuple objects and the resulting MSE:In%5B43%5D:i%3D0defmean_squared_error(p0):''' Returns the mean-squared error given             the model and market values.             Parameters             %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D             p0 : tuple/list                 tuple of kappa, theta, volatility             Returns             %3D%3D%3D%3D%3D%3D%3D             MSE : float                 mean-squared error             '''globalimodel_values%3Dnp.array(calculate_model_values(p0).values())market_values%3Doption_selection%5B'PRICE'%5D.valuesoption_diffs%3Dmodel_values-market_valuesMSE%3Dnp.sum(option_diffs**2)/len(option_diffs)# vectorized MSE calculationifi%2520%3D%3D0:ifi%3D%3D0:print'%254s%256s%256s%256s --%3E %256s'%25 %5C('i','kappa','theta','vola','MSE')print'%254d%256.3f%256.3f%256.3f --%3E %256.3f'%25 %5C(i,p0%5B0%5D,p0%5B1%5D,p0%5B2%5D,MSE)i%2B%3D1returnMSEAgain, a brief check to see if the function works in principle:In%5B44%5D:mean_squared_error((0.5,27.5,vol_est))Out%5B44%5D:    i   kappa   theta    vola --%3E    MSE            0   0.500  27.500   1.038 --%3E  4.390         4.3899900376937779Chapter 9 introduces the Python and SciPy functions for convex optimization prob%E2%80%90lems. We will apply these here as well, so we begin with an import:In%5B45%5D:importscipy.optimizeasspoThe following calibration uses both global optimization via the brute function and localoptimization via the fmin function. First, the global optimization:In%5B46%5D:%25%25timei%3D0","opt_global%3Dspo.brute(mean_squared_error,((0.5,3.01,0.5),# range for kappa(15.,30.1,5.),# range for theta(0.5,5.51,1)),# range for volatilityfinish%3DNone)Out%5B46%5D:    i   kappa   theta    vola --%3E    MSE            0   0.500  15.000   0.500 --%3E 10.393           20   0.500  30.000   1.500 --%3E  2.071           40   1.000  25.000   3.500 --%3E  0.180           60   1.500  20.000   5.500 --%3E  0.718           80   2.000  20.000   1.500 --%3E  5.501          100   2.500  15.000   3.500 --%3E  5.571          120   2.500  30.000   5.500 --%3E 22.992          140   3.000  30.000   1.500 --%3E 14.493         CPU times: user 18.6 s, sys: 1.68 s, total: 20.3 s         Wall time: 20.3 sThe intermediate optimal results are as follows. The MSE is already quite low:In%5B47%5D:i%3D0mean_squared_error(opt_global)Out%5B47%5D:    i   kappa   theta    vola --%3E    MSE            0   1.500  20.000   4.500 --%3E  0.008         0.0076468730485555626Next, we use the intermediate optimal parameters as input for the local optimization:In%5B48%5D:%25%25timei%3D0opt_local%3Dspo.fmin(mean_squared_error,opt_global,xtol%3D0.00001,ftol%3D0.00001,maxiter%3D100,maxfun%3D350)Out%5B48%5D:    i   kappa   theta    vola --%3E    MSE            0   1.500  20.000   4.500 --%3E  0.008           20   1.510  19.235   4.776 --%3E  0.008           40   1.563  18.926   4.844 --%3E  0.005           60   1.555  18.957   4.828 --%3E  0.005           80   1.556  18.947   4.832 --%3E  0.005          100   1.556  18.948   4.831 --%3E  0.005          120   1.556  18.948   4.831 --%3E  0.005         Optimization terminated successfully.                  Current function value: 0.004654                  Iterations: 64                  Function evaluations: 138         CPU times: user 17.7 s, sys: 1.67 s, total: 19.3 s         Wall time: 19.4 sThis time the results are:In%5B49%5D:i%3D0mean_squared_error(opt_local)","Out%5B49%5D:    i   kappa   theta    vola --%3E    MSE            0   1.556  18.948   4.831 --%3E  0.005         0.0046542736439999875The resulting model values are:In%5B50%5D:calculate_model_values(opt_local)Out%5B50%5D: %7B46482: 4.746597,          46483: 4.286923,          46484: 3.863346,          46485: 3.474144,          46486: 3.119211,          46487: 2.793906,          46488: 2.494882,          46489: 2.224775,          46490: 1.98111%7DLet us store these in the option_selectionDataFrame and calculate the differencesfrom the market prices:In%5B51%5D:option_selection%5B'MODEL'%5D%3D %5Cnp.array(calculate_model_values(opt_local).values())option_selection%5B'ERRORS'%5D%3D %5Coption_selection%5B'MODEL'%5D-option_selection%5B'PRICE'%5DWe get the following results:In%5B52%5D:option_selection%5B%5B'MODEL','PRICE','ERRORS'%5D%5DOut%5B52%5D:           MODEL  PRICE    ERRORS         46482  4.746597   4.85 -0.103403         46483  4.286923   4.30 -0.013077         46484  3.863346   3.80  0.063346         46485  3.474144   3.40  0.074144         46486  3.119211   3.05  0.069211         46487  2.793906   2.75  0.043906         46488  2.494882   2.50 -0.005118         46489  2.224775   2.25 -0.025225         46490  1.981110   2.10 -0.118890The average pricing error is relatively low, at less than 1 cent:In%5B53%5D:round(option_selection%5B'ERRORS'%5D.mean(),3)Out%5B53%5D: -0.002Figure 19-1shows all the results graphically. The largest difference is observed for thecall option that is farthest out of the money:In%5B54%5D:importmatplotlib.pyplotasplt%25matplotlibinlinefix,(ax1,ax2)%3Dplt.subplots(2,sharex%3DTrue,figsize%3D(8,8))strikes%3Doption_selection%5B'STRIKE'%5D.valuesax1.plot(strikes,option_selection%5B'PRICE'%5D,label%3D'market quotes')","ax1.plot(strikes,option_selection%5B'MODEL'%5D,'ro',label%3D'model values')ax1.set_ylabel('option values')ax1.grid(True)ax1.legend(loc%3D0)wi%3D0.25ax2.bar(strikes-wi/2.,option_selection%5B'ERRORS'%5D,label%3D'market quotes',width%3Dwi)ax2.grid(True)ax2.set_ylabel('differences')ax2.set_xlabel('strikes')Figure 19-1. Calibrated model values for VSTOXX call options vs. market quotes","A major prerequisite for valuing and managing options not traded at exchanges is acalibrated model that is as consistent as possible with market realities-i.e., quotes forliquidly traded options in the relevant market. This is what the previous section has asthe main result. This main result is used in this section to value American put optionson the VSTOXX, a kind of derivative instrument not traded in the market. We assumea portfolio consisting of American put options with the same maturity and strikes asthe European call options used for the model calibration.","The first step when valuing a derivatives portfolio with the DX analytics library is todefine the relevant risk factors by a market_environment object. At this stage, it doesnot necessarily have to be complete%3B missing data and objects might be added duringthe portfolio valuation (e.g., paths or frequency):In%5B55%5D:me_vstoxx%3Dmarket_environment('me_vstoxx',pricing_date)me_vstoxx.add_constant('initial_value',initial_value)me_vstoxx.add_constant('final_date',pricing_date)me_vstoxx.add_constant('currency','NONE')Of course, we use the optimal parameters from the model calibration:In%5B56%5D:# adding optimal parameters to environmentme_vstoxx.add_constant('kappa',opt_local%5B0%5D)me_vstoxx.add_constant('theta',opt_local%5B1%5D)me_vstoxx.add_constant('volatility',opt_local%5B2%5D)In a portfolio context, the specification of a simulation class/model is necessary:In%5B57%5D:me_vstoxx.add_constant('model','srd')To define the valuation classes for the American put options, we are mainly missing anappropriate payoff function:In%5B58%5D:payoff_func%3D'np.maximum(strike - instrument_values, 0)'As before, all American options differ only with respect to their strike prices. It thereforemakes sense to define a shared market_environment object first:In%5B59%5D:shared%3Dmarket_environment('share',pricing_date)shared.add_constant('maturity',maturity)shared.add_constant('currency','EUR')It remains to loop over all relevant options, pick the relevant strike, and define onederivatives_position after the other, using the defining market_environmentobject:In%5B60%5D:option_positions%3D%7B%7D# dictionary for option positionsoption_environments%3D%7B%7D# dictionary for option environmentsforoptioninoption_selection.index:option_environments%5Boption%5D%3D %5Cmarket_environment('am_put_%25d'%25option,pricing_date)# define new option environment, one for each optionstrike%3Doption_selection%5B'STRIKE'%5D.ix%5Boption%5D# pick the relevant strikeoption_environments%5Boption%5D.add_constant('strike',strike)# add it to the environmentoption_environments%5Boption%5D.add_environment(shared)# add the shared dataoption_positions%5B'am_put_%25d'%25strike%5D%3D %5Cderivatives_position(","'am_put_%25d'%25strike,quantity%3D100.,underlying%3D'vstoxx_model',mar_env%3Doption_environments%5Boption%5D,otype%3D'American',payoff_func%3Dpayoff_func)Note that we use 100 as the position quantity throughout, which is the typical contractsize for VSTOXX options.","To compose the portfolio, we need to specify a couple of parameters that together defineour valuation environment-i.e., those parameters shared by all objects in the portfolio:In%5B61%5D:val_env%3Dmarket_environment('val_env',pricing_date)val_env.add_constant('starting_date',pricing_date)val_env.add_constant('final_date',pricing_date)# temporary value, is updated during valuationval_env.add_curve('discount_curve',csr)val_env.add_constant('frequency','B')val_env.add_constant('paths',25000)The market is rather simple%3B it consists of a single risk factor:In%5B62%5D:underlyings%3D%7B'vstoxx_model':me_vstoxx%7DTaking all this together allows us to define a derivatives_portfolio object:In%5B63%5D:portfolio%3Dderivatives_portfolio('portfolio',option_positions,val_env,underlyings)The valuation takes quite a bit of time, since multiple American options are valued bythe Least-Squares Monte Carlo approach and multiple Greeks also have to be estimatedby revaluations using the same computationally demanding algorithm:In%5B64%5D:%25timeresults%3Dportfolio.get_statistics(fixed_seed%3DTrue)Out%5B64%5D: CPU times: user 38.6 s, sys: 1.96 s, total: 40.6 s         Wall time: 40.6 sThe resultsDataFrameobject is best sorted by the namecolumn to have a better com%E2%80%90parative view of the statistics:In%5B65%5D:results.sort(columns%3D'name')Out%5B65%5D:         name  quant.      value curr.  pos_value  pos_delta  pos_vega         8  am_put_17     100   4.575197   EUR   457.5197     -24.85    102.77         1  am_put_18     100   5.203648   EUR   520.3648     -30.62    107.93         0  am_put_19     100   5.872686   EUR   587.2686     -33.31    107.79         2  am_put_20     100   6.578714   EUR   657.8714     -34.82    110.01         6  am_put_21     100   7.320523   EUR   732.0523     -39.46    105.20         7  am_put_22     100   8.081625   EUR   808.1625     -40.61    102.38         3  am_put_23     100   8.871962   EUR   887.1962     -43.26    104.37","         4  am_put_24     100   9.664272   EUR   966.4272     -40.14    101.04         5  am_put_25     100  10.475168   EUR  1047.5168     -45.74    102.81This portfolio is, as expected for a portfolio of long American put options, short (neg%E2%80%90ative) Delta and long (positive) Vega:In%5B66%5D:results%5B%5B'pos_value','pos_delta','pos_vega'%5D%5D.sum()Out%5B66%5D: pos_value    6664.3795         pos_delta    -332.8100         pos_vega      944.3000         dtype: float64","This chapter presents a larger, realistic use case for the application of the DXanalyticslibrary to the valuation of a portfolio of nontraded American options on the VSTOXXvolatility index. The chapter addresses three main tasks involved in any real-worldapplication:Data gatheringCurrent, correct market data builds the basis of any modeling and valuation effortin derivatives analytics%3B we need index data and futures data, as well as options datafor the VSTOXX.Model calibrationTo value, manage, and hedge nontraded options and derivatives in a market-consistent fashion, one needs to calibrate the model parameters to the relevantoption market quotes (relevant with regard to maturity and strikes). Our model ofchoice is the square-root diffusion, which is appropriate for modeling a volatilityindex%3B the calibration results are quite good although the model only offers threedegrees of freedom (kappa as the mean-reversion factor, thetaas the long-termvolatility, and volatility as the volatility of the volatility, or so-called %E2%80%9Cvol-vol%E2%80%9D).Portfolio valuationBased on the market data and the calibrated model, a portfolio with the Americanput options on the VSTOXX is modeled and major statistics (position values, Deltas,and Vegas) are generated.The realistic use case in this chapter shows the flexibility and the power of the DXlibrary%3Bit essentially allows us to address any analytical task with regard to derivatives. The veryapproach and architecture make the application largely comparable to the benchmarkcase of a Black-Scholes-Merton analytical formula for European options. Once the val%E2%80%90uation objects are defined, you can use them similarly to an analytical formula-andthis despite the fact that underneath the surface, heavy numerical routines and algo%E2%80%90rithms are applied.","Eurex's %E2%80%9CVSTOXX Advanced Services%E2%80%9D tutorial pages provide a wealth of informationabout the VSTOXX index and related volatility derivatives. These pages also providelots of readily usable Python scripts to replicate the results and analyses presented inthe tutorials:%E2%80%A2The VSTOXX Advanced Services tutorial pages from Eurex are available at http://www.eurexchange.com/advanced-services/vstoxx/, while a backtesting applicationis provided at http://www.eurexchange.com/advanced-services/app2/.The following book is a good general reference for the topics covered in this chapter,especially when it comes to the calibration of option pricing models:%E2%80%A2Hilpisch, Yves (2015): Derivatives Analytics with Python. Wiley Finance, Chiches%E2%80%90ter, England. http://derivatives-analytics-with-python.com.With regard to the consistent valuation and management of derivatives portfolios, seealso the hints at the end of Chapter 18.","Best practicesin general are those rules, either written down formally or just practicedin daily life, that may distinguish the expert Pythondeveloper from the casual Pythonuser. There are many of these, and this appendix will introduce some of the more im%E2%80%90portant ones.","One really helpful feature of Spyder as an integrated development environment is itsautomatic syntax and code checking, which checks Python code for compliance withthe PEP 8 recommendations for Pythonsyntax. But what is codified in %E2%80%9CPython En%E2%80%90hancement Proposal 8%E2%80%9D%3F Principally, there are some code formatting rules that shouldboth establish a common standard and allow for better readability of the code. In thatsense, this approach is not too dissimilar from a written or printed natural languagewhere certain syntax rules also apply.For example, consider the code in Example 1-1 of Chapter 1for the valuation of aEuropean call option via Monte Carlo simulation. First, have a look at the version ofthis code in Example A-1that does not conform to PEP 8. It is rather packed, becausethere are blank lines and spaces missing (sometimes there are also too many spaces orblank lines).Example A-1. A Python script that does not conform to PEP 8#   Monte Carlo valuation of European call option# in Black-Scholes-Merton model#  bsm_mcs_euro_syntax_false.pyimportnumpyasnp#Parameter ValuesS0%3D100.#initial index levelK%3D105.#strike priceT%3D1.0#time-to-maturity","r%3D0.05#riskless short ratesigma%3D0.2#volatilityI%3D100000# number of simulations# Valuation Algorithmz%3Dnp.random.standard_normal(I)#pseudorandom numbersST%3DS0*np.exp((r-0.5*sigma**2)%2Bsigma*sqrt(T)*z)#index values at maturityhT%3Dnp.maximum(ST-K,0)#inner values at maturityC0%3Dnp.exp(-r*T)*sum(hT)/I# Monte Carlo estimator# Result Outputprint%22Value of the European Call Option %255.3f%22%25C0Now, take a look at the version in Example A-2that conforms to PEP 8 (i.e., exactly theone found in Example 1-1). The main difference in readability stems from two facts:%E2%80%A2Use of blank lines to indicate code blocks%E2%80%A2Use of spaces around Python operators (e.g., %3D or *) as well as before any hashcharacter for comments (here: two spaces)Example A-2. A Python script that conforms to PEP 8## Monte Carlo valuation of European call option# in Black-Scholes-Merton model# bsm_mcs_euro_syntax_correct.py#importnumpyasnp# Parameter ValuesS0%3D100.# initial index levelK%3D105.# strike priceT%3D1.0# time-to-maturityr%3D0.05# riskless short ratesigma%3D0.2# volatilityI%3D100000# number of simulations# Valuation Algorithmz%3Dnp.random.standard_normal(I)# pseudorandom numbersST%3DS0*np.exp((r-0.5*sigma**2)*T%2Bsigma*np.sqrt(T)*z)# index values at maturityhT%3Dnp.maximum(ST-K,0)# inner values at maturityC0%3Dnp.exp(-r*T)*np.sum(hT)/I# Monte Carlo estimator# Result Outputprint%22Value of the European Call Option %255.3f%22%25C0","1.The majority of (Python) editors allow us to configure the use of a certain number of spaces even whenpushing the Tab key. Some editors also allow semiautomatic replacement of tabs with spaces.Although the first version is perfectly executable by the Python interpreter, the secondversion for sure is more readable for both the programmer and any others who may tryto understand it.Some special rules apply to functions and classes when it comes to formatting. In gen%E2%80%90eral, there are supposed to be twoblank lines before any new function (method) defi%E2%80%90nition as well as any new class definition. With functions, indentation also comes intoplay. In general, indentation is achieved through spaces and not through tabulators. Asa general rule, take four spaces per level of indentation.1 Consider now Example A-3.Example A-3. A Python function with multiple indentations## Function to check prime characteristic of integer# is_prime_no_doc.py#defis_prime(I):iftype(I)!%3Dint:raiseTypeError(%22Input has not the right type.%22)ifI%3C%3D3:raiseValueError(%22Number too small.%22)else:ifI%252%3D%3D0:print%22Number is even, therefore not prime.%22else:end%3Dint(I/2.)%2B1foriinrange(3,end,2):ifI%25i%3D%3D0:print%22Number is not prime, it is divided by %25d.%22%25ibreakifi%3E%3Dend-2:print%22Number is prime.%22We immediately notice the role indentation plays in Python. There are multiple levelsof indentation to indicate code blocks, here mainly %E2%80%9Ccaused%E2%80%9D by control structure ele%E2%80%90ments (e.g., if or else) or loops (e.g., the for loop).Control structure elements are explained in Chapter 4, but the basic working of thefunction should be clear even if you are not yet used to Python syntax. Table A-1 lists anumber of heavily used Python operators. Whenever there is a question mark in thedescription column of Table A-1, the operation returns a Boolean object (i.e., True orFalse).","Table A-1. Selected Python operators","%2BAddition-Subtraction/Division*Multiplication%25Modulo%3D%3DIs equal%3F!%3DIs not equal%3F%3CIs smaller%3F%3C%3DIs equal or smaller%3F%3EIs larger%3F%3E%3DIs equal or larger%3F","The two main elements of Python documentation are:Inline documentationInline documentation can in principle be placed anywhere in the code%3B it is indicatedby the use of one or more leading hash characters (#). In general, there should beat least two spaces before a hash.Documentation stringsSuch strings are used to provide documentation for Pythonfunctions (methods)and classes, and are generally placed within their definition (at the beginning of theindented code).The code in Example A-2 contains multiple examples of inline documentation.Example A-4shows the same function definition as in Example A-3, but this time witha documentation string added.Example A-4. The Python function is_prime with documentation string## Function to check prime characteristic of integer# is_prime_with_doc.py#defis_prime(I):''' Function to test for prime characteristic of an integer.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D","    I : int        number to be checked for prime characteristc    Returns    %3D%3D%3D%3D%3D%3D%3D    output: string        states whether number is prime or not%3B        if not, provide a prime factor    Raises    %3D%3D%3D%3D%3D%3D    TypeError        if argument is not an integer    ValueError        if the integer is too small (2 or smaller)    Examples    %3D%3D%3D%3D%3D%3D%3D%3D    %3E%3E%3E is_prime(11)    Number is prime.    %3E%3E%3E is_prime(8)    Number is even, therefore not prime.    %3E%3E%3E is_prime(int(1e8 %2B 7))    Number is prime.    %3E%3E%3E    '''iftype(I)!%3Dint:raiseTypeError(%22Input has not the right type.%22)ifI%3C%3D3:raiseValueError(%22Number too small.%22)else:ifI%252%3D%3D0:print%22Number is even, therefore not prime.%22else:end%3Dint(I/2.)%2B1foriinrange(3,end,2):ifI%25i%3D%3D0:print%22Number is not prime, it is divided by %25d.%22%25ibreakifi%3E%3Dend-2:print%22Number is prime.%22In general, such a documentation string provides information about the followingelements:InputWhich parameters/arguments to provide, and in which format (e.g., int)OutputWhat the function/method returns, and in which format","ErrorsWhich (%E2%80%9Cspecial%E2%80%9D) errors might be raisedExamplesExample usage of the function/methodsThe use of documentation strings is not only helpful for those who take a look at thecode itself. The majority of Python tools, like IPython and Spyder, allow direct accessto this documentation and help source. Figure A-1shows a screenshot of Spyder, thistime with the function is_primeshown in the editor and the rendered documentationstring of the function in the object inspector (upper right). This illustrates how helpfulit is to always include meaningful documentation strings in functions and classes.Figure A-1. Screenshot of Spyder with custom function and nicely rendered documenta%E2%80%90tion string","As a final best practice, we want to consider unit testing. Among the different testingapproaches, unit testing can indeed be considered a best practice because it tests Pythoncode on a rather fundamental level-i.e., the single units. What it does not test, however,is the integration of the single units. Typically, such units are functions, classes, or","methods of classes. As a pretty simple example of a Pythonfunction that is also easilytestable, consider the one in Example A-5.Example A-5. A rather simple Python function## Simple function to calculate# the square of the square root# of a positive number# simple_function.py#frommathimportsqrtdeff(x):''' Function to calculate the square of the square root.    Parameters    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    x : float or int        input number    Returns    %3D%3D%3D%3D%3D%3D%3D    fx : float        square of the square root, i.e. sqrt(x) ** 2    Raises    %3D%3D%3D%3D%3D%3D    TypeError        if argument is neither float nor integer    ValueError        if argument is negative    Examples    %3D%3D%3D%3D%3D%3D%3D%3D    %3E%3E%3E f(1)    1    %3E%3E%3E f(10.5)    10.5    '''iftype(x)!%3Dfloatandtype(x)!%3Dint:raiseTypeError(%22Input has not the right type.%22)ifx%3C0:raiseValueError(%22Number negative.%22)fx%3Dsqrt(x)**2returnfxThere are many tools available that help support unit tests. We will make use of nose inwhat follows. Example A-6 contains a small test suite for the simple function ffromExample A-5.","Example A-6. A test suite for the function f## Test suite for simple function f# nose_test.py#importnose.toolsasntfromsimple_functionimportfdeftest_f_calculation():''' Tests if function f calculates correctly. '''nt.assert_equal(f(4.),4.)nt.assert_equal(f(1000),1000)nt.assert_equal(f(5000.5),5000.5)deftest_f_type_error():''' Tests if type error is raised. '''nt.assert_raises(TypeError,f,'test string')nt.assert_raises(TypeError,f,%5B3,'string'%5D)deftest_f_value_error():''' Tests if value error is raised. '''nt.assert_raises(ValueError,f,-1)nt.assert_raises(ValueError,f,-2.5)deftest_f_test_fails():''' Tests if function test fails. '''nt.assert_equal(f(5.),10)Table A-2 describes the test functions that are implemented.Table A-2. Test functions for simple function f","test_f_calculationTests if the function generates correct resultstest_f_type_errorChecks if the function raises a type error when expectedtest_f_value_errorChecks if the function raises a value error when expectedtest_f_test_failsTests if the calculation test fails as expected (for illustration)From the command line/shell, you can run the following tests:%24 nosetests nose_test.py...F%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3DFAIL: Test if function test fails. ---------------------------------------------------------------------- Traceback (most recent call last):","   File %22/Library/anaconda/lib/python2.7/site-packages/nose/case.py%22,   line 197, in runTest self.test(*self.arg)   File %22//Users/yhilpisch/Documents/Work/Python4Finance/python/nose_test.py%22,   line 30, in test_f_test_fails     nt.assert_equal(f(5.), 10) AssertionError: 5.000000000000001 !%3D 10 ----------------------------------------------------------------------Ran 4 tests in 0.002sFAILED (failures%3D1)%24Obviously, the first three tests are successful, while the last one fails as expected. Usingsuch tools-and more importantly, implementing a rigorous approach to unit testing-may require more effort up front, but you and those working with your code will benefitin the long run.","Example B-1 contains a class definition for a European call option in the Black-Scholes-Merton (1973) model (cf. Chapter 3, and in particular Example 3-1).Example B-1. Implementation of a Black-Scholes-Merton call option class## Valuation of European call options in Black-Scholes-Merton Model# incl. Vega function and implied volatility estimation# -- class-based implementation# bsm_option_class.py#frommathimportlog,sqrt,expfromscipyimportstatsclasscall_option(object):''' Class for European call options in BSM model.    Attributes    %3D%3D%3D%3D%3D%3D%3D%3D%3D%3D    S0 : float        initial stock/index level    K : float        strike price    T : float        maturity (in year fractions)    r : float        constant risk-free short rate    sigma : float        volatility factor in diffusion term    Methods    %3D%3D%3D%3D%3D%3D%3D    value : float        return present value of call option","    vega : float        return Vega of call option    imp_vol: float        return implied volatility given option quote    '''def__init__(self,S0,K,T,r,sigma):self.S0%3Dfloat(S0)self.K%3DKself.T%3DTself.r%3Drself.sigma%3Dsigmadefvalue(self):''' Returns option value. '''d1%3D((log(self.S0/self.K)%2B(self.r%2B0.5*self.sigma**2)*self.T)/(self.sigma*sqrt(self.T)))d2%3D((log(self.S0/self.K)%2B(self.r-0.5*self.sigma**2)*self.T)/(self.sigma*sqrt(self.T)))value%3D(self.S0*stats.norm.cdf(d1,0.0,1.0)-self.K*exp(-self.r*self.T)*stats.norm.cdf(d2,0.0,1.0))returnvaluedefvega(self):''' Returns Vega of option. '''d1%3D((log(self.S0/self.K)%2B(self.r%2B0.5*self.sigma**2)*self.T)/(self.sigma*sqrt(self.T)))vega%3Dself.S0*stats.norm.cdf(d1,0.0,1.0)*sqrt(self.T)returnvegadefimp_vol(self,C0,sigma_est%3D0.2,it%3D100):''' Returns implied volatility given option price. '''option%3Dcall_option(self.S0,self.K,self.T,self.r,sigma_est)foriinrange(it):option.sigma-%3D(option.value()-C0)/option.vega()returnoption.sigmaThis class can be used in an interactive IPython session as follows:In%5B1%5D:frombsm_option_classimportcall_optionIn%5B2%5D:o%3Dcall_option(100.,105.,1.0,0.05,0.2)type(o)Out%5B2%5D: bsm_option_class.call_optionIn%5B3%5D:value%3Do.value()valueOut%5B3%5D: 8.0213522351431763In%5B4%5D:o.vega()","Out%5B4%5D: 54.222833358480528In%5B5%5D:o.imp_vol(C0%3Dvalue)Out%5B5%5D: 0.20000000000000001The option class can be easily used to visualize, for example, the value and Vega of theoption for different strikes and maturities. This is, in the end, one of the major advan%E2%80%90tages of having such formulae available. The following Python code generates the optionstatistics for different maturity-strike combinations:In%5B6%5D:importnumpyasnpmaturities%3Dnp.linspace(0.05,2.0,20)strikes%3Dnp.linspace(80,120,20)T,K%3Dnp.meshgrid(strikes,maturities) C %3Dnp.zeros_like(K)V%3Dnp.zeros_like( C )fortinenumerate(maturities):forkinenumerate(strikes):o.T%3Dt%5B1%5Do.K%3Dk%5B1%5D C %5Bt%5B0%5D,k%5B0%5D%5D%3Do.value()V%5Bt%5B0%5D,k%5B0%5D%5D%3Do.vega()First, let us have a look at the option values. For plotting, we need to import somelibraries and functions:In%5B7%5D:importmatplotlib.pyplotaspltfrommpl_toolkits.mplot3dimportAxes3Dfrompylabimportcm%25matplotlibinlineThe output of the following code is presented in Figure B-1:In%5B8%5D:fig%3Dplt.figure(figsize%3D(12,7))ax%3Dfig.gca(projection%3D'3d')surf%3Dax.plot_surface(T,K, C ,rstride%3D1,cstride%3D1,cmap%3Dcm.coolwarm,linewidth%3D0.5,antialiased%3DTrue)ax.set_xlabel('strike')ax.set_ylabel('maturity')ax.set_zlabel('European call option value')fig.colorbar(surf,shrink%3D0.5,aspect%3D5)Second, we have the results for the Vega of the call option, as shown in Figure B-2:In%5B9%5D:fig%3Dplt.figure(figsize%3D(12,7))ax%3Dfig.gca(projection%3D'3d')surf%3Dax.plot_surface(T,K,V,rstride%3D1,cstride%3D1,cmap%3Dcm.coolwarm,linewidth%3D0.5,antialiased%3DTrue)ax.set_xlabel('strike')ax.set_ylabel('maturity')ax.set_zlabel('Vega of European call option')fig.colorbar(surf,shrink%3D0.5,aspect%3D5)plt.show()","Figure B-1. Value of European call optionFigure B-2. Vega of European call option","Compared with the code in Example 3-1 of Chapter 3, the code in Example B-1 of thisappendix shows a number of advantages:%E2%80%A2Better overall code structure and readability%E2%80%A2Avoidance of redundant definitions as far as possible%E2%80%A2Better reusability and more compact method callsThe option class also lends itself pretty well to the visualization of option statistics.","1.For more information on this module, see the online documentation.","As in the majority of scientific disciplines, dates and times play an important role infinance. This appendix introduces different aspects of this topic when it comes to Pythonprogramming. It cannot, of course, not be exhaustive. However, it provides an intro%E2%80%90duction into the main areas of the Pythonecosystem that support the modeling of dateand time information.","The datetimemodule from the Pythonstandard library allows for the implementationof the most important date and time-related tasks.1 We start by importing the module:In%5B1%5D:importdatetimeasdtTwo different functions provide the exact current date and time:In%5B2%5D:dt.datetime.now()Out%5B2%5D: datetime.datetime(2014, 9, 14, 19, 22, 24, 366619)In%5B3%5D:to%3Ddt.datetime.today()toOut%5B3%5D: datetime.datetime(2014, 9, 14, 19, 22, 24, 491234)The resulting object is a datetime object:In%5B4%5D:type(to)Out%5B4%5D: datetime.datetimeThe method weekday provides the number for the day of the week, given a datetimeobject:","In%5B5%5D:dt.datetime.today().weekday()# zero-based numbering%3B 0 %3D MondayOut%5B5%5D: 6Such an object can, of course, be directly constructed:In%5B6%5D:d%3Ddt.datetime(2016,10,31,10,5,30,500000)dOut%5B6%5D: datetime.datetime(2016, 10, 31, 10, 5, 30, 500000)In%5B7%5D:printdOut%5B7%5D: 2016-10-31 10:05:30.500000In%5B8%5D:str(d)Out%5B8%5D: '2016-10-31 10:05:30.500000'From such an object you can easily extract, for example, year, month, day information,and so forth:In%5B9%5D:d.yearOut%5B9%5D: 2016In%5B10%5D:d.monthOut%5B10%5D: 10In%5B11%5D:d.dayOut%5B11%5D: 31In%5B12%5D:d.hourOut%5B12%5D: 10Via the method toordinal, you can translate the date information to ordinal numberrepresentation:In%5B13%5D:o%3Dd.toordinal()oOut%5B13%5D: 736268This also works the other way around. However, you lose the time information duringthis process:In%5B14%5D:dt.datetime.fromordinal(o)Out%5B14%5D: datetime.datetime(2016, 10, 31, 0, 0)On the other hand, you can separate out the time information from the datetimeobject,which then gives you a time object:In%5B15%5D:t%3Ddt.datetime.time(d)tOut%5B15%5D: datetime.time(10, 5, 30, 500000)","In%5B16%5D:type(t)Out%5B16%5D: datetime.timeSimilarly, you can separate out the date information only, ending up with a dateobject:In%5B17%5D:dd%3Ddt.datetime.date(d)ddOut%5B17%5D: datetime.date(2016, 10, 31)Often, a certain degree of precision is sufficient. To this end, you can simply replacecertain attributes of the datetime object with literal:In%5B18%5D:d.replace(second%3D0,microsecond%3D0)Out%5B18%5D: datetime.datetime(2016, 10, 31, 10, 5)timedelta is another class of objects that result from arithmetic operations on the otherdate-time-related objects:In%5B19%5D:td%3Dd-dt.datetime.now()tdOut%5B19%5D: datetime.timedelta(777, 52983, 863732)In%5B20%5D:type(td)Out%5B20%5D: datetime.timedeltaAgain, you can access the attributes directly to extract detailed information:In%5B21%5D:td.daysOut%5B21%5D: 777In%5B22%5D:td.secondsOut%5B22%5D: 52983In%5B23%5D:td.microsecondsOut%5B23%5D: 863732In%5B24%5D:td.total_seconds()Out%5B24%5D: 67185783.863732There are multiple ways to transform a datetime object into different representations,as well as to generate datetimeobjects out of, say, a stringobject. Details are found inthe documentation of the datetime module. Here are a few examples:In%5B25%5D:d.isoformat()Out%5B25%5D: '2016-10-31T10:05:30.500000'In%5B26%5D:d.strftime(%22%25A, %25d. %25B %25Y %25I:%25M%25p%22)Out%5B26%5D: 'Monday, 31. October 2016 10:05AM'","In%5B27%5D:dt.datetime.strptime('2017-03-31','%25Y-%25m-%25d')# year first and four-digit yearOut%5B27%5D: datetime.datetime(2017, 3, 31, 0, 0)In%5B28%5D:dt.datetime.strptime('30-4-16','%25d-%25m-%25y')# day first and two-digit yearOut%5B28%5D: datetime.datetime(2016, 4, 30, 0, 0)In%5B29%5D:ds%3Dstr(d)dsOut%5B29%5D: '2016-10-31 10:05:30.500000'In%5B30%5D:dt.datetime.strptime(ds,'%25Y-%25m-%25d %25H:%25M:%25S.%25f')Out%5B30%5D: datetime.datetime(2016, 10, 31, 10, 5, 30, 500000)In addition to the now and today functions, there is also the utcnowfunction, whichgives the exact date and time information in UTC (Coordinated Universal Time, for%E2%80%90merly known as Greenwich Mean Time, or GMT). This represents a two-hour differencefrom the author's time zone (CET):In%5B31%5D:dt.datetime.now()Out%5B31%5D: datetime.datetime(2014, 9, 14, 19, 22, 28, 123943)In%5B32%5D:dt.datetime.utcnow()#  Coordinated Universal TimeOut%5B32%5D: datetime.datetime(2014, 9, 14, 17, 22, 28, 240319)In%5B33%5D:dt.datetime.now()-dt.datetime.utcnow()# UTC %2B 2h %3D CET (summer)Out%5B33%5D: datetime.timedelta(0, 7199, 999982)Another class of the datetime module is the tzinfo class, a generic time zone class withmethods utcoffset, dst, and tzname. dst stands for Daylight Saving Time (DST). Adefinition for UTC time might look as follows:In%5B34%5D:classUTC(dt.tzinfo):defutcoffset(self,d):returndt.timedelta(hours%3D0)defdst(self,d):returndt.timedelta(hours%3D0)deftzname(self,d):return%22UTC%22This can be used as an attribute to a datetime object and be defined via the replacemethod:In%5B35%5D:u%3Ddt.datetime.utcnow()u%3Du.replace(tzinfo%3DUTC())# attach time zone informationu","Out%5B35%5D: datetime.datetime(2014, 9, 14, 17, 22, 28, 597383, tzinfo%3D%3C__main__.UTC          object at 0x7f59e496ec10%3E)Similarly, the following definition is for CET during the summer:In%5B36%5D:classCET(dt.tzinfo):defutcoffset(self,d):returndt.timedelta(hours%3D2)defdst(self,d):returndt.timedelta(hours%3D1)deftzname(self,d):return%22CET %2B 1%22Making use of the astimezonemethod then makes it straightforward to transform theUTC-based datetime object u into a CET-based one:In%5B37%5D:u.astimezone(CET())Out%5B37%5D: datetime.datetime(2014, 9, 14, 19, 22, 28, 597383, tzinfo%3D%3C__main__.CET          object at 0x7f59e79d8f10%3E)There is a Python module available called pytz that implements the most importanttime zones from around the world:In%5B38%5D:importpytzcountry_names and country_timezonesare dictionaries containing the countries andtime zones covered:In%5B39%5D:pytz.country_names%5B'US'%5DOut%5B39%5D: u'United States'In%5B40%5D:pytz.country_timezones%5B'BE'%5DOut%5B40%5D: %5Bu'Europe/Brussels'%5DIn%5B41%5D:pytz.common_timezones%5B-10:%5DOut%5B41%5D: %5B'Pacific/Wake',          'Pacific/Wallis',          'US/Alaska',          'US/Arizona',          'US/Central',          'US/Eastern',          'US/Hawaii',          'US/Mountain',          'US/Pacific',          'UTC'%5DWith pytz, there is generally no need to define your own tzinfo objects:In%5B42%5D:u%3Ddt.datetime.utcnow()u%3Du.replace(tzinfo%3Dpytz.utc)uOut%5B42%5D: datetime.datetime(2014, 9, 14, 17, 22, 29, 503702, tzinfo%3D%3CUTC%3E)","In%5B43%5D:u.astimezone(pytz.timezone(%22CET%22))Out%5B43%5D: datetime.datetime(2014, 9, 14, 19, 22, 29, 503702, tzinfo%3D%3CDstTzInfo 'C         ET' CEST%2B2:00:00 DST%3E)In%5B44%5D:u.astimezone(pytz.timezone(%22GMT%22))Out%5B44%5D: datetime.datetime(2014, 9, 14, 17, 22, 29, 503702, tzinfo%3D%3CStaticTzInfo          'GMT'%3E)In%5B45%5D:u.astimezone(pytz.timezone(%22US/Central%22))Out%5B45%5D: datetime.datetime(2014, 9, 14, 12, 22, 29, 503702, tzinfo%3D%3CDstTzInfo 'U         S/Central' CDT-1 day, 19:00:00 DST%3E)","Since NumPy1.7, there has been native date-time information support in NumPy. Thebasic class is called datetime64:In%5B46%5D:importnumpyasnpIn%5B47%5D:nd%3Dnp.datetime64('2015-10-31')ndOut%5B47%5D: numpy.datetime64('2015-10-31')Like datetime objects, datetime64 objects can be represented as string objects:In%5B48%5D:np.datetime_as_string(nd)Out%5B48%5D: '2015-10-31'Every such object has metadata stored with it, which can be accessed via the datetime_datamethod. The two main components are the frequency information (e.g., Dfor day) and the unit (e.g., 1 for one day in our case):In%5B49%5D:np.datetime_data(nd)Out%5B49%5D: ('D', 1)A datetime64 object can easily be constructed from a datetime object:In%5B50%5D:dOut%5B50%5D: datetime.datetime(2016, 10, 31, 10, 5, 30, 500000)In%5B51%5D:nd%3Dnp.datetime64(d)ndOut%5B51%5D: numpy.datetime64('2016-10-31T11:05:30.500000%2B0100')Similarly, using the astype method, a datetime64 object can be converted into a datetime object:In%5B52%5D:nd.astype(dt.datetime)Out%5B52%5D: datetime.datetime(2016, 10, 31, 10, 5, 30, 500000)","Another way to construct such an object is by providing a stringobject, e.g., with yearand month, and the frequency information. Note that in the following case, the objectvalue defaults to the first day of the month:In%5B53%5D:nd%3Dnp.datetime64('2015-10','D')ndOut%5B53%5D: numpy.datetime64('2015-10-01')Comparing two datetime64 objects yields a True value whenever the information givenis the same-even if the level of detail is different:In%5B54%5D:np.datetime64('2015-10')%3D%3Dnp.datetime64('2015-10-01')Out%5B54%5D: TrueOf course, you can also define ndarray objects containing multiple datetime64objects:In%5B55%5D:np.array(%5B'2016-06-10','2016-07-10','2016-08-10'%5D,dtype%3D'datetime64')Out%5B55%5D: array(%5B'2016-06-10', '2016-07-10', '2016-08-10'%5D, dtype%3D'datetime64%5BD%5D')In%5B56%5D:np.array(%5B'2016-06-10T12:00:00','2016-07-10T12:00:00','2016-08-10T12:00:00'%5D,dtype%3D'datetime64%5Bs%5D')Out%5B56%5D: array(%5B'2016-06-10T12:00:00%2B0200', '2016-07-10T12:00:00%2B0200',                '2016-08-10T12:00:00%2B0200'%5D, dtype%3D'datetime64%5Bs%5D')You can also generate ranges of dates by using the function arange. Different frequencies(e.g., days, months, or weeks) are easily taken care of:In%5B57%5D:np.arange('2016-01-01','2016-01-04',dtype%3D'datetime64')# daily frequency as default in this caseOut%5B57%5D: array(%5B'2016-01-01', '2016-01-02', '2016-01-03'%5D, dtype%3D'datetime64%5BD%5D')In%5B58%5D:np.arange('2016-01-01','2016-10-01',dtype%3D'datetime64%5BM%5D')# monthly frequencyOut%5B58%5D: array(%5B'2016-01', '2016-02', '2016-03', '2016-04', '2016-05', '2016-06',                '2016-07', '2016-08', '2016-09'%5D, dtype%3D'datetime64%5BM%5D')In%5B59%5D:np.arange('2016-01-01','2016-10-01',dtype%3D'datetime64%5BW%5D')%5B:10%5D# weekly frequencyOut%5B59%5D: array(%5B'2015-12-31', '2016-01-07', '2016-01-14', '2016-01-21',                '2016-01-28', '2016-02-04', '2016-02-11', '2016-02-18',                '2016-02-25', '2016-03-03'%5D, dtype%3D'datetime64%5BW%5D')You can also easily use subday frequencies, like hours or seconds (refer to the docu%E2%80%90mentation for all options):In%5B60%5D:dtl%3Dnp.arange('2016-01-01T00:00:00','2016-01-02T00:00:00',dtype%3D'datetime64%5Bh%5D')# hourly frequencydtl%5B:10%5D","Out%5B60%5D: array(%5B'2016-01-01T00%2B0100', '2016-01-01T01%2B0100', '2016-01-01T02%2B0100',                '2016-01-01T03%2B0100', '2016-01-01T04%2B0100', '2016-01-01T05%2B0100',                '2016-01-01T06%2B0100', '2016-01-01T07%2B0100', '2016-01-01T08%2B0100',                '2016-01-01T09%2B0100'%5D, dtype%3D'datetime64%5Bh%5D')Plotting date-time and/or time series data can sometimes be tricky. matplotlibhasgood support for standard datetime objects. Transforming datetime64informationinto datetime information generally does the trick, as the following example, whoseresult is shown in Figure C-1, illustrates:In%5B61%5D:importmatplotlib.pyplotasplt%25matplotlibinlineIn%5B62%5D:np.random.seed(3000)rnd%3Dnp.random.standard_normal(len(dtl)).cumsum()**2In%5B63%5D:fig%3Dplt.figure()plt.plot(dtl.astype(dt.datetime),rnd)# convert np.datetime to datetime.datetimeplt.grid(True)fig.autofmt_xdate()# autoformatting of datetime x-ticksFigure C-1. Plot with datetime.datetime x-ticks autoformattedFinally, we also have an illustration of using arangewith seconds and milliseconds asfrequencies:In%5B64%5D:np.arange('2016-01-01T00:00:00','2016-01-02T00:00:00',dtype%3D'datetime64%5Bs%5D')%5B:10%5D# seconds as frequencyOut%5B64%5D: array(%5B'2016-01-01T00:00:00%2B0100', '2016-01-01T00:00:01%2B0100',                '2016-01-01T00:00:02%2B0100', '2016-01-01T00:00:03%2B0100',                '2016-01-01T00:00:04%2B0100', '2016-01-01T00:00:05%2B0100',                '2016-01-01T00:00:06%2B0100', '2016-01-01T00:00:07%2B0100',                '2016-01-01T00:00:08%2B0100', '2016-01-01T00:00:09%2B0100'%5D, dtype%3D'         datetime64%5Bs%5D')","In%5B65%5D:np.arange('2016-01-01T00:00:00','2016-01-02T00:00:00',dtype%3D'datetime64%5Bms%5D')%5B:10%5D# milliseconds as frequencyOut%5B65%5D: array(%5B'2016-01-01T00:00:00.000%2B0100', '2016-01-01T00:00:00.001%2B0100',                '2016-01-01T00:00:00.002%2B0100', '2016-01-01T00:00:00.003%2B0100',                '2016-01-01T00:00:00.004%2B0100', '2016-01-01T00:00:00.005%2B0100',                '2016-01-01T00:00:00.006%2B0100', '2016-01-01T00:00:00.007%2B0100',                '2016-01-01T00:00:00.008%2B0100', '2016-01-01T00:00:00.009%2B0100'%5D,          dtype%3D'datetime64%5Bms%5D')","The pandaslibrary was specifically designed with time series data in mind. Therefore,the library provides classes that are able to efficiently handle date-time information, likethe DatetimeIndex class for time indices (cf. the documentation):In%5B66%5D:importpandasaspdDate-time information in pandas is generally stored as a Timestamp object:In%5B67%5D:ts%3Dpd.Timestamp('2016-06-30')tsOut%5B67%5D: Timestamp('2016-06-30 00:00:00')Such objects are easily transformed into regular datetime objects with the to_datetimemethod:In%5B68%5D:d%3Dts.to_datetime()dOut%5B68%5D: datetime.datetime(2016, 6, 30, 0, 0)Similarly, a Timestampobject is straightforwardly constructed from a datetime object:In%5B69%5D:pd.Timestamp(d)Out%5B69%5D: Timestamp('2016-06-30 00:00:00')or from a NumPydatetime64 object:In%5B70%5D:pd.Timestamp(nd)Out%5B70%5D: Timestamp('2015-10-01 00:00:00')Another important class is the DatetimeIndex class, which is a collection of Timestampobjects with a number of powerful methods attached (cf. http://bit.ly/date_range_docand http://bit.ly/datetimeindex_doc). Such an object can be instantiated with thedate_range function, which is rather flexible and powerful for constructing time indices(see Chapter 6 for more details on this function):In%5B71%5D:dti%3Dpd.date_range('2016/01/01',freq%3D'M',periods%3D12)dti","Out%5B71%5D: %3Cclass 'pandas.tseries.index.DatetimeIndex'%3E         %5B2016-01-31, ..., 2016-12-31%5D         Length: 12, Freq: M, Timezone: NoneSingle elements of the object are accessed by the usual indexing operations:In%5B72%5D:dti%5B6%5DOut%5B72%5D: Timestamp('2016-07-31 00:00:00', offset%3D'M')DatetimeIndex objects can be transformed into arrays of datetimeobjects through themethod to_pydatetime:In%5B73%5D:pdi%3Ddti.to_pydatetime()pdiOut%5B73%5D: array(%5Bdatetime.datetime(2016, 1, 31, 0, 0),                datetime.datetime(2016, 2, 29, 0, 0),                datetime.datetime(2016, 3, 31, 0, 0),                datetime.datetime(2016, 4, 30, 0, 0),                datetime.datetime(2016, 5, 31, 0, 0),                datetime.datetime(2016, 6, 30, 0, 0),                datetime.datetime(2016, 7, 31, 0, 0),                datetime.datetime(2016, 8, 31, 0, 0),                datetime.datetime(2016, 9, 30, 0, 0),                datetime.datetime(2016, 10, 31, 0, 0),                datetime.datetime(2016, 11, 30, 0, 0),                datetime.datetime(2016, 12, 31, 0, 0)%5D, dtype%3Dobject)Using the DatetimeIndex constructor also allows the opposite operation:In%5B74%5D:pd.DatetimeIndex(pdi)Out%5B74%5D: %3Cclass 'pandas.tseries.index.DatetimeIndex'%3E         %5B2016-01-31, ..., 2016-12-31%5D         Length: 12, Freq: None, Timezone: NoneIn the case of NumPydatetime64 objects, the astype method has to be used:In%5B75%5D:pd.DatetimeIndex(dtl.astype(pd.datetime))Out%5B75%5D: %3Cclass 'pandas.tseries.index.DatetimeIndex'%3E         %5B2015-12-31 23:00:00, ..., 2016-01-01 22:00:00%5D         Length: 24, Freq: None, Timezone: Nonepandastakes care of proper plotting of date-time information (see Figure C-2 and alsoChapter 6):In%5B76%5D:rnd%3Dnp.random.standard_normal(len(dti)).cumsum()**2In%5B77%5D:df%3Dpd.DataFrame(rnd,columns%3D%5B'data'%5D,index%3Ddti)In%5B78%5D:df.plot()","Figure C-2. pandas plot with Timestamp x-ticks autoformattedpandas also integrates well with the pytz module to manage time zones:In%5B79%5D:pd.date_range('2016/01/01',freq%3D'M',periods%3D12,tz%3Dpytz.timezone('CET'))Out%5B79%5D: %3Cclass 'pandas.tseries.index.DatetimeIndex'%3E         %5B2016-01-31 00:00:00%2B01:00, ..., 2016-12-31 00:00:00%2B01:00%5D         Length: 12, Freq: M, Timezone: CETIn%5B80%5D:dti%3Dpd.date_range('2016/01/01',freq%3D'M',periods%3D12,tz%3D'US/Eastern')dtiOut%5B80%5D: %3Cclass 'pandas.tseries.index.DatetimeIndex'%3E         %5B2016-01-31 00:00:00-05:00, ..., 2016-12-31 00:00:00-05:00%5D         Length: 12, Freq: M, Timezone: US/EasternUsing the tz_convert method, DatetimeIndex objects can be transformed from onetime zone to another:In%5B81%5D:dti.tz_convert('GMT')Out%5B81%5D: %3Cclass 'pandas.tseries.index.DatetimeIndex'%3E         %5B2016-01-31 05:00:00%2B00:00, ..., 2016-12-31 05:00:00%2B00:00%5D         Length: 12, Freq: M, Timezone: GMT","We'd like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.","64-bit double precision standard, 83","absolute minimum variance portfolio, 330actual continuation value, 502adaptive quadrature, 256American exercisedefinition of, 489, 500Least-Squares Monte Carlo (LSM) algo%E2%80%90rithm, 501use case, 504%E2%80%93507valuation class, 502American optionsdefinition of, 291on the VSTOXX, 542%E2%80%93545valuation of contingent claims, 295Anaconda, 26%E2%80%9332benefits of, 26conda package manager, 30downloading, 26installing, 27libraries/packages available, 27multiple Python environments, 31analyticsbasic, 146derivatives analytics libraryderivatives valuation, 489%E2%80%93507extensions to, 526modularization offered by, 511portfolio valuation, 511%E2%80%93525simulation of financial models, 467%E2%80%93486valuation framework, 455%E2%80%93465volatility options, 529%E2%80%93545financialdefinition of, 12implied volatilities example, 50%E2%80%9359Monte Carlo simulation example, 59%E2%80%9368retrieving data, 151%E2%80%93156size of data sets, 173technical analysis example, 68%E2%80%9374interactivebenefits of Python for, 18%E2%80%9321publishing platform for sharing, 39tools for, 34%E2%80%9347real-time, 12annualized performance, 324antithetic paths, 469antithetic variates, 288application developmentbenefits of Python for end-to-end, 21documentation best practices, 550rapid web applications, 424%E2%80%93442syntax best practices, 547tools for, 34%E2%80%9347unit testing best practices, 553","approximation of functions, 234%E2%80%93249interpolation, 245regression, 234%E2%80%93245arbitrary precision floats, 83arraysDataFrames and, 146input-output operations with PyTables, 197memory layout and, 105regular NumPy arrays, 97%E2%80%93101structure of, 95structured arrays, 101with Python lists, 96writing/reading NumPy, 181average loss level, 302","basic analytics, 146Bayesian regression, 341%E2%80%93355diachronic interpretation of Bayes's formula,341introductory example, 343overview of, 308, 355PyMC3 library, 342real data, 347%E2%80%93355beliefs of agents, 308Bermudan exercises, 295, 500best practicesdocumentation, 550functional programming tools, 92syntax, 547unit testing, 553bfill parameter, 162big data, 12, 173binomial model, 501binomial option pricing, 218%E2%80%93223Black-Scholes-Merton modelclass definition for European call option,557%E2%80%93561European call option, 14%E2%80%9316formula for, 50LaTeX code for, 42parameters meanings, 50simulating future index level, 271stochastic differential equation, 60Vega of a European option, 51Bokeh librarybenefits of, 412default output, 413interactive plots, 414plotting styles, 413real-time plots, 417stand-alone graphics files, 415boxplots, 125broadcasting, 103brute function, 250, 539","call optionsclass definition for European, 557%E2%80%93561definition of, 291candlestick plots, 128capital asset pricing model, 308capital market line, 332cash flow series, 391, 398cellsin DataNitro, 371in Excel spreadsheets, 363in IPython, 37characters, symbols for, 114classesaccessing attribute values, 382assigning new attribute values, 383attributes and, 382cash flow series example, 391defining, 382defining object attributes, 383for risk-neutral discounting, 460generic simulation class, 470generic valuation class, 489geometric Brownian motion, 473inheritance in, 382iteration over, 385jump diffusion, 478private attributes, 385readability and maintainability of, 384reusability and, 383simple short rate class example, 387square-root diffusion, 482to model derivatives portfolios, 516to model derivatives positions, 512valuation class for American exercise, 502valuation class for European exercise, 494coefficient of determination, 243color abbreviations, 114comma-separated value (CSV) filesgenerating Excel spreadsheets with, 359input-output operations with pandas, 188parameters of read_csv function, 161","reading/writing, 177regular expressions and, 85retrieving via the Web, 408communication protocolsfile transfer protocol, 404hypertext transfer protocol, 407providing web services via, 442%E2%80%93451secure connections, 406uniform resource locators, 408compilationdynamic, 217%E2%80%93223static, 223%E2%80%93226compiled languages, 80compressed tables, working with, 196concatenate function, 288conda package manager, 30configure_traits method, 394constant short rate, 460constrained optimization, 253contingent claims, valuation of, 290%E2%80%93297American options, 295European options, 291continuation value, 295, 501control structures, 89convenience methods, 146convex optimization, 249%E2%80%93254constrained, 253functions for, 539global, 250local, 251covariance matrix, 324covariances, 308Cox-Ingersoll-Ross SDE, 276Cox-Ross-Rubinstein binomial model, 501credit value adjustment (CVA), 302credit value-at-risk (CVaR), 302CSS (Cascading Style Sheets), 440cubic splines, 245Cython library, 80, 223","databasic data structures, 86%E2%80%9395basic data types, 80%E2%80%9386big data, 12, 173formats supported by pandas library, 183high frequency, 166high-frequency, 421missing data, 141, 147noisy data, 240NumPy data structures, 95%E2%80%93102provision/gathering with web technology,403quality of web sources, 129, 151real-time foreign exchange, 418real-time stock price quotes, 421resampling of, 168retrieving, 151%E2%80%93156sources of, 152storage of, 173unsorted data, 241VSTOXX data, 530%E2%80%93534data visualization3D plotting, 132Bokeh library for, 412financial plots, 128for implied volatilities, 57graphical analysis of Monte Carlo simula%E2%80%90tion, 67interactive plots, 414panning/zooming, 414plot_surface parameters, 134plt.axis options, 112plt.candlestick parameters, 130plt.hist parameters, 124plt.legend options, 116real-time plots, 417standard color abbreviations, 114standard style characters, 114static plots, 411two-dimensional plotting, 109%E2%80%93128DataFrame class, 138%E2%80%93146arrays and, 146features of, 139frequency parameters for date-range func%E2%80%90tion, 145line plot of DataFrame object, 147parameters of DataFrame function, 143, 183parameters of date-range function, 144similarity to SQL database table, 138vectorization with, 154DataNitrobenefits of, 369cell attributes, 371cell methods, 373cell typesetting options, 372combining with Excel, 370installing, 369","optimizing performance, 374plotting with, 374scripting with, 371user-defined functions, 376DataReader function, 152dates and timesdescribed by regular expressions, 85implied volatilities example, 50%E2%80%9359in risk-neutral discounting, 458Monte Carlo simulation example, 59%E2%80%9368NumPy support for, 568%E2%80%93571pandas support for, 571%E2%80%93573Python datetime module, 563%E2%80%93568technical analysis example, 68%E2%80%9374(see also financial time series data)datetime module, 563%E2%80%93568datetime64 class, 568%E2%80%93571date_range function, 144default, probability of, 302Deltas, 492dependent observations, 234deploymentAnaconda, 26%E2%80%9332Python Quant platform, 32via web browser, 32derivatives analytics libraryderivatives valuation, 489%E2%80%93507extensions to, 526goals for, 453modularization offered by, 511portfolio valuation, 511%E2%80%93525simulation of financial models, 467%E2%80%93486valuation framework, 455%E2%80%93465volatility options, 529%E2%80%93545derivatives portfoliosclass for valuation, 516relevant market for, 515use case, 520%E2%80%93525derivatives positionsdefinition of, 512modeling class, 512use case, 514derivatives valuationAmerican exercise, 500%E2%80%93507European exercise, 493%E2%80%93500generic valuation class, 489methods available, 489deserialization, 174diachronic interpretation (of Bayes's formula),341dicts, 92differentiation, 261discounting, 387, 458discretization error, 274diversification, 323documentationbest practices, 550documentation strings, 550IPython Notebook for, 38dot function, 238, 326DX (Derivatives AnalytiX) library, 453dynamic compiling, 217%E2%80%93223binomial option pricing, 218%E2%80%93223example of, 217dynamically typed languages, 80","early exercise premium, 297editorsconfiguring, 45Spyder, 45efficiency, 17%E2%80%9321efficient frontier, 330efficient markets hypothesis, 308encryption, 406errorsdiscretization error, 274mean-squared error (MSE), 538sampling error, 274estimated continuation value, 502Euler scheme, 65, 277, 483European exercisedefinition of, 489Monte Carlo estimator for option values, 493use case, 496%E2%80%93500valuation class, 494European optionsdefinition of, 291valuation of contingent claims, 291Excelbasic spreadsheet interaction, 358%E2%80%93369benefits of, 357cell types in, 363drawbacks of, 358features of, 357file input-output operations, 189integration with Python, 358","integration with xlwings, 379scripting with Python, 369%E2%80%93379excursioncontrol structures, 89functional programming, 91expected portfolio return, 325expected portfolio variance, 325","fat tails, 300, 320ffill parameter, 162file transfer protocol, 404fillna method, 162financemathematical tools for, 233%E2%80%93262role of Python in, 13%E2%80%9322role of technology in, 9%E2%80%9313role of web technologies in, 403financial analyticsbasic analytics, 146(see also financial time series data)definition of, 12implied volatilities example, 50%E2%80%9359Monte Carlo simulation example, 59%E2%80%9368retrieving data, 151%E2%80%93156size of data sets, 173technical analysis example, 68%E2%80%9374financial plots, 128%E2%80%93131financial time series datadefinition of, 137financial data, 151%E2%80%93156high frequency data, 166pandas library, 138%E2%80%93151regression analysis, 157%E2%80%93166first in, first out (FIFO) principle, 177fixed Gaussian quadrature, 256flash trading, 11Flask frameworkbenefits of, 425commenting functionality, 430connection/log in, 429data modeling, 426database infrastructure, 428importing libraries, 427libraries required, 425security issues, 434styling web pages in, 440templating in, 434traders' chat room application, 426floats, 81%E2%80%9383fmin function, 250, 539frequency distribution, 523ftplib library, 404full truncation, 277, 483functional programming, 91Fundamental Theorem of Asset Pricing, 290,455, 515FX (foreign exchange) data, 418","general market model, 457, 515General Purpose Graphical Processing Units(GPGPUs), 226generate_payoff method, 494geometric Brownian motion, 467, 473get_info method, 512global optimization, 250, 539graphical analysis, 67(see also matplotlib library)graphical user interfaces (GUIs)cash flow series with, 398libraries required, 393Microsoft Excel as, 358short rate class with, 394updating values, 396Greeks, estimation of, 492groupby operations, 150Gruenbichler and Longstaff model, 443Guassian quadrature, 256","HDF5 database format, 198Heston stochastic volatility model, 281high frequency data, 166histograms, 123HTML-based web pages, 407httplib library, 407hypertext transfer protocol, 407","immutability, 88implied volatilitiesBlack-Scholes-Merton formula, 50definition of, 50futures data, 54Newton scheme for, 51","option quotes, 54visualizing data, 57volatility smile, 57importing, definition of, 6independent observations, 234inline documentation, 550input-output operationswith pandasdata as CSV file, 188data as Excel file, 189from SQL to pandas, 185SQL databases, 184with PyTablesout-of-memory computations, 198working with arrays, 197working with compressed tables, 196working with tables, 190with Pythonreading/writing text files, 177SQL databases, 179writing objects to disk, 174writing/reading Numpy arrays, 181input/output operationsimportance of, 173integer index, 58integers, 80integrate sublibrary, 256integrationby simulation, 257numerical, 256scipy.integrate sublibrary, 255symbolic computation, 260interactive analyticsbenefits of Python for, 18%E2%80%9321publishing platform for sharing, 39rise of real-time, 12tools for, 34%E2%80%9347interactive web plots, 414interpolation, 245%E2%80%93249interpretersIPython, 35%E2%80%9345standard, 34IPython, 35%E2%80%9345basic usage, 37benefits of, 7documentation with, 38help functions in, 44importing libraries, 36invoking, 35IPython.parallel library, 209%E2%80%93214magic commands, 43Markdown commands, 41rendering capabilities, 41system shell commands, 45versions of, 35iter method, 385","Jinja2 library, 425jump diffusion, 285, 467, 478","KernelPCA function, 336killer application, 7kurtosis test, 314","large integers, 81LaTeXcommands, 41IPython Notebook cells and, 40least-squares function, 238Least-Squares Monte Carlo (LSM) algorithm,295, 489, 501leverage effect, 155, 282librariesavailable in Anaconda, 27Cython library, 80importing, 6, 105, 234importing to IPython, 36standard, 6list comprehensions, 91lists, 88, 96LLVM compiler infrastructure, 217local maximum a posteriori point, 344local optimization, 251, 539lognormal function, 272Longstaff-Schwartz model, 501, 504loss level, 302","magic commands/functions, 43Markdown commands, 41market environments, 462(Markov Chain) Monte Carlo (MCMC) sam%E2%80%90pling, 344","Markov property, 274martingale measures, 455, 501mathematical syntax, 17mathematical toolsapproximation of functions, 234%E2%80%93262convex optimization, 249%E2%80%93254integration, 255symbolic computation, 257matplotlib library3D plotting, 132benefits of, 8financial plots, 128%E2%80%93131importing matplotlib.pyplot, 234NumPy arrays and, 111pandas library wrapper for, 148strengths of, 411two-dimensional plotting, 109%E2%80%93128maximization of Sharpe ratio, 328mean returns, 308mean-squared error (MSE), 538mean-variance, 324mean-variance portfolio theory (MPT), 322memory layout, 105memory-less processes, 274Microsoft Excel (see Excel)minimization function, 328missing data, 141, 147model calibrationoption modeling, 536procedure for, 538relevant market data, 535modern portfolio theory (MPT), 307, 322moment matching, 289, 469Monte Carlo simulationapproaches to, 59benefits of, 59BSM stochastic differential equation, 60drawbacks of, 59, 501for European call option, 61full vectorization with log Euler scheme, 65graphical analysis of, 67importance of, 271integration by simulation, 257Least-Squares Monte Carlo (LSM) algo%E2%80%90rithm, 295, 489pure Python approach, 61valuation of contingent claims, 290%E2%80%93297vectorization with NumPy, 63moving averages, 155multiple dimensions, 242multiprocessing module, 215mutability, 88","ndarray class, 63Newton scheme, 51noisy data, 240normality tests, 308%E2%80%93322benchmark case, 309importance of, 308normality assumption, 317overview of, 307, 355real-world data, 317Numba library, 217%E2%80%93223NumbaPro library, 226numexpr library, 205NumPybenefits of, 8concatenate function, 288data structures, 95%E2%80%93102date-time information support in, 568importing, 234Monte Carlo simulation with, 63numpy.linalg sublibrary, 238numpy.random sublibrary, 266universal functions, 147writing/reading arrays, 181NUTS algorithm, 344","OANDA online broker, 418object orientation, 381%E2%80%93393cash flow series class example, 391definition of, 381Python classes, 382simple short rate class example, 387observation points, 234, 241OpenPyxl library, 364operators, 550optimal decision step, 502optimal stopping problems, 295, 501optimizationconstrained, 253convex, 249%E2%80%93254global, 250local, 251option pricing theory, 309","ordinary least-squares regression (OLS), 157,243, 501out-of-memory computations, 198","pandas library, 138%E2%80%93151basic analytics, 146benefits of, 9, 74data formats supported, 183data sources supported, 152DataFrame class, 138%E2%80%93146date-time information support in, 571%E2%80%93573development of, 137error tolerance in, 147groupby operations, 150hierarchically indexed data sets and, 58input-output operationsdata as CSV file, 188data as Excel file, 189from SQL to pandas, 185SQL databases, 184reading/writing spreadsheets with, 366Series class, 149working with missing data, 141wrapper for matplotlib library, 148parallel computing, 209%E2%80%93214Monte Carlo algorithm, 209parallel calculation, 211performance comparison, 214sequential calculation, 210PEP (Python Enhancement Proposal) 20, 4PEP (Python Enhancement Proposal) 8, 547performance computingbenefits of Python for, 19dynamic compiling, 217%E2%80%93223memory layout and, 207multiprocessing module, 215parallel computing, 209%E2%80%93214Python paradigms and, 204random number generation on GPUs, 226static compiling with Cython, 223petascale processing, 173pickle module, 174plot function, 110plot method, 148plot_surface function, 134plt.axis method, 112plt.candlestick, 130plt.hist function, 124plt.legend function, 116PNG (portable network graphics) format, 412Poisson distribution, 270polyfit function, 235portfolio theory/portfolio optimizationbasic idea of, 323basic theory, 324capital market line, 332data collection for, 323efficient frontier, 330importance of, 322overview of, 308, 355portfolio covariance matrix, 325portfolio optimizations, 328portfolio valuationbenefits of analytics library for, 511derivatives portfolios, 515%E2%80%93525derivatives positions, 512%E2%80%93515requirements for complex portfolios, 512precision floats, 83presentation, IPython Notebook for, 38present_value method, 494principal component analysis (PCA), 335%E2%80%93340applying, 337constructing PCA indices, 338DAX index stocks, 336definition of, 335overview of, 308, 355print_statistics helper function, 273private attributes, 385probability of default, 302productivity, 17%E2%80%9321pseudocode, 17pseudorandom numbers, 266, 287publishing platform, 39put options, definition of, 291PyMC3 library, 342pyplot sublibrary, 110PyTablesbenefits of, 8, 190importing, 190input-output operationsout-of-memory computations, 198working with arrays, 197working with compressed tables, 196working with tables, 190Pythonas ecosystem vs. language, 6benefits for finance, 13%E2%80%9322, 174, 404","benefits of, 3classes in, 382%E2%80%93393, 460deployment, 26%E2%80%9333features of, 3history of, 5input-output operationsreading/writing text files, 177SQL databases, 179writing objects to disk, 174writing/reading Numpy arrays, 181invoking interpreter, 34multiple environments for, 31Quant platform, 32, 454rapid web application development, 424%E2%80%93442scientific stack, 8, 69user spectrum, 7zero-based numbering in, 87Python Quants GmbHbenefits of, 32features of, 33, 454","quadratures, fixed Gaussian and adaptive, 256Quant platformbenefits of, 32features of, 33, 454quantile-quantile (qq) plots, 313queries, 195","random number generation, 226, 266%E2%80%93270, 468functions according to distribution laws, 268functions for simple, 267random variables, 271rapid web application developmentbenefits of Python for, 425commenting functionality, 430connection/log in, 429data modeling, 426database infrastructure, 428Flask framework for, 425importing libraries, 427popular frameworks for, 425security issues, 434styling web pages, 440templating, 434traders' chat room, 426read_csv function, 161real-time analytics, 12real-time economy, 12real-time plots, 417real-time stock price quotes, 421regression analysismathematical tools forindividual basis functions, 238monomials as basis functions, 235multiple dimensions and, 242noisy data and, 240strengths of, 234unsorted data and, 241of financial time series data, 157%E2%80%93166regular expressions, 85reg_func function, 244requests library, 418resampling, 168risk management, 489(see also derivatives valuation%3B risk meas%E2%80%90ures)risk measures, 298%E2%80%93305credit value adjustments, 302value-at-risk (VaR), 298risk-neutral discounting, 458risk-neutral valuation approach, 457rolling functions, 155Romberg integration, 256","sampling error, 274scatter plots, 121scientific stack, 8, 69scikit-learn library, 336SciPybenefits of, 8scipy.integrate sublibrary, 255scipy.optimize sublibrary, 250scipy.optimize.minimize function, 253scipy.stats sublibrary, 273, 309sensitivity analysis, 392serialization, 174Series class, 149sets, 94Sharpe ratio, 328short rates, 387, 394, 460simple random number generation, 267Simpson's rule, 256simulationdiscretization error in, 274","generic simulation class, 470geometric Brownian motion, 272%E2%80%93273, 473jump diffusion, 478noisy data from, 240numerical integration by, 257random number generation, 468random variables, 271sampling error in, 274square-root diffusion, 482stochastic processes, 274%E2%80%93290, 467variance reduction, 287skewness test, 314Software-as-a-Service (SaaS), 403splev function, 246spline interpolation, 245splrep function, 246spreadsheetsExcel cell types, 363generating xls workbooks, 359generating xlsx workbooks, 360OpenPyxl library for, 364Python libraries for, 358reading from workbooks, 362reading/writing with pandas, 366Spyderbenefits of, 45features of, 46SQL databasesinput-output operations with pandas, 184input-output operations with Python, 179square-root diffusion, 276, 467, 482, 536standard color abbreviations, 114standard interpreter, 34standard normally distributed random num%E2%80%90bers, 468standard style characters, 114star import, 6, 105static plots, 411statically typed languages, 80statistics, 307%E2%80%93355Bayesian regression, 308, 341%E2%80%93355focus areas covered, 307normality tests, 307%E2%80%93322portfolio theory, 307, 322%E2%80%93335principal component analysis, 308, 335%E2%80%93340statmodels library, 243stochastic differential equation (SDE), 274stochastic processes, 274%E2%80%93290definition of, 274geometric Brownian motion, 274, 467, 473importance of, 265jump diffusion, 285, 467, 478square-root diffusion, 276, 467, 482stochastic volatility model, 281stringsdocumentation strings, 550Python string class, 84%E2%80%9386selected string methods, 84string objects, 84structured arrays, 101Symbol class, 258symbolic computationbasics of, 258differentiation, 261equations, 259integration, 260SymPy librarybenefits for symbolic computations, 262differentiation with, 261equation solving with, 259integration with, 260mathematical function definitions, 258Symbol class, 258syntaxbenefits of Python for finance, 14%E2%80%9317best practices, 4, 547mathematical, 17Python 2.7 vs. 3.x, 31","tablescompressed, 196working with, 190tail risk, 298technical analysisbacktesting example, 69definition of, 68retrieving time series data, 69testing investment strategy, 73trading signal rules, 71trend strategy, 70technology, role in finance, 9%E2%80%9313templating, 434testing, unit testing, 553textreading/writing text files, 177representation with strings, 84three-dimensional plotting, 132","tools, 34%E2%80%9347IPython, 35%E2%80%9345Python interpreter, 34Spyder, 45%E2%80%9347(see also mathematical tools)traders' chat room applicationbasic idea of, 426commenting functionality, 430connection/log in, 429data modeling, 426database infrastructure, 428importing libraries, 427security issues, 434styling, 440templating, 434traits library, 393traitsui.api library, 396trapezoidal rule, 256tuples, 87two-dimensional plottingimporting libraries, 109one-dimensional data set, 110%E2%80%93115other plot styles, 121%E2%80%93128two-dimensional data set, 115%E2%80%93120","unit testing best practices, 553universal functions, 104, 147unsorted data, 241updating of beliefs, 308urllib library, 408URLs (uniform resource locators), 408user-defined functions (UDF), 376","valuation frameworkFundamental Theorem of Asset Pricing, 455overview of, 455risk-neutral discounting, 458valuation of contingent claims, 290%E2%80%93297American options, 295European options, 291valuation theory, 501value-at-risk (VaR), 298values, updating in GUI, 396variance of returns, 308variance reduction, 287vectorizationbasic, 102full with log Euler scheme, 65fundamental idea of, 102memory layout, 105speed increase achieved by, 65with DataFrames, 154with NumPy, 63Vegadefinition of, 493of a European option in BSM model, 51visualization (see data visualization)VIX volatility index, 529volatility clustering, 155volatility index, 443volatility optionsAmerican on the VSTOXX, 542%E2%80%93545main index, 529model calibration, 534%E2%80%93541tasks undertaken, 530VSTOXX data, 530%E2%80%93534volatility smile, 57volatility, stochastic model, 281VSTOXX datafutures data, 531index data, 530libraries required, 530options data, 533","web browser deployment, 32web technologiescommunication protocols, 404%E2%80%93411rapid web applications, 424%E2%80%93442role in finance, 403web plotting, 411%E2%80%93423web services, 442%E2%80%93451Werkzeug library, 425workbooksgenerating xls workbooks, 359generating xlsx workbooks, 360OpenPyxl library for, 364pandas generated, 366reading from, 362","xlrd library, 358xlsxwriter library, 358","xlwings library, 379xlwt library, 358","Yahoo! Finance, 129, 152","Zen of Python, 4zero-based numbering schemes, 87"," is founder and managing partner of The Python Quants GmbH, Ger%E2%80%90many, as well as cofounder of The Python Quants LLC, New York City. The groupprovides Python-based financial and derivatives analytics software (cf. http://pythonquants.com, http://quant-platform.com, and http://dx-analytics.com), as well as con%E2%80%90sulting, development, and training services related to Python and finance.Yves is also author of the book Derivatives Analytics with Python (Wiley Finance, 2015).As a graduate in Business Administration with a Dr.rer.pol. in Mathematical Finance,he lectures on Numerical Methods in Computational Finance at Saarland University."]}